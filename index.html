<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2510.08553.pdf' target='_blank'>https://arxiv.org/pdf/2510.08553.pdf</a></span>   <span><a href='https://github.com/xyz9911/Memoir' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunzhe Xu, Yiyuan Pan, Zhe Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08553">Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) requires agents to follow natural language instructions through environments, with memory-persistent variants demanding progressive improvement through accumulated experience. Existing approaches for memory-persistent VLN face critical limitations: they lack effective memory access mechanisms, instead relying on entire memory incorporation or fixed-horizon lookup, and predominantly store only environmental observations while neglecting navigation behavioral patterns that encode valuable decision-making strategies. We present Memoir, which employs imagination as a retrieval mechanism grounded by explicit memory: a world model imagines future navigation states as queries to selectively retrieve relevant environmental observations and behavioral histories. The approach comprises: 1) a language-conditioned world model that imagines future states serving dual purposes: encoding experiences for storage and generating retrieval queries; 2) Hybrid Viewpoint-Level Memory that anchors both observations and behavioral patterns to viewpoints, enabling hybrid retrieval; and 3) an experience-augmented navigation model that integrates retrieved knowledge through specialized encoders. Extensive evaluation across diverse memory-persistent VLN benchmarks with 10 distinctive testing scenarios demonstrates Memoir's effectiveness: significant improvements across all scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent baseline, accompanied by 8.3x training speedup and 74% inference memory reduction. The results validate that predictive retrieval of both environmental and behavioral memories enables more effective navigation, with analysis indicating substantial headroom (73.3% vs 93.4% upper bound) for this imagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2509.15250.pdf' target='_blank'>https://arxiv.org/pdf/2509.15250.pdf</a></span>   <span><a href='https://github.com/wdqin/VLN-NAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenda Qin, Andrea Burns, Bryan A. Plummer, Margrit Betke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15250">Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large models achieve strong performance on Vision-and-Language Navigation (VLN) tasks, but are costly to run in resource-limited environments. Token pruning offers appealing tradeoffs for efficiency with minimal performance loss by reducing model input size, but prior work overlooks VLN-specific challenges. For example, information loss from pruning can effectively increase computational cost due to longer walks. Thus, the inability to identify uninformative tokens undermines the supposed efficiency gains from pruning. To address this, we propose Navigation-Aware Pruning (NAP), which uses navigation-specific traits to simplify the pruning process by pre-filtering tokens into foreground and background. For example, image views are filtered based on whether the agent can navigate in that direction. We also extract navigation-relevant instructions using a Large Language Model. After filtering, we focus pruning on background tokens, minimizing information loss. To further help avoid increases in navigation length, we discourage backtracking by removing low-importance navigation nodes. Experiments on standard VLN benchmarks show NAP significantly outperforms prior work, preserving higher success rates while saving more than 50% FLOPS.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2509.06644.pdf' target='_blank'>https://arxiv.org/pdf/2509.06644.pdf</a></span>   <span><a href='https://github.com/AlexTraveling/T-araVLN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobei Zhao, Xingqi Lyu, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06644">T-araVLN: Translator for Agricultural Robotic Agents on Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agricultural robotic agents have been becoming powerful helpers in a wide range of agricultural tasks, however, still heavily rely on manual operation or fixed railways for movement. To address this limitation, the AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling agents to navigate to the target positions following the natural language instructions. AgriVLN effectively understands the simple instructions, but often misunderstands the complex ones. To bridge this gap, we propose the method of Translator for Agricultural Robotic Agents on Vision-and-Language Navigation (T-araVLN), in which the Instruction Translator module translates the original instruction to be more refined and precise. When evaluated on the A2A benchmark, our T-araVLN effectively improves Success Rate from 0.47 to 0.63 and reduces Navigation Error from 2.91m to 2.28m, demonstrating the state-of-the-art performance in the agricultural domain. Code: https://github.com/AlexTraveling/T-araVLN.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2508.09262.pdf' target='_blank'>https://arxiv.org/pdf/2508.09262.pdf</a></span>   <span><a href='https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongwoo Kang, Akhil Perincherry, Zachary Coalson, Aiden Gabriel, Stefan Lee, Sanghyun Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09262">Harnessing Input-Adaptive Inference for Efficient VLN</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An emerging paradigm in vision-and-language navigation (VLN) is the use of history-aware multi-modal transformer models. Given a language instruction, these models process observation and navigation history to predict the most appropriate action for an agent. While they have significantly improved performance, the scale of these models can be a bottleneck in practical settings with limited computational resources. In this work, we propose a novel input-adaptive navigation method to enhance VLN model efficiency. We first show that existing input-adaptive mechanisms fail to reduce computations without substantial performance degradation. To address this, we introduce three adaptive algorithms, each deployed at a different level: (1) To improve spatial efficiency, we selectively process panoramic views at each observation of an agent. (2) To improve intra-model efficiency, we propose importance-based adaptive thresholding for the early-exit methods. (3) To improve temporal efficiency, we implement a caching mechanism that prevents reprocessing of views previously seen by the agent. In evaluations on seven VLN benchmarks, we demonstrate over a 2$\times$ reduction in computation across three off-the-shelf agents in both standard and continuous environments. Our code is publicly available at https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2508.01766.pdf' target='_blank'>https://arxiv.org/pdf/2508.01766.pdf</a></span>   <span><a href='https://github.com/farlit/VPN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Feng, Zihan Wang, Yuchen Li, Rui Kong, Hengyi Cai, Shuaiqiang Wang, Gim Hee Lee, Piji Li, Shuqiang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01766">VPN: Visual Prompt Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While natural language is commonly used to guide embodied agents, the inherent ambiguity and verbosity of language often hinder the effectiveness of language-guided navigation in complex environments. To this end, we propose Visual Prompt Navigation (VPN), a novel paradigm that guides agents to navigate using only user-provided visual prompts within 2D top-view maps. This visual prompt primarily focuses on marking the visual navigation trajectory on a top-down view of a scene, offering intuitive and spatially grounded guidance without relying on language instructions. It is more friendly for non-expert users and reduces interpretive ambiguity. We build VPN tasks in both discrete and continuous navigation settings, constructing two new datasets, R2R-VP and R2R-CE-VP, by extending existing R2R and R2R-CE episodes with corresponding visual prompts. Furthermore, we introduce VPNet, a dedicated baseline network to handle the VPN tasks, with two data augmentation strategies: view-level augmentation (altering initial headings and prompt orientations) and trajectory-level augmentation (incorporating diverse trajectories from large-scale 3D scenes), to enhance navigation performance. Extensive experiments evaluate how visual prompt forms, top-view map formats, and data augmentation strategies affect the performance of visual prompt navigation. The code is available at https://github.com/farlit/VPN.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2506.23468.pdf' target='_blank'>https://arxiv.org/pdf/2506.23468.pdf</a></span>   <span><a href='https://github.com/Feliciaxyao/NavMorph' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Yao, Junyu Gao, Changsheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23468">NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to execute sequential navigation actions in complex environments guided by natural language instructions. Current approaches often struggle with generalizing to novel environments and adapting to ongoing changes during navigation. Inspired by human cognition, we present NavMorph, a self-evolving world model framework that enhances environmental understanding and decision-making in VLN-CE tasks. NavMorph employs compact latent representations to model environmental dynamics, equipping agents with foresight for adaptive planning and policy refinement. By integrating a novel Contextual Evolution Memory, NavMorph leverages scene-contextual information to support effective navigation while maintaining online adaptability. Extensive experiments demonstrate that our method achieves notable performance improvements on popular VLN-CE benchmarks. Code is available at https://github.com/Feliciaxyao/NavMorph.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2506.19433.pdf' target='_blank'>https://arxiv.org/pdf/2506.19433.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/Mem4Nav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lixuan He, Haoyu Dong, Zhenxing Chen, Yangcheng Yu, Jie Feng, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19433">Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2506.01950.pdf' target='_blank'>https://arxiv.org/pdf/2506.01950.pdf</a></span>   <span><a href='https://github.com/Eku127/DualMap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajun Jiang, Yiming Zhu, Zirui Wu, Jie Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01950">DualMap: Online Open-Vocabulary Semantic Mapping for Natural Language Navigation in Dynamic Changing Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DualMap, an online open-vocabulary mapping system that enables robots to understand and navigate dynamically changing environments through natural language queries. Designed for efficient semantic mapping and adaptability to changing environments, DualMap meets the essential requirements for real-world robot navigation applications. Our proposed hybrid segmentation frontend and object-level status check eliminate the costly 3D object merging required by prior methods, enabling efficient online scene mapping. The dual-map representation combines a global abstract map for high-level candidate selection with a local concrete map for precise goal-reaching, effectively managing and updating dynamic changes in the environment. Through extensive experiments in both simulation and real-world scenarios, we demonstrate state-of-the-art performance in 3D open-vocabulary segmentation, efficient scene mapping, and online language-guided navigation.Project page: https://eku127.github.io/DualMap/
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2505.20897.pdf' target='_blank'>https://arxiv.org/pdf/2505.20897.pdf</a></span>   <span><a href='https://github.com/zhangpingrui/Adaptive-Text-Dreamer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pingrui Zhang, Yifei Su, Pengyuan Wu, Dong An, Li Zhang, Zhigang Wang, Dong Wang, Yan Ding, Bin Zhao, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20897">Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) requires the agent to navigate by following natural instructions under partial observability, making it difficult to align perception with language. Recent methods mitigate this by imagining future scenes, yet they rely on vision-based synthesis, leading to high computational cost and redundant details. To this end, we propose to adaptively imagine key environmental semantics via \textit{language} form, enabling a more reliable and efficient strategy. Specifically, we introduce a novel Adaptive Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a large language model (LLM). ATD is designed with a human-like left-right brain architecture, where the left brain focuses on logical integration, and the right brain is responsible for imaginative prediction of future scenes. To achieve this, we fine-tune only the Q-former within both brains to efficiently activate domain-specific knowledge in the LLM, enabling dynamic updates of logical reasoning and imagination during navigation. Furthermore, we introduce a cross-interaction mechanism to regularize the imagined outputs and inject them into a navigation expert module, allowing ATD to jointly exploit both the reasoning capacity of the LLM and the expertise of the navigation model. We conduct extensive experiments on the R2R benchmark, where ATD achieves state-of-the-art performance with fewer parameters. The code is \href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2505.05622.pdf' target='_blank'>https://arxiv.org/pdf/2505.05622.pdf</a></span>   <span><a href='https://github.com/VinceOuti/CityNavAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weichen Zhang, Chen Gao, Shiquan Yu, Ruiying Peng, Baining Zhao, Qian Zhang, Jinqiang Cui, Xinlei Chen, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05622">CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerial vision-and-language navigation (VLN), requiring drones to interpret natural language instructions and navigate complex urban environments, emerges as a critical embodied AI challenge that bridges human-robot interaction, 3D spatial reasoning, and real-world deployment. Although existing ground VLN agents achieved notable results in indoor and outdoor settings, they struggle in aerial VLN due to the absence of predefined navigation graphs and the exponentially expanding action space in long-horizon exploration. In this work, we propose \textbf{CityNavAgent}, a large language model (LLM)-empowered agent that significantly reduces the navigation complexity for urban aerial VLN. Specifically, we design a hierarchical semantic planning module (HSPM) that decomposes the long-horizon task into sub-goals with different semantic levels. The agent reaches the target progressively by achieving sub-goals with different capacities of the LLM. Additionally, a global memory module storing historical trajectories into a topological graph is developed to simplify navigation for visited targets. Extensive benchmark experiments show that our method achieves state-of-the-art performance with significant improvement. Further experiments demonstrate the effectiveness of different modules of CityNavAgent for aerial VLN in continuous city environments. The code is available at \href{https://github.com/VinceOuti/CityNavAgent}{link}.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2412.08467.pdf' target='_blank'>https://arxiv.org/pdf/2412.08467.pdf</a></span>   <span><a href='https://github.com/wz0919/VLN-SRDF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zun Wang, Jialu Li, Yicong Hong, Songze Li, Kunchang Li, Shoubin Yu, Yi Wang, Yu Qiao, Yali Wang, Mohit Bansal, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08467">Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-quality data for training robust language-instructed agents is a long-lasting challenge in embodied AI. In this paper, we introduce a Self-Refining Data Flywheel (SRDF) that generates high-quality and large-scale navigational instruction-trajectory pairs by iteratively refining the data pool through the collaboration between two models, the instruction generator and the navigator, without any human-in-the-loop annotation. Specifically, SRDF starts with using a base generator to create an initial data pool for training a base navigator, followed by applying the trained navigator to filter the data pool. This leads to higher-fidelity data to train a better generator, which can, in turn, produce higher-quality data for training the next-round navigator. Such a flywheel establishes a data self-refining process, yielding a continuously improved and highly effective dataset for large-scale language-guided navigation learning. Our experiments demonstrate that after several flywheel rounds, the navigator elevates the performance boundary from 70% to 78% SPL on the classic R2R test set, surpassing human performance (76%) for the first time. Meanwhile, this process results in a superior generator, evidenced by a SPICE increase from 23.5 to 26.2, better than all previous VLN instruction generation methods. Finally, we demonstrate the scalability of our method through increasing environment and instruction diversity, and the generalization ability of our pre-trained navigator across various downstream navigation tasks, surpassing state-of-the-art methods by a large margin in all cases.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2411.11394.pdf' target='_blank'>https://arxiv.org/pdf/2411.11394.pdf</a></span>   <span><a href='https://github.com/yanyu0526/InstruGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Yan, Rongtao Xu, Jiazhao Zhang, Peiyang Li, Xiaodan Liang, Jianqin Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11394">InstruGen: Automatic Instruction Generation for Vision-and-Language Navigation Via Large Multimodal Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research on Vision-and-Language Navigation (VLN) indicates that agents suffer from poor generalization in unseen environments due to the lack of realistic training environments and high-quality path-instruction pairs. Most existing methods for constructing realistic navigation scenes have high costs, and the extension of instructions mainly relies on predefined templates or rules, lacking adaptability. To alleviate the issue, we propose InstruGen, a VLN path-instruction pairs generation paradigm. Specifically, we use YouTube house tour videos as realistic navigation scenes and leverage the powerful visual understanding and generation abilities of large multimodal models (LMMs) to automatically generate diverse and high-quality VLN path-instruction pairs. Our method generates navigation instructions with different granularities and achieves fine-grained alignment between instructions and visual observations, which was difficult to achieve with previous methods. Additionally, we design a multi-stage verification mechanism to reduce hallucinations and inconsistency of LMMs. Experimental results demonstrate that agents trained with path-instruction pairs generated by InstruGen achieves state-of-the-art performance on the R2R and RxR benchmarks, particularly in unseen environments. Code is available at https://github.com/yanyu0526/InstruGen.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2409.05583.pdf' target='_blank'>https://arxiv.org/pdf/2409.05583.pdf</a></span>   <span><a href='https://github.com/gmuraleekrishna/SAS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muraleekrishna Gopinathan, Martin Masek, Jumana Abu-Khalaf, David Suter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05583">Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI aims to develop robots that can \textit{understand} and execute human language instructions, as well as communicate in natural languages. On this front, we study the task of generating highly detailed navigational instructions for the embodied robots to follow. Although recent studies have demonstrated significant leaps in the generation of step-by-step instructions from sequences of images, the generated instructions lack variety in terms of their referral to objects and landmarks. Existing speaker models learn strategies to evade the evaluation metrics and obtain higher scores even for low-quality sentences. In this work, we propose SAS (Spatially-Aware Speaker), an instruction generator or \textit{Speaker} model that utilises both structural and semantic knowledge of the environment to produce richer instructions. For training, we employ a reward learning method in an adversarial setting to avoid systematic bias introduced by language evaluation metrics. Empirically, our method outperforms existing instruction generation models, evaluated using standard metrics. Our code is available at \url{https://github.com/gmuraleekrishna/SAS}.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2407.11487.pdf' target='_blank'>https://arxiv.org/pdf/2407.11487.pdf</a></span>   <span><a href='https://github.com/iSEE-Laboratory/VLN-PRET' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Renjie Lu, Jingke Meng, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11487">PRET: Planning with Directed Fidelity Trajectory for Vision and Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision and language navigation is a task that requires an agent to navigate according to a natural language instruction. Recent methods predict sub-goals on constructed topology map at each step to enable long-term action planning. However, they suffer from high computational cost when attempting to support such high-level predictions with GCN-like models. In this work, we propose an alternative method that facilitates navigation planning by considering the alignment between instructions and directed fidelity trajectories, which refers to a path from the initial node to the candidate locations on a directed graph without detours. This planning strategy leads to an efficient model while achieving strong performance. Specifically, we introduce a directed graph to illustrate the explored area of the environment, emphasizing directionality. Then, we firstly define the trajectory representation as a sequence of directed edge features, which are extracted from the panorama based on the corresponding orientation. Ultimately, we assess and compare the alignment between instruction and different trajectories during navigation to determine the next navigation target. Our method outperforms previous SOTA method BEVBert on RxR dataset and is comparable on R2R dataset while largely reducing the computational cost. Code is available: https://github.com/iSEE-Laboratory/VLN-PRET.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2407.07035.pdf' target='_blank'>https://arxiv.org/pdf/2407.07035.pdf</a></span>   <span><a href='https://github.com/zhangyuejoslin/VLN-Survey-with-Foundation-Models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Zhang, Ziqiao Ma, Jialu Li, Yanyuan Qiao, Zun Wang, Joyce Chai, Qi Wu, Mohit Bansal, Parisa Kordjamshidi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07035">Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) has gained increasing attention over recent years and many approaches have emerged to advance their development. The remarkable achievements of foundation models have shaped the challenges and proposed methods for VLN research. In this survey, we provide a top-down review that adopts a principled framework for embodied planning and reasoning, and emphasizes the current methods and future opportunities leveraging foundation models to address VLN challenges. We hope our in-depth discussions could provide valuable resources and insights: on one hand, to milestone the progress and explore opportunities and potential roles for foundation models in this field, and on the other, to organize different challenges and solutions in VLN to foundation model researchers.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2406.17960.pdf' target='_blank'>https://arxiv.org/pdf/2406.17960.pdf</a></span>   <span><a href='https://github.com/CrystalSixone/VLN-MAGIC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuyi Wang, Zongtao He, Mengjiao Shen, Jingwei Yang, Chengju Liu, Qijun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17960">MAGIC: Meta-Ability Guided Interactive Chain-of-Distillation for Effective-and-Efficient Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the remarkable developments of recent large models in Embodied Artificial Intelligence (E-AI), their integration into robotics is hampered by their excessive parameter sizes and computational demands. Towards the Vision-and-Language Navigation (VLN) task, a core task in E-AI, this paper reveals the great potential of using knowledge distillation for obtaining lightweight student models by proposing a Meta-Ability Guided Interactive Chain-of-distillation (MAGIC) method. Specifically, a Meta-Ability Knowledge Distillation (MAKD) framework is proposed for decoupling and refining the necessary meta-abilities of VLN agents. A Meta-Knowledge Randomization Weighting (MKRW) and a Meta-Knowledge Transferable Determination (MKTD) module are incorporated to dynamically adjust aggregation weights at the meta-ability and sample levels, respectively. Move beyond the traditional one-step unidirectional distillation, an Interactive Chain-of-Distillation (ICoD) learning strategy is proposed to allow students to give feedback to teachers, forming a new multi-step teacher-student co-evolution pipeline. Remarkably, on the R2R test unseen public leaderboard, our smallest model, MAGIC-S, with only 5% (11M) of the teacher's size, outperforms all previous methods under the same training data. Additionally, our largest model, MAGIC-L, surpasses the previous state-of-the-art by 5.84% in SPL and 3.18% in SR. Furthermore, a new dataset was collected and annotated from our living environments, where MAGIC-S demonstrated superior performance and real-time efficiency. Our code is publicly available on https://github.com/CrystalSixone/VLN-MAGIC.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2406.09798.pdf' target='_blank'>https://arxiv.org/pdf/2406.09798.pdf</a></span>   <span><a href='https://github.com/MrZihan/Sim2Real-VLN-3DFF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Shuqiang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09798">Sim-to-Real Transfer via 3D Feature Fields for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) enables the agent to navigate to a remote location in 3D environments following the natural language instruction. In this field, the agent is usually trained and evaluated in the navigation simulators, lacking effective approaches for sim-to-real transfer. The VLN agents with only a monocular camera exhibit extremely limited performance, while the mainstream VLN models trained with panoramic observation, perform better but are difficult to deploy on most monocular robots. For this case, we propose a sim-to-real transfer approach to endow the monocular robots with panoramic traversability perception and panoramic semantic understanding, thus smoothly transferring the high-performance panoramic VLN models to the common monocular robots. In this work, the semantic traversable map is proposed to predict agent-centric navigable waypoints, and the novel view representations of these navigable waypoints are predicted through the 3D feature fields. These methods broaden the limited field of view of the monocular robots and significantly improve navigation performance in the real world. Our VLN system outperforms previous SOTA monocular VLN methods in R2R-CE and RxR-CE benchmarks within the simulation environments and is also validated in real-world environments, providing a practical and high-performance solution for real-world VLN.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2404.10241.pdf' target='_blank'>https://arxiv.org/pdf/2404.10241.pdf</a></span>   <span><a href='https://github.com/CrystalSixone/VLN-GOAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuyi Wang, Zongtao He, Ronghao Dang, Mengjiao Shen, Chengju Liu, Qijun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10241">Vision-and-Language Navigation via Causal Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the pursuit of robust and generalizable environment perception and language understanding, the ubiquitous challenge of dataset bias continues to plague vision-and-language navigation (VLN) agents, hindering their performance in unseen environments. This paper introduces the generalized cross-modal causal transformer (GOAT), a pioneering solution rooted in the paradigm of causal inference. By delving into both observable and unobservable confounders within vision, language, and history, we propose the back-door and front-door adjustment causal learning (BACL and FACL) modules to promote unbiased learning by comprehensively mitigating potential spurious correlations. Additionally, to capture global confounder features, we propose a cross-modal feature pooling (CFP) module supervised by contrastive learning, which is also shown to be effective in improving cross-modal representations during pre-training. Extensive experiments across multiple VLN datasets (R2R, REVERIE, RxR, and SOON) underscore the superiority of our proposed method over previous state-of-the-art approaches. Code is available at https://github.com/CrystalSixone/VLN-GOAT.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2404.01943.pdf' target='_blank'>https://arxiv.org/pdf/2404.01943.pdf</a></span>   <span><a href='https://github.com/MrZihan/HNR-VLN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Junjie Hu, Ming Jiang, Shuqiang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01943">Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. At each navigation step, the agent selects from possible candidate locations and then makes the move. For better navigation planning, the lookahead exploration strategy aims to effectively evaluate the agent's next action by accurately anticipating the future environment of candidate locations. To this end, some existing works predict RGB images for future environments, while this strategy suffers from image distortion and high computational cost. To address these issues, we propose the pre-trained hierarchical neural radiance representation model (HNR) to produce multi-level semantic features for future environments, which are more robust and efficient than pixel-wise RGB reconstruction. Furthermore, with the predicted future environmental representations, our lookahead VLN model is able to construct the navigable future path tree and select the optimal path via efficient parallel evaluation. Extensive experiments on the VLN-CE datasets confirm the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2403.11541.pdf' target='_blank'>https://arxiv.org/pdf/2403.11541.pdf</a></span>   <span><a href='https://github.com/iCityLab/HSPR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming Xu, Zilong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11541">Hierarchical Spatial Proximity Reasoning for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most Vision-and-Language Navigation (VLN) algorithms are prone to making inaccurate decisions due to their lack of visual common sense and limited reasoning capabilities. To address this issue, we propose a Hierarchical Spatial Proximity Reasoning (HSPR) method. First, we introduce a scene understanding auxiliary task to help the agent build a knowledge base of hierarchical spatial proximity. This task utilizes panoramic views and object features to identify types of nodes and uncover the adjacency relationships between nodes, objects, and between nodes and objects. Second, we propose a multi-step reasoning navigation algorithm based on the hierarchical spatial proximity knowledge base, which continuously plans feasible paths to enhance exploration efficiency. Third, we introduce a residual fusion method to improve navigation decision accuracy. Finally, we validate our approach with experiments on publicly available datasets including REVERIE, SOON, R2R, and R4R. Our code is available at https://github.com/iCityLab/HSPR
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2403.07376.pdf' target='_blank'>https://arxiv.org/pdf/2403.07376.pdf</a></span>   <span><a href='https://github.com/expectorlin/NavCoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07376">NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the instruction, 2) selecting the candidate observation that best aligns with the imagination, and 3) determining the action based on the reasoning from the prior steps. Through constructing formalized labels for training, the LLM can learn to generate desired and reasonable chain-of-thought outputs for improving the action decision. Experimental results across various training settings and popular VLN benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room (R4R)) show the significant superiority of NavCoT over the direct action prediction variants. Through simple parameter-efficient finetuning, our NavCoT outperforms a recent GPT4-based approach with ~7% relative improvement on the R2R dataset. We believe that NavCoT will help unlock more task-adaptive and scalable LLM-based embodied agents, which are helpful for developing real-world robotics applications. Code is available at https://github.com/expectorlin/NavCoT.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2312.15820.pdf' target='_blank'>https://arxiv.org/pdf/2312.15820.pdf</a></span>   <span><a href='https://github.com/WebVLN/WebVLN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Chen, Dileepa Pitawela, Chongyang Zhao, Gengze Zhou, Hsiang-Ting Chen, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15820">WebVLN: Vision-and-Language Navigation on Websites</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) task aims to enable AI agents to accurately understand and follow natural language instructions to navigate through real-world environments, ultimately reaching specific target locations. We recognise a promising opportunity to extend VLN to a comparable navigation task that holds substantial significance in our daily lives, albeit within the virtual realm: navigating websites on the Internet. This paper proposes a new task named Vision-and-Language Navigation on Websites (WebVLN), where we use question-based instructions to train an agent, emulating how users naturally browse websites. Unlike the existing VLN task that only pays attention to vision and instruction (language), the WebVLN agent further considers underlying web-specific content like HTML, which could not be seen on the rendered web pages yet contains rich visual and textual information. Toward this goal, we contribute a dataset, WebVLN-v1, and introduce a novel approach called Website-aware VLN Network (WebVLN-Net), which is built upon the foundation of state-of-the-art VLN techniques. Experimental results show that WebVLN-Net outperforms current VLN and web-related navigation methods. We believe that the introduction of the new WebVLN task and its dataset will establish a new dimension within the VLN domain and contribute to the broader vision-and-language research community. The code is available at: https://github.com/WebVLN/WebVLN.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2311.13209.pdf' target='_blank'>https://arxiv.org/pdf/2311.13209.pdf</a></span>   <span><a href='https://github.com/Feliciaxyao/ICML2024-FSTTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Gao, Xuan Yao, Changsheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13209">Fast-Slow Test-Time Adaptation for Online Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to accurately comprehend natural language instructions and navigate to the target location is essential for an embodied agent. Such agents are typically required to execute user instructions in an online manner, leading us to explore the use of unlabeled test samples for effective online model adaptation. However, for online Vision-and-Language Navigation (VLN), due to the intrinsic nature of inter-sample online instruction execution and intra-sample multi-step action decision, frequent updates can result in drastic changes in model parameters, while occasional updates can make the model ill-equipped to handle dynamically changing environments. Therefore, we propose a Fast-Slow Test-Time Adaptation (FSTTA) approach for online VLN by performing joint decomposition-accumulation analysis for both gradients and parameters in a unified framework. Extensive experiments show that our method obtains impressive performance gains on four popular benchmarks. Code is available at https://github.com/Feliciaxyao/ICML2024-FSTTA.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2308.06735.pdf' target='_blank'>https://arxiv.org/pdf/2308.06735.pdf</a></span>   <span><a href='https://github.com/AirVLN/AirVLN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shubo Liu, Hongsheng Zhang, Yuankai Qi, Peng Wang, Yaning Zhang, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06735">AerialVLN: Vision-and-Language Navigation for UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently emerged Vision-and-Language Navigation (VLN) tasks have drawn significant attention in both computer vision and natural language processing communities. Existing VLN tasks are built for agents that navigate on the ground, either indoors or outdoors. However, many tasks require intelligent agents to carry out in the sky, such as UAV-based goods delivery, traffic/security patrol, and scenery tour, to name a few. Navigating in the sky is more complicated than on the ground because agents need to consider the flying height and more complex spatial relationship reasoning. To fill this gap and facilitate research in this field, we propose a new task named AerialVLN, which is UAV-based and towards outdoor environments. We develop a 3D simulator rendered by near-realistic pictures of 25 city-level scenarios. Our simulator supports continuous navigation, environment extension and configuration. We also proposed an extended baseline model based on the widely-used cross-modal-alignment (CMA) navigation methods. We find that there is still a significant gap between the baseline model and human performance, which suggests AerialVLN is a new challenging task. Dataset and code is available at https://github.com/AirVLN/AirVLN.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2307.12907.pdf' target='_blank'>https://arxiv.org/pdf/2307.12907.pdf</a></span>   <span><a href='https://github.com/MrZihan/GridMM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Shuqiang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12907">GridMM: Grid Memory Map for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. To represent the previously visited environment, most approaches for VLN implement memory using recurrent states, topological maps, or top-down semantic maps. In contrast to these approaches, we build the top-down egocentric and dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited environment. From a global perspective, historical observations are projected into a unified grid map in a top-down view, which can better represent the spatial relations of the environment. From a local perspective, we further propose an instruction relevance aggregation method to capture fine-grained visual clues in each grid region. Extensive experiments are conducted on both the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE dataset in the continuous environments, showing the superiority of our proposed method.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2307.11984.pdf' target='_blank'>https://arxiv.org/pdf/2307.11984.pdf</a></span>   <span><a href='https://github.com/JeremyLinky/YouTube-VLN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyang Lin, Peihao Chen, Diwei Huang, Thomas H. Li, Mingkui Tan, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11984">Learning Vision-and-Language Navigation from YouTube Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) requires an embodied agent to navigate in realistic 3D environments using natural language instructions. Existing VLN methods suffer from training on small-scale environments or unreasonable path-instruction datasets, limiting the generalization to unseen environments. There are massive house tour videos on YouTube, providing abundant real navigation experiences and layout information. However, these videos have not been explored for VLN before. In this paper, we propose to learn an agent from these videos by creating a large-scale dataset which comprises reasonable path-instruction pairs from house tour videos and pre-training the agent on it. To achieve this, we have to tackle the challenges of automatically constructing path-instruction pairs and exploiting real layout knowledge from raw and unlabeled videos. To address these, we first leverage an entropy-based method to construct the nodes of a path trajectory. Then, we propose an action-aware generator for generating instructions from unlabeled trajectories. Last, we devise a trajectory judgment pretext task to encourage the agent to mine the layout knowledge. Experimental results show that our method achieves state-of-the-art performance on two popular benchmarks (R2R and REVERIE). Code is available at https://github.com/JeremyLinky/YouTube-VLN
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2305.03602.pdf' target='_blank'>https://arxiv.org/pdf/2305.03602.pdf</a></span>   <span><a href='https://github.com/CrystalSixone/DSRG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuyi Wang, Zongtao He, Jiagui Tang, Ronghao Dang, Naijia Wang, Chengju Liu, Qijun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03602">A Dual Semantic-Aware Recurrent Global-Adaptive Network For Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) is a realistic but challenging task that requires an agent to locate the target region using verbal and visual cues. While significant advancements have been achieved recently, there are still two broad limitations: (1) The explicit information mining for significant guiding semantics concealed in both vision and language is still under-explored; (2) The previously structured map method provides the average historical appearance of visited nodes, while it ignores distinctive contributions of various images and potent information retention in the reasoning process. This work proposes a dual semantic-aware recurrent global-adaptive network (DSRG) to address the above problems. First, DSRG proposes an instruction-guidance linguistic module (IGL) and an appearance-semantics visual module (ASV) for boosting vision and language semantic learning respectively. For the memory mechanism, a global adaptive aggregation module (GAA) is devised for explicit panoramic observation fusion, and a recurrent memory fusion module (RMF) is introduced to supply implicit temporal hidden states. Extensive experimental results on the R2R and REVERIE datasets demonstrate that our method achieves better performance than existing methods. Code is available at https://github.com/CrystalSixone/DSRG.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2303.15796.pdf' target='_blank'>https://arxiv.org/pdf/2303.15796.pdf</a></span>   <span><a href='https://github.com/XiangyangLi20/KERM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyang Li, Zihan Wang, Jiahao Yang, Yaowei Wang, Shuqiang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15796">KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) is the task to enable an embodied agent to navigate to a remote location following the natural language instruction in real scenes. Most of the previous approaches utilize the entire features or object-centric features to represent navigable candidates. However, these representations are not efficient enough for an agent to perform actions to arrive the target location. As knowledge provides crucial information which is complementary to visible content, in this paper, we propose a Knowledge Enhanced Reasoning Model (KERM) to leverage knowledge to improve agent navigation ability. Specifically, we first retrieve facts (i.e., knowledge described by language descriptions) for the navigation views based on local regions from the constructed knowledge base. The retrieved facts range from properties of a single object (e.g., color, shape) to relationships between objects (e.g., action, spatial position), providing crucial information for VLN. We further present the KERM which contains the purification, fact-aware interaction, and instruction-guided aggregation modules to integrate visual, history, instruction, and fact features. The proposed KERM can automatically select and gather crucial and relevant cues, obtaining more accurate action prediction. Experimental results on the REVERIE, R2R, and SOON datasets demonstrate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2303.01032.pdf' target='_blank'>https://arxiv.org/pdf/2303.01032.pdf</a></span>   <span><a href='https://github.com/qizhust/esceme' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Zheng, Daqing Liu, Chaoyue Wang, Jing Zhang, Dadong Wang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01032">ESceme: Vision-and-Language Navigation with Episodic Scene Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) simulates a visual agent that follows natural-language navigation instructions in real-world scenes. Existing approaches have made enormous progress in navigation in new environments, such as beam search, pre-exploration, and dynamic or hierarchical history encoding. To balance generalization and efficiency, we resort to memorizing visited scenarios apart from the ongoing route while navigating. In this work, we introduce a mechanism of Episodic Scene memory (ESceme) for VLN that wakes an agent's memories of past visits when it enters the current scene. The episodic scene memory allows the agent to envision a bigger picture of the next prediction. This way, the agent learns to utilize dynamically updated information instead of merely adapting to the current observations. We provide a simple yet effective implementation of ESceme by enhancing the accessible views at each location and progressively completing the memory while navigating. We verify the superiority of ESceme on short-horizon (R2R), long-horizon (R4R), and vision-and-dialog (CVDN) VLN tasks. Our ESceme also wins first place on the CVDN leaderboard. Code is available: \url{https://github.com/qizhust/esceme}.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2212.04385.pdf' target='_blank'>https://arxiv.org/pdf/2212.04385.pdf</a></span>   <span><a href='https://github.com/MarSaKi/VLN-BEVBert' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong An, Yuankai Qi, Yangguang Li, Yan Huang, Liang Wang, Tieniu Tan, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.04385">BEVBert: Multimodal Map Pre-training for Language-guided Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale pre-training has shown promising results on the vision-and-language navigation (VLN) task. However, most existing pre-training methods employ discrete panoramas to learn visual-textual associations. This requires the model to implicitly correlate incomplete, duplicate observations within the panoramas, which may impair an agent's spatial understanding. Thus, we propose a new map-based pre-training paradigm that is spatial-aware for use in VLN. Concretely, we build a local metric map to explicitly aggregate incomplete observations and remove duplicates, while modeling navigation dependency in a global topological map. This hybrid design can balance the demand of VLN for both short-term reasoning and long-term planning. Then, based on the hybrid map, we devise a pre-training framework to learn a multimodal map representation, which enhances spatial-aware cross-modal reasoning thereby facilitating the language-guided navigation goal. Extensive experiments demonstrate the effectiveness of the map-based pre-training route for VLN, and the proposed method achieves state-of-the-art on four VLN benchmarks.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2509.10454.pdf' target='_blank'>https://arxiv.org/pdf/2509.10454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Yin, Haoyu Wei, Xiuwei Xu, Wenxuan Guo, Jie Zhou, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10454">GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a training-free framework for vision-and-language navigation (VLN). Existing zero-shot VLN methods are mainly designed for discrete environments or involve unsupervised training in continuous simulator environments, which makes it challenging to generalize and deploy them in real-world scenarios. To achieve a training-free framework in continuous environments, our framework formulates navigation guidance as graph constraint optimization by decomposing instructions into explicit spatial constraints. The constraint-driven paradigm decodes spatial semantics through constraint solving, enabling zero-shot adaptation to unseen environments. Specifically, we construct a spatial constraint library covering all types of spatial relationship mentioned in VLN instructions. The human instruction is decomposed into a directed acyclic graph, with waypoint nodes, object nodes and edges, which are used as queries to retrieve the library to build the graph constraints. The graph constraint optimization is solved by the constraint solver to determine the positions of waypoints, obtaining the robot's navigation path and final goal. To handle cases of no solution or multiple solutions, we construct a navigation tree and the backtracking mechanism. Extensive experiments on standard benchmarks demonstrate significant improvements in success rate and navigation efficiency compared to state-of-the-art zero-shot VLN methods. We further conduct real-world experiments to show that our framework can effectively generalize to new environments and instruction sets, paving the way for a more robust and autonomous navigation framework.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2404.10054.pdf' target='_blank'>https://arxiv.org/pdf/2404.10054.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niyati Rawal, Roberto Bigazzi, Lorenzo Baraldi, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10054">AIGeN: An Adversarial Approach for Instruction Generation in VLN</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the last few years, the research interest in Vision-and-Language Navigation (VLN) has grown significantly. VLN is a challenging task that involves an agent following human instructions and navigating in a previously unknown environment to reach a specified goal. Recent work in literature focuses on different ways to augment the available datasets of instructions for improving navigation performance by exploiting synthetic training data. In this work, we propose AIGeN, a novel architecture inspired by Generative Adversarial Networks (GANs) that produces meaningful and well-formed synthetic instructions to improve navigation agents' performance. The model is composed of a Transformer decoder (GPT-2) and a Transformer encoder (BERT). During the training phase, the decoder generates sentences for a sequence of images describing the agent's path to a particular point while the encoder discriminates between real and fake instructions. Experimentally, we evaluate the quality of the generated instructions and perform extensive ablation studies. Additionally, we generate synthetic instructions for 217K trajectories using AIGeN on Habitat-Matterport 3D Dataset (HM3D) and show an improvement in the performance of an off-the-shelf VLN method. The validation analysis of our proposal is conducted on REVERIE and R2R and highlights the promising aspects of our proposal, achieving state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2410.08500.pdf' target='_blank'>https://arxiv.org/pdf/2410.08500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunpeng Gao, Zhigang Wang, Pengfei Han, Linglin Jing, Dong Wang, Bin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08500">Exploring Spatial Representation to Enhance LLM Reasoning in Aerial Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerial Vision-and-Language Navigation (VLN) is a novel task enabling Unmanned Aerial Vehicles (UAVs) to navigate in outdoor environments through natural language instructions and visual cues. However, it remains challenging due to the complex spatial relationships in aerial scenes.In this paper, we propose a training-free, zero-shot framework for aerial VLN tasks, where the large language model (LLM) is leveraged as the agent for action prediction. Specifically, we develop a novel Semantic-Topo-Metric Representation (STMR) to enhance the spatial reasoning capabilities of LLMs. This is achieved by extracting and projecting instruction-related semantic masks onto a top-down map, which presents spatial and topological information about surrounding landmarks and grows during the navigation process. At each step, a local map centered at the UAV is extracted from the growing top-down map, and transformed into a ma trix representation with distance metrics, serving as the text prompt to LLM for action prediction in response to the given instruction. Experiments conducted in real and simulation environments have proved the effectiveness and robustness of our method, achieving absolute success rate improvements of 26.8% and 5.8% over current state-of-the-art methods on simple and complex navigation tasks, respectively. The dataset and code will be released soon.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2306.10322.pdf' target='_blank'>https://arxiv.org/pdf/2306.10322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiwen Liang, Liang Ma, Shanshan Guo, Jianhua Han, Hang Xu, Shikui Ma, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10322">CorNav: Autonomous Agent with Self-Corrected Planning for Zero-Shot Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and following natural language instructions while navigating through complex, real-world environments poses a significant challenge for general-purpose robots. These environments often include obstacles and pedestrians, making it essential for autonomous agents to possess the capability of self-corrected planning to adjust their actions based on feedback from the surroundings. However, the majority of existing vision-and-language navigation (VLN) methods primarily operate in less realistic simulator settings and do not incorporate environmental feedback into their decision-making processes. To address this gap, we introduce a novel zero-shot framework called CorNav, utilizing a large language model for decision-making and comprising two key components: 1) incorporating environmental feedback for refining future plans and adjusting its actions, and 2) multiple domain experts for parsing instructions, scene understanding, and refining predicted actions. In addition to the framework, we develop a 3D simulator that renders realistic scenarios using Unreal Engine 5. To evaluate the effectiveness and generalization of navigation agents in a zero-shot multi-task setting, we create a benchmark called NavBench. Extensive experiments demonstrate that CorNav consistently outperforms all baselines by a significant margin across all tasks. On average, CorNav achieves a success rate of 28.1\%, surpassing the best baseline's performance of 20.5\%.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2509.24910.pdf' target='_blank'>https://arxiv.org/pdf/2509.24910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songze Li, Zun Wang, Gengze Zhou, Jialu Li, Xiangyu Zeng, Limin Wang, Yu Qiao, Qi Wu, Mohit Bansal, Yi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24910">Learning Goal-Oriented Language-Guided Navigation with Self-Improving Demonstrations at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Goal-oriented language-guided navigation requires robust exploration capabilities for agents to navigate to specified goals in unknown environments without step-by-step instructions. Existing methods tend to exclusively utilize shortest-path trajectories, lacking effective exploration priors for training navigation agents. To address the above challenges, we present SID, a goal-oriented language-guided navigation learning approach with Self-Improving Demonstrations. Specifically, SID learns an initial agent on the shortest-path data sampled from environments and then leverages this agent to generate novel exploration trajectories. The novel rollouts provide demonstrations with stronger exploration strategies to train a better agent, which in turn produces higher-quality agent demonstrations for the next round of training. We show that this iterative self-improving pipeline readily scales to new environments, and the resulting demonstrations can be transferred across a variety of language-guided navigation tasks, elevating the performance ceiling in diverse goal-oriented navigation tasks. Extensive experiments demonstrate that SID significantly boosts the exploration capabilities and generalization of navigation agents. The resulting agent achieves new state-of-the-art performance on goal-oriented language-guided navigation tasks, including REVERIE, SOON, notably achieving a 50.9% success rate on the unseen validation splits of SOON, surpassing the prior leading approaches by a margin of 13.9%.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2307.15644.pdf' target='_blank'>https://arxiv.org/pdf/2307.15644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, Yu Qiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15644">Scaling Data Generation in Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research in language-guided visual navigation has demonstrated a significant demand for the diversity of traversable environments and the quantity of supervision for training generalizable agents. To tackle the common data scarcity issue in existing vision-and-language navigation datasets, we propose an effective paradigm for generating large-scale data for learning, which applies 1200+ photo-realistic environments from HM3D and Gibson datasets and synthesizes 4.9 million instruction trajectory pairs using fully-accessible resources on the web. Importantly, we investigate the influence of each component in this paradigm on the agent's performance and study how to adequately apply the augmented data to pre-train and fine-tune an agent. Thanks to our large-scale dataset, the performance of an existing agent can be pushed up (+11% absolute with regard to previous SoTA) to a significantly new best of 80% single-run success rate on the R2R test split by simple imitation learning. The long-lasting generalization gap between navigating in seen and unseen environments is also reduced to less than 1% (versus 8% in the previous best method). Moreover, our paradigm also facilitates different models to achieve new state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuous environments.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2502.13451.pdf' target='_blank'>https://arxiv.org/pdf/2502.13451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingfeng Zhang, Xiaoshuai Hao, Qinwen Xu, Qiang Zhang, Xinyao Zhang, Pengwei Wang, Jing Zhang, Zhongyuan Wang, Shanghang Zhang, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13451">MapNav: A Novel Memory Representation via Annotated Semantic Maps for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) is a key task in Embodied AI, requiring agents to navigate diverse and unseen environments while following natural language instructions. Traditional approaches rely heavily on historical observations as spatio-temporal contexts for decision making, leading to significant storage and computational overhead. In this paper, we introduce MapNav, a novel end-to-end VLN model that leverages Annotated Semantic Map (ASM) to replace historical frames. Specifically, our approach constructs a top-down semantic map at the start of each episode and update it at each timestep, allowing for precise object mapping and structured navigation information. Then, we enhance this map with explicit textual labels for key regions, transforming abstract semantics into clear navigation cues and generate our ASM. MapNav agent using the constructed ASM as input, and use the powerful end-to-end capabilities of VLM to empower VLN. Extensive experiments demonstrate that MapNav achieves state-of-the-art (SOTA) performance in both simulated and real-world environments, validating the effectiveness of our method. Moreover, we will release our ASM generation source code and dataset to ensure reproducibility, contributing valuable resources to the field. We believe that our proposed MapNav can be used as a new memory representation method in VLN, paving the way for future research in this field.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2307.13368.pdf' target='_blank'>https://arxiv.org/pdf/2307.13368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitian Zeng, Xiaohan Wang, Wenguan Wang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13368">Kefa: A Knowledge Enhanced and Fine-grained Aligned Speaker for Navigation Instruction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel speaker model \textsc{Kefa} for navigation instruction generation. The existing speaker models in Vision-and-Language Navigation suffer from the large domain gap of vision features between different environments and insufficient temporal grounding capability. To address the challenges, we propose a Knowledge Refinement Module to enhance the feature representation with external knowledge facts, and an Adaptive Temporal Alignment method to enforce fine-grained alignment between the generated instructions and the observation sequences. Moreover, we propose a new metric SPICE-D for navigation instruction evaluation, which is aware of the correctness of direction phrases. The experimental results on R2R and UrbanWalk datasets show that the proposed KEFA speaker achieves state-of-the-art instruction generation performance for both indoor and outdoor scenes.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2303.08409.pdf' target='_blank'>https://arxiv.org/pdf/2303.08409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohan Wang, Wenguan Wang, Jiayi Shao, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08409">Lana: A Language-Capable Navigator for Instruction Following and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, visual-language navigation (VLN) -- entailing robot agents to follow navigation instructions -- has shown great advance. However, existing literature put most emphasis on interpreting instructions into actions, only delivering "dumb" wayfinding agents. In this article, we devise LANA, a language-capable navigation agent which is able to not only execute human-written navigation commands, but also provide route descriptions to humans. This is achieved by simultaneously learning instruction following and generation with only one single model. More specifically, two encoders, respectively for route and language encoding, are built and shared by two decoders, respectively, for action prediction and instruction generation, so as to exploit cross-task knowledge and capture task-specific characteristics. Throughout pretraining and fine-tuning, both instruction following and generation are set as optimization objectives. We empirically verify that, compared with recent advanced task-specific solutions, LANA attains better performances on both instruction following and route description, with nearly half complexity. In addition, endowed with language generation capability, LANA can explain to humans its behaviors and assist human's wayfinding. This work is expected to foster future efforts towards building more trustworthy and socially-intelligent navigation robots.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2404.01994.pdf' target='_blank'>https://arxiv.org/pdf/2404.01994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengfei Du, Binhao Wu, Jiwen Zhang, Zhihao Fan, Zejun Li, Ruipu Luo, Xuanjing Huang, Zhongyu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01994">DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction. For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history. Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective. Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision. To address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal contrastive learning. This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making. Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation level according to their semantic correlations. We also reconstruct a dual-level instruction for adaptation to the dual-level alignment. As the training signals for pre-fusion alignment are extremely limited, self-supervised contrastive learning strategies are employed to enforce the matching between different modalities. Our approach seamlessly integrates with the majority of existing models, resulting in improved navigation performance on various VLN benchmarks, including R2R, R4R, RxR and CVDN.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2507.13019.pdf' target='_blank'>https://arxiv.org/pdf/2507.13019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuyi Wang, Xinyuan Xia, Hui Zhao, Hanqing Wang, Tai Wang, Yilun Chen, Chengju Liu, Qijun Chen, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13019">Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at https://crystalsixone.github.io/vln_pe.github.io/.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2507.13019.pdf' target='_blank'>https://arxiv.org/pdf/2507.13019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuyi Wang, Xinyuan Xia, Hui Zhao, Hanqing Wang, Tai Wang, Yilun Chen, Chengju Liu, Qijun Chen, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13019">Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at https://crystalsixone.github.io/vln_pe.github.io/.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2507.05240.pdf' target='_blank'>https://arxiv.org/pdf/2507.05240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Wei, Chenyang Wan, Xiqian Yu, Tai Wang, Yuqiang Yang, Xiaohan Mao, Chenming Zhu, Wenzhe Cai, Hanqing Wang, Yilun Chen, Xihui Liu, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05240">StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: \href{https://streamvln.github.io/}{https://streamvln.github.io/}.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2412.06413.pdf' target='_blank'>https://arxiv.org/pdf/2412.06413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhong, Rui Zhang, Zihao Zhang, Shuo Wang, Chuan Fang, Xishan Zhang, Jiaming Guo, Shaohui Peng, Di Huang, Yanyang Yan, Xing Hu, Qi Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06413">World-Consistent Data Generation for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) is a challenging task that requires an agent to navigate through photorealistic environments following natural-language instructions. One main obstacle existing in VLN is data scarcity, leading to poor generalization performance over unseen environments. Though data argumentation is a promising way for scaling up the dataset, how to generate VLN data both diverse and world-consistent remains problematic. To cope with this issue, we propose the world-consistent data generation (WCGEN), an efficacious data-augmentation framework satisfying both diversity and world-consistency, aimed at enhancing the generalization of agents to novel environments. Roughly, our framework consists of two stages, the trajectory stage which leverages a point-cloud based technique to ensure spatial coherency among viewpoints, and the viewpoint stage which adopts a novel angle synthesis method to guarantee spatial and wraparound consistency within the entire observation. By accurately predicting viewpoint changes with 3D knowledge, our approach maintains the world-consistency during the generation procedure. Experiments on a wide range of datasets verify the effectiveness of our method, demonstrating that our data augmentation strategy enables agents to achieve new state-of-the-art results on all navigation tasks, and is capable of enhancing the VLN agents' generalization ability to unseen environments.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2506.15757.pdf' target='_blank'>https://arxiv.org/pdf/2506.15757.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoyu Wang, Tong Yu, Junda Wu, Yao Liu, Julian McAuley, Lina Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15757">Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Language Navigation (VLN) is a fundamental task within the field of Embodied AI, focusing on the ability of agents to navigate complex environments based on natural language instructions. Despite the progress made by existing methods, these methods often present some common challenges. First, they rely on pre-trained backbone models for visual perception, which struggle with the dynamic viewpoints in VLN scenarios. Second, the performance is limited when using pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN domain knowledge. Third, while fine-tuning LLMs and VLMs can improve results, their computational costs are higher than those without fine-tuning. To address these limitations, we propose Weakly-supervised Partial Contrastive Learning (WPCL), a method that enhances an agent's ability to identify objects from dynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM knowledge into the perception process, without requiring VLM fine-tuning. Our method enhances the agent's ability to interpret and respond to environmental cues while ensuring computational efficiency. Experimental results have shown that our method outperforms the baseline methods on multiple benchmarks, which validate the effectiveness, robustness and generalizability of our method.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2505.02388.pdf' target='_blank'>https://arxiv.org/pdf/2505.02388.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huangyue Yu, Baoxiong Jia, Yixin Chen, Yandan Yang, Puhao Li, Rongpeng Su, Jiaxin Li, Qing Li, Wei Liang, Song-Chun Zhu, Tengyu Liu, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02388">MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI (EAI) research requires high-quality, diverse 3D scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. Achieving these quality standards, however, necessitates the precise replication of real-world object diversity. Existing datasets demonstrate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. To scalably produce realistic and interactive 3D scenes, we first present MetaScenes, a large-scale, simulatable 3D scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3D scenes. We further propose two benchmarks to evaluate MetaScenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (VLN) to validate cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for EAI research. Project website: https://meta-scenes.github.io/.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2412.04453.pdf' target='_blank'>https://arxiv.org/pdf/2412.04453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem BÄ±yÄ±k, Hongxu Yin, Sifei Liu, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04453">NaVILA: Legged Robot Vision-Language-Action Model for Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes to solve the problem of Vision-and-Language Navigation with legged robots, which not only provides a flexible way for humans to command but also allows the robot to navigate through more challenging and cluttered scenes. However, it is non-trivial to translate human language instructions all the way to low-level leg joint actions. We propose NaVILA, a 2-level framework that unifies a Vision-Language-Action model (VLA) with locomotion skills. Instead of directly predicting low-level actions from VLA, NaVILA first generates mid-level actions with spatial information in the form of language, (e.g., "moving forward 75cm"), which serves as an input for a visual locomotion RL policy for execution. NaVILA substantially improves previous approaches on existing benchmarks. The same advantages are demonstrated in our newly developed benchmarks with IsaacLab, featuring more realistic scenes, low-level controls, and real-world robot experiments. We show more results at https://navila-bot.github.io/
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2307.12335.pdf' target='_blank'>https://arxiv.org/pdf/2307.12335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yicong Hong, Yang Zhou, Ruiyi Zhang, Franck Dernoncourt, Trung Bui, Stephen Gould, Hao Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12335">Learning Navigational Visual Representations with Semantic Map Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Being able to perceive the semantics and the spatial structure of the environment is essential for visual navigation of a household robot. However, most existing works only employ visual backbones pre-trained either with independent images for classification or with self-supervised learning methods to adapt to the indoor navigation domain, neglecting the spatial relationships that are essential to the learning of navigation. Inspired by the behavior that humans naturally build semantically and spatially meaningful cognitive maps in their brains during navigation, in this paper, we propose a novel navigational-specific visual representation learning method by contrasting the agent's egocentric views and semantic maps (Ego$^2$-Map). We apply the visual transformer as the backbone encoder and train the model with data collected from the large-scale Habitat-Matterport3D environments. Ego$^2$-Map learning transfers the compact and rich information from a map, such as objects, structure and transition, to the agent's egocentric representations for navigation. Experiments show that agents using our learned representations on object-goal navigation outperform recent visual pre-training methods. Moreover, our representations significantly improve vision-and-language navigation in continuous environments for both high-level and low-level action spaces, achieving new state-of-the-art results of 47% SR and 41% SPL on the test server.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2503.13966.pdf' target='_blank'>https://arxiv.org/pdf/2503.13966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqi Zhang, Yanyuan Qiao, Qunbo Wang, Longteng Guo, Zhihua Wei, Jing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13966">FlexVLN: Flexible Adaptation for Diverse Vision-and-Language Navigation Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The aspiration of the Vision-and-Language Navigation (VLN) task has long been to develop an embodied agent with robust adaptability, capable of seamlessly transferring its navigation capabilities across various tasks. Despite remarkable advancements in recent years, most methods necessitate dataset-specific training, thereby lacking the capability to generalize across diverse datasets encompassing distinct types of instructions. Large language models (LLMs) have demonstrated exceptional reasoning and generalization abilities, exhibiting immense potential in robot action planning. In this paper, we propose FlexVLN, an innovative hierarchical approach to VLN that integrates the fundamental navigation ability of a supervised-learning-based Instruction Follower with the robust generalization ability of the LLM Planner, enabling effective generalization across diverse VLN datasets. Moreover, a verification mechanism and a multi-model integration mechanism are proposed to mitigate potential hallucinations by the LLM Planner and enhance execution accuracy of the Instruction Follower. We take REVERIE, SOON, and CVDN-target as out-of-domain datasets for assessing generalization ability. The generalization performance of FlexVLN surpasses that of all the previous methods to a large extent.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2507.10894.pdf' target='_blank'>https://arxiv.org/pdf/2507.10894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongtao He, Liuyi Wang, Lu Chen, Chengju Liu, Qijun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10894">NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-guided navigation is a cornerstone of embodied AI, enabling agents to interpret language instructions and navigate complex environments. However, expert-provided instructions are limited in quantity, while synthesized annotations often lack quality, making them insufficient for large-scale research. To address this, we propose NavComposer, a novel framework for automatically generating high-quality navigation instructions. NavComposer explicitly decomposes semantic entities such as actions, scenes, and objects, and recomposes them into natural language instructions. Its modular architecture allows flexible integration of state-of-the-art techniques, while the explicit use of semantic entities enhances both the richness and accuracy of instructions. Moreover, it operates in a data-agnostic manner, supporting adaptation to diverse navigation trajectories without domain-specific training. Complementing NavComposer, we introduce NavInstrCritic, a comprehensive annotation-free evaluation system that assesses navigation instructions on three dimensions: contrastive matching, semantic consistency, and linguistic diversity. NavInstrCritic provides a holistic evaluation of instruction quality, addressing limitations of traditional metrics that rely heavily on expert annotations. By decoupling instruction generation and evaluation from specific navigation agents, our method enables more scalable and generalizable research. Extensive experiments provide direct and practical evidence for the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2403.03405.pdf' target='_blank'>https://arxiv.org/pdf/2403.03405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuyi Wang, Zongtao He, Ronghao Dang, Huiyi Chen, Chengju Liu, Qijun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03405">Causality-based Cross-Modal Representation Learning for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) has gained significant research interest in recent years due to its potential applications in real-world scenarios. However, existing VLN methods struggle with the issue of spurious associations, resulting in poor generalization with a significant performance gap between seen and unseen environments. In this paper, we tackle this challenge by proposing a unified framework CausalVLN based on the causal learning paradigm to train a robust navigator capable of learning unbiased feature representations. Specifically, we establish reasonable assumptions about confounders for vision and language in VLN using the structured causal model (SCM). Building upon this, we propose an iterative backdoor-based representation learning (IBRL) method that allows for the adaptive and effective intervention on confounders. Furthermore, we introduce the visual and linguistic backdoor causal encoders to enable unbiased feature expression for multi-modalities during training and validation, enhancing the agent's capability to generalize across different environments. Experiments on three VLN datasets (R2R, RxR, and REVERIE) showcase the superiority of our proposed method over previous state-of-the-art approaches. Moreover, detailed visualization analysis demonstrates the effectiveness of CausalVLN in significantly narrowing down the performance gap between seen and unseen environments, underscoring its strong generalization capability.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2305.11918.pdf' target='_blank'>https://arxiv.org/pdf/2305.11918.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuyi Wang, Chengju Liu, Zongtao He, Shu Li, Qingqing Yan, Huiyi Chen, Qijun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11918">PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) is a crucial but challenging cross-modal navigation task. One powerful technique to enhance the generalization performance in VLN is the use of an independent speaker model to provide pseudo instructions for data augmentation. However, current speaker models based on Long-Short Term Memory (LSTM) lack the ability to attend to features relevant at different locations and time steps. To address this, we propose a novel progress-aware spatio-temporal transformer speaker (PASTS) model that uses the transformer as the core of the network. PASTS uses a spatio-temporal encoder to fuse panoramic representations and encode intermediate connections through steps. Besides, to avoid the misalignment problem that could result in incorrect supervision, a speaker progress monitor (SPM) is proposed to enable the model to estimate the progress of instruction generation and facilitate more fine-grained caption results. Additionally, a multifeature dropout (MFD) strategy is introduced to alleviate overfitting. The proposed PASTS is flexible to be combined with existing VLN models. The experimental results demonstrate that PASTS outperforms all existing speaker models and successfully improves the performance of previous VLN models, achieving state-of-the-art performance on the standard Room-to-Room (R2R) dataset.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2303.01396.pdf' target='_blank'>https://arxiv.org/pdf/2303.01396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongtao He, Liuyi Wang, Shu Li, Qingqing Yan, Chengju Liu, Qijun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01396">MLANet: Multi-Level Attention Network with Sub-instruction for Continuous Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) aims to develop intelligent agents to navigate in unseen environments only through language and vision supervision. In the recently proposed continuous settings (continuous VLN), the agent must act in a free 3D space and faces tougher challenges like real-time execution, complex instruction understanding, and long action sequence prediction. For a better performance in continuous VLN, we design a multi-level instruction understanding procedure and propose a novel model, Multi-Level Attention Network (MLANet). The first step of MLANet is to generate sub-instructions efficiently. We design a Fast Sub-instruction Algorithm (FSA) to segment the raw instruction into sub-instructions and generate a new sub-instruction dataset named ``FSASub". FSA is annotation-free and faster than the current method by 70 times, thus fitting the real-time requirement in continuous VLN. To solve the complex instruction understanding problem, MLANet needs a global perception of the instruction and observations. We propose a Multi-Level Attention (MLA) module to fuse vision, low-level semantics, and high-level semantics, which produce features containing a dynamic and global comprehension of the task. MLA also mitigates the adverse effects of noise words, thus ensuring a robust understanding of the instruction. To correctly predict actions in long trajectories, MLANet needs to focus on what sub-instruction is being executed every step. We propose a Peak Attention Loss (PAL) to improve the flexible and adaptive selection of the current sub-instruction. PAL benefits the navigation agent by concentrating its attention on the local information, thus helping the agent predict the most appropriate actions. We train and test MLANet in the standard benchmark. Experiment results show MLANet outperforms baselines by a significant margin.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2406.19236.pdf' target='_blank'>https://arxiv.org/pdf/2406.19236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heng Li, Minghan Li, Zhi-Qi Cheng, Yifei Dong, Yuxuan Zhou, Jun-Yan He, Qi Dai, Teruko Mitamura, Alexander G. Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19236">Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) aims to develop embodied agents that navigate based on human instructions. However, current VLN frameworks often rely on static environments and optimal expert supervision, limiting their real-world applicability. To address this, we introduce Human-Aware Vision-and-Language Navigation (HA-VLN), extending traditional VLN by incorporating dynamic human activities and relaxing key assumptions. We propose the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R) dataset, extending R2R with human activity descriptions. To tackle HA-VLN challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and Non-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing cross-modal fusion and diverse training strategies for effective navigation in dynamic human environments. A comprehensive evaluation, including metrics considering human activities, and systematic analysis of HA-VLN's unique challenges, underscores the need for further research to enhance HA-VLN agents' real-world robustness and adaptability. Ultimately, this work provides benchmarks and insights for future research on embodied AI and Sim2Real transfer, paving the way for more realistic and applicable VLN systems in human-populated environments.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2409.18800.pdf' target='_blank'>https://arxiv.org/pdf/2409.18800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyou Zhu, Yanyuan Qiao, Siqi Zhang, Xingjian He, Qi Wu, Jing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18800">MiniVLN: Efficient Vision-and-Language Navigation by Progressive Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Embodied Artificial Intelligence (Embodied AI) has advanced rapidly, yet the increasing size of models conflicts with the limited computational capabilities of Embodied AI platforms. To address this challenge, we aim to achieve both high model performance and practical deployability. Specifically, we focus on Vision-and-Language Navigation (VLN), a core task in Embodied AI. This paper introduces a two-stage knowledge distillation framework, producing a student model, MiniVLN, and showcasing the significant potential of distillation techniques in developing lightweight models. The proposed method aims to capture fine-grained knowledge during the pretraining phase and navigation-specific knowledge during the fine-tuning phase. Our findings indicate that the two-stage distillation approach is more effective in narrowing the performance gap between the teacher model and the student model compared to single-stage distillation. On the public R2R and REVERIE benchmarks, MiniVLN achieves performance on par with the teacher model while having only about 12% of the teacher model's parameter count.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2403.05770.pdf' target='_blank'>https://arxiv.org/pdf/2403.05770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingqian Lin, Yanxin Long, Yi Zhu, Fengda Zhu, Xiaodan Liang, Qixiang Ye, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05770">Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) asks an agent to follow a given language instruction to navigate through a real 3D environment. Despite significant advances, conventional VLN agents are trained typically under disturbance-free environments and may easily fail in real-world scenarios, since they are unaware of how to deal with various possible disturbances, such as sudden obstacles or human interruptions, which widely exist and may usually cause an unexpected route deviation. In this paper, we present a model-agnostic training paradigm, called Progressive Perturbation-aware Contrastive Learning (PROPER) to enhance the generalization ability of existing VLN agents, by requiring them to learn towards deviation-robust navigation. Specifically, a simple yet effective path perturbation scheme is introduced to implement the route deviation, with which the agent is required to still navigate successfully following the original instruction. Since directly enforcing the agent to learn perturbed trajectories may lead to inefficient training, a progressively perturbed trajectory augmentation strategy is designed, where the agent can self-adaptively learn to navigate under perturbation with the improvement of its navigation performance for each specific trajectory. For encouraging the agent to well capture the difference brought by perturbation, a perturbation-aware contrastive learning mechanism is further developed by contrasting perturbation-free trajectory encodings and perturbation-based counterparts. Extensive experiments on R2R show that PROPER can benefit multiple VLN baselines in perturbation-free scenarios. We further collect the perturbed path data to construct an introspection subset based on the R2R, called Path-Perturbed R2R (PP-R2R). The results on PP-R2R show unsatisfying robustness of popular VLN agents and the capability of PROPER in improving the navigation robustness.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2508.13446.pdf' target='_blank'>https://arxiv.org/pdf/2508.13446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Catherine Glossop, William Chen, Arjun Bhorkar, Dhruv Shah, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13446">CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalist robots should be able to understand and follow user instructions, but current vision-language-action (VLA) models struggle with following fine-grained commands despite providing a powerful architecture for mapping open-vocabulary natural language instructions to robot actions. One cause for this is a lack of semantic diversity and language grounding in existing robot datasets and, specifically, a lack of fine-grained task diversity for similar observations. To address this, we present a novel method to augment existing robot datasets by leveraging vision language models to create counterfactual labels. Our method improves the language-following capabilities of VLAs by increasing the diversity and granularity of language grounding for robot datasets by generating counterfactual language and actions. We evaluate the resulting model's ability to follow language instructions, ranging from simple object-centric commands to complex referential tasks, by conducting visual language navigation experiments in 3 different indoor and outdoor environments. Our experiments demonstrate that counterfactual relabeling, without any additional data collection, significantly improves instruction-following in VLA policies, making them competitive with state-of-the-art methods and increasing success rate by 27% on navigation tasks.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2412.08591.pdf' target='_blank'>https://arxiv.org/pdf/2412.08591.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingfei Han, Liang Ma, Kamila Zhumakhanova, Ekaterina Radionova, Jingyi Zhang, Xiaojun Chang, Xiaodan Liang, Ivan Laptev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08591">RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) suffers from the limited diversity and scale of training data, primarily constrained by the manual curation of existing simulators. To address this, we introduce RoomTour3D, a video-instruction dataset derived from web-based room tour videos that capture real-world indoor spaces and human walking demonstrations. Unlike existing VLN datasets, RoomTour3D leverages the scale and diversity of online videos to generate open-ended human walking trajectories and open-world navigable instructions. To compensate for the lack of navigation data in online videos, we perform 3D reconstruction and obtain 3D trajectories of walking paths augmented with additional information on the room types, object locations and 3D shape of surrounding scenes. Our dataset includes $\sim$100K open-ended description-enriched trajectories with $\sim$200K instructions, and 17K action-enriched trajectories from 1847 room tour environments. We demonstrate experimentally that RoomTour3D enables significant improvements across multiple VLN tasks including CVDN, SOON, R2R, and REVERIE. Moreover, RoomTour3D facilitates the development of trainable zero-shot VLN agents, showcasing the potential and challenges of advancing towards open-world navigation.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2412.01857.pdf' target='_blank'>https://arxiv.org/pdf/2412.01857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyuan Pan, Yunzhe Xu, Zhe Liu, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01857">Planning from Imagination: Episodic Simulation and Episodic Memory for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans navigate unfamiliar environments using episodic simulation and episodic memory, which facilitate a deeper understanding of the complex relationships between environments and objects. Developing an imaginative memory system inspired by human mechanisms can enhance the navigation performance of embodied agents in unseen environments. However, existing Vision-and-Language Navigation (VLN) agents lack a memory mechanism of this kind. To address this, we propose a novel architecture that equips agents with a reality-imagination hybrid memory system. This system enables agents to maintain and expand their memory through both imaginative mechanisms and navigation actions. Additionally, we design tailored pre-training tasks to develop the agent's imaginative capabilities. Our agent can imagine high-fidelity RGB images for future scenes, achieving state-of-the-art result in Success rate weighted by Path Length (SPL).
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2408.11051.pdf' target='_blank'>https://arxiv.org/pdf/2408.11051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunzhe Xu, Yiyuan Pan, Zhe Liu, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11051">FLAME: Learning to Navigate with Multimodal LLM in Urban Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated potential in Vision-and-Language Navigation (VLN) tasks, yet current applications face challenges. While LLMs excel in general conversation scenarios, they struggle with specialized navigation tasks, yielding suboptimal performance compared to specialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied Agent), a novel Multimodal LLM-based agent and architecture designed for urban VLN tasks that efficiently handles multiple observations. Our approach implements a three-phase tuning technique for effective adaptation to navigation tasks, including single perception tuning for street view description, multiple perception tuning for route summarization, and end-to-end training on VLN datasets. The augmented datasets are synthesized automatically. Experimental results demonstrate FLAME's superiority over existing methods, surpassing state-of-the-art methods by a 7.3% increase in task completion on Touchdown dataset. This work showcases the potential of Multimodal LLMs (MLLMs) in complex navigation tasks, representing an advancement towards applications of MLLMs in the field of embodied intelligence.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2309.11382.pdf' target='_blank'>https://arxiv.org/pdf/2309.11382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxing Long, Xiaoqi Li, Wenzhe Cai, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.11382">Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual language navigation (VLN) is an embodied task demanding a wide range of skills encompassing understanding, perception, and planning. For such a multifaceted challenge, previous VLN methods totally rely on one model's own thinking to make predictions within one round. However, existing models, even the most advanced large language model GPT4, still struggle with dealing with multiple tasks by single-round self-thinking. In this work, drawing inspiration from the expert consultation meeting, we introduce a novel zero-shot VLN framework. Within this framework, large models possessing distinct abilities are served as domain experts. Our proposed navigation agent, namely DiscussNav, can actively discuss with these experts to collect essential information before moving at every step. These discussions cover critical navigation subtasks like instruction understanding, environment perception, and completion estimation. Through comprehensive experiments, we demonstrate that discussions with domain experts can effectively facilitate navigation by perceiving instruction-relevant information, correcting inadvertent errors, and sifting through in-consistent movement decisions. The performances on the representative VLN task R2R show that our method surpasses the leading zero-shot VLN model by a large margin on all metrics. Additionally, real-robot experiments display the obvious advantages of our method over single-round self-thinking.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2509.12129.pdf' target='_blank'>https://arxiv.org/pdf/2509.12129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhao Zhang, Anqi Li, Yunpeng Qi, Minghan Li, Jiahang Liu, Shaoan Wang, Haoran Liu, Gengze Zhou, Yuze Wu, Xingxing Li, Yuxin Fan, Wenjun Li, Zhibo Chen, Fei Gao, Qi Wu, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12129">Embodied Navigation Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigation is a fundamental capability in embodied AI, representing the intelligence required to perceive and interact within physical environments following language instructions. Despite significant progress in large Vision-Language Models (VLMs), which exhibit remarkable zero-shot performance on general vision-language tasks, their generalization ability in embodied navigation remains largely confined to narrow task settings and embodiment-specific architectures. In this work, we introduce a cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained on eight million navigation samples that encompass quadrupeds, drones, wheeled robots, and vehicles, and spanning diverse tasks such as vision-and-language navigation, object searching, target tracking, and autonomous driving. NavFoM employs a unified architecture that processes multimodal navigation inputs from varying camera configurations and navigation horizons. To accommodate diverse camera setups and temporal horizons, NavFoM incorporates identifier tokens that embed camera view information of embodiments and the temporal context of tasks. Furthermore, to meet the demands of real-world deployment, NavFoM controls all observation tokens using a dynamically adjusted sampling strategy under a limited token length budget. Extensive evaluations on public benchmarks demonstrate that our model achieves state-of-the-art or highly competitive performance across multiple navigation tasks and embodiments without requiring task-specific fine-tuning. Additional real-world experiments further confirm the strong generalization capability and practical applicability of our approach.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2508.10416.pdf' target='_blank'>https://arxiv.org/pdf/2508.10416.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoyuan Yu, Yuxing Long, Zihan Yang, Chengyan Zeng, Hongwei Fan, Jiyao Zhang, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10416">CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing vision-and-language navigation models often deviate from the correct trajectory when executing instructions. However, these models lack effective error correction capability, hindering their recovery from errors. To address this challenge, we propose Self-correction Flywheel, a novel post-training paradigm. Instead of considering the model's error trajectories on the training set as a drawback, our paradigm emphasizes their significance as a valuable data source. We have developed a method to identify deviations in these error trajectories and devised innovative techniques to automatically generate self-correction data for perception and action. These self-correction data serve as fuel to power the model's continued training. The brilliance of our paradigm is revealed when we re-evaluate the model on the training set, uncovering new error trajectories. At this time, the self-correction flywheel begins to spin. Through multiple flywheel iterations, we progressively enhance our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2% and 16.4%. Real robot tests in various indoor and outdoor environments demonstrate \method's superior capability of error correction, dynamic obstacle avoidance, and long instruction following.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2402.15852.pdf' target='_blank'>https://arxiv.org/pdf/2402.15852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15852">NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navigate in unseen environments following linguistic instructions. In this field, generalization is a long-standing challenge, either to out-of-distribution scenes or from Sim to Real. In this paper, we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap. NaVid makes the first endeavor to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometers, or depth inputs. Following human instruction, NaVid only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action. Our formulation mimics how humans navigate and naturally gets rid of the problems introduced by odometer noises, and the Sim2Real gaps from map or depth inputs. Moreover, our video-based approach can effectively encode the historical observations of robots as spatio-temporal contexts for decision making and instruction following. We train NaVid with 510k navigation samples collected from continuous environments, including action-planning and instruction-reasoning samples, along with 763k large-scale web data. Extensive experiments show that NaVid achieves state-of-the-art performance in simulation environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our proposed VLM approach plans the next step for not only the navigation agents but also this research field.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2401.07314.pdf' target='_blank'>https://arxiv.org/pdf/2401.07314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xiaodan Liang, Kwan-Yee K. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.07314">MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied agents equipped with GPT as their brains have exhibited extraordinary decision-making and generalization abilities across various tasks. However, existing zero-shot agents for vision-and-language navigation (VLN) only prompt GPT-4 to select potential locations within localized environments, without constructing an effective "global-view" for the agent to understand the overall environment. In this work, we present a novel map-guided GPT-based agent, dubbed MapGPT, which introduces an online linguistic-formed map to encourage global exploration. Specifically, we build an online map and incorporate it into the prompts that include node information and topological relationships, to help GPT understand the spatial environment. Benefiting from this design, we further propose an adaptive planning mechanism to assist the agent in performing multi-step path planning based on a map, systematically exploring multiple candidate nodes or sub-goals step by step. Extensive experiments demonstrate that our MapGPT is applicable to both GPT-4 and GPT-4V, achieving state-of-the-art zero-shot performance on R2R and REVERIE simultaneously (~10% and ~12% improvements in SR), and showcasing the newly emergent global thinking and path planning abilities of the GPT.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2506.14009.pdf' target='_blank'>https://arxiv.org/pdf/2506.14009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianzhong Chen, Naixiang Gao, Suning Huang, JunEn Low, Timothy Chen, Jiankai Sun, Mac Schwager
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14009">GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous drones capable of interpreting and executing high-level language instructions in unstructured environments remain a long-standing goal. Yet existing approaches are constrained by their dependence on hand-crafted skills, extensive parameter tuning, or computationally intensive models unsuitable for onboard use. We introduce GRaD-Nav++, a lightweight Vision-Language-Action (VLA) framework that runs fully onboard and follows natural-language commands in real time. Our policy is trained in a photorealistic 3D Gaussian Splatting (3DGS) simulator via Differentiable Reinforcement Learning (DiffRL), enabling efficient learning of low-level control from visual and linguistic inputs. At its core is a Mixture-of-Experts (MoE) action head, which adaptively routes computation to improve generalization while mitigating forgetting. In multi-task generalization experiments, GRaD-Nav++ achieves a success rate of 83% on trained tasks and 75% on unseen tasks in simulation. When deployed on real hardware, it attains 67% success on trained tasks and 50% on unseen ones. In multi-environment adaptation experiments, GRaD-Nav++ achieves an average success rate of 81% across diverse simulated environments and 67% across varied real-world settings. These results establish a new benchmark for fully onboard Vision-Language-Action (VLA) flight and demonstrate that compact, efficient models can enable reliable, language-guided navigation without relying on external infrastructure.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2501.17403.pdf' target='_blank'>https://arxiv.org/pdf/2501.17403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Hong, Yanyuan Qiao, Sen Wang, Jiajun Liu, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17403">General Scene Adaptation for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on one-time execution of individual instructions across multiple environments, aiming to develop agents capable of functioning in any environment in a zero-shot manner. However, real-world navigation robots often operate in persistent environments with relatively consistent physical layouts, visual observations, and language styles from instructors. Such a gap in the task setting presents an opportunity to improve VLN agents by incorporating continuous adaptation to specific environments. To better reflect these real-world conditions, we introduce GSA-VLN, a novel task requiring agents to execute navigation instructions within a specific scene and simultaneously adapt to it for improved performance over time. To evaluate the proposed task, one has to address two challenges in existing VLN datasets: the lack of OOD data, and the limited number and style diversity of instructions for each scene. Therefore, we propose a new dataset, GSA-R2R, which significantly expands the diversity and quantity of environments and instructions for the R2R dataset to evaluate agent adaptability in both ID and OOD contexts. Furthermore, we design a three-stage instruction orchestration pipeline that leverages LLMs to refine speaker-generated instructions and apply role-playing techniques to rephrase instructions into different speaking styles. This is motivated by the observation that each individual user often has consistent signatures or preferences in their instructions. We conducted extensive experiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various methods. Based on our findings, we propose a novel method, GR-DUET, which incorporates memory-based navigation graphs with an environment-specific training strategy, achieving state-of-the-art results on all GSA-R2R splits.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2411.16053.pdf' target='_blank'>https://arxiv.org/pdf/2411.16053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangzhao Dai, Jian Zhao, Yuantao Chen, Yusen Qin, Hao Zhao, Guosen Xie, Yazhou Yao, Xiangbo Shu, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16053">UnitedVLN: Generalizable Gaussian Splatting for Continuous Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN), where an agent follows instructions to reach a target destination, has recently seen significant advancements. In contrast to navigation in discrete environments with predefined trajectories, VLN in Continuous Environments (VLN-CE) presents greater challenges, as the agent is free to navigate any unobstructed location and is more vulnerable to visual occlusions or blind spots. Recent approaches have attempted to address this by imagining future environments, either through predicted future visual images or semantic features, rather than relying solely on current observations. However, these RGB-based and feature-based methods lack intuitive appearance-level information or high-level semantic complexity crucial for effective navigation. To overcome these limitations, we introduce a novel, generalizable 3DGS-based pre-training paradigm, called UnitedVLN, which enables agents to better explore future environments by unitedly rendering high-fidelity 360 visual images and semantic features. UnitedVLN employs two key schemes: search-then-query sampling and separate-then-united rendering, which facilitate efficient exploitation of neural primitives, helping to integrate both appearance and semantic information for more robust navigation. Extensive experiments demonstrate that UnitedVLN outperforms state-of-the-art methods on existing VLN-CE benchmarks.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2407.21452.pdf' target='_blank'>https://arxiv.org/pdf/2407.21452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Hong, Sen Wang, Zi Huang, Qi Wu, Jiajun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21452">Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world navigation often involves dealing with unexpected obstructions such as closed doors, moved objects, and unpredictable entities. However, mainstream Vision-and-Language Navigation (VLN) tasks typically assume instructions perfectly align with the fixed and predefined navigation graphs without any obstructions. This assumption overlooks potential discrepancies in actual navigation graphs and given instructions, which can cause major failures for both indoor and outdoor agents. To address this issue, we integrate diverse obstructions into the R2R dataset by modifying both the navigation graphs and visual observations, introducing an innovative dataset and task, R2R with UNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers of path obstructions to generate instruction-reality mismatches for VLN research. Experiments on R2R-UNO reveal that state-of-the-art VLN methods inevitably encounter significant challenges when facing such mismatches, indicating that they rigidly follow instructions rather than navigate adaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN), which includes a curriculum training strategy and virtual graph construction to help agents effectively adapt to obstructed environments. Empirical results show that ObVLN not only maintains robust performance in unobstructed scenarios but also achieves a substantial performance advantage with unexpected obstructions.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2406.02208.pdf' target='_blank'>https://arxiv.org/pdf/2406.02208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Hong, Sen Wang, Zi Huang, Qi Wu, Jiajun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02208">Why Only Text: Empowering Vision-and-Language Navigation with Multi-modal Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current Vision-and-Language Navigation (VLN) tasks mainly employ textual instructions to guide agents. However, being inherently abstract, the same textual instruction can be associated with different visual signals, causing severe ambiguity and limiting the transfer of prior knowledge in the vision domain from the user to the agent. To fill this gap, we propose Vision-and-Language Navigation with Multi-modal Prompts (VLN-MP), a novel task augmenting traditional VLN by integrating both natural language and images in instructions. VLN-MP not only maintains backward compatibility by effectively handling text-only prompts but also consistently shows advantages with different quantities and relevance of visual prompts. Possible forms of visual prompts include both exact and similar object images, providing adaptability and versatility in diverse navigation scenarios. To evaluate VLN-MP under a unified framework, we implement a new benchmark that offers: (1) a training-free pipeline to transform textual instructions into multi-modal forms with landmark images; (2) diverse datasets with multi-modal instructions for different downstream tasks; (3) a novel module designed to process various image prompts for seamless integration with state-of-the-art VLN models. Extensive experiments on four VLN benchmarks (R2R, RxR, REVERIE, CVDN) show that incorporating visual prompts significantly boosts navigation performance. While maintaining efficiency with text-only prompts, VLN-MP enables agents to navigate in the pre-explore setting and outperform text-based models, showing its broader applicability.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2310.07889.pdf' target='_blank'>https://arxiv.org/pdf/2310.07889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris, Aude Oliva, Phillip Isola, Yoon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07889">LangNav: Language as a Perceptual Representation for Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore the use of language as a perceptual representation for vision-and-language navigation (VLN), with a focus on low-data settings. Our approach uses off-the-shelf vision systems for image captioning and object detection to convert an agent's egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore several use cases of our language-based navigation (LangNav) approach on the R2R VLN benchmark: generating synthetic trajectories from a prompted language model (GPT-4) with which to finetune a smaller language model; domain transfer where we transfer a policy learned on one simulated environment (ALFRED) to another (more realistic) environment (R2R); and combining both vision- and language-based representations for VLN. Our approach is found to improve upon baselines that rely on visual features in settings where only a few expert trajectories (10-100) are available, demonstrating the potential of language as a perceptual representation for navigation.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2505.07868.pdf' target='_blank'>https://arxiv.org/pdf/2505.07868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanjia Huang, Mingyang Wu, Renjie Li, Zhengzhong Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07868">VISTA: Generative Visual Imagination for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) tasks agents with locating specific objects in unseen environments using natural language instructions and visual cues. Many existing VLN approaches typically follow an 'observe-and-reason' schema, that is, agents observe the environment and decide on the next action to take based on the visual observations of their surroundings. They often face challenges in long-horizon scenarios due to limitations in immediate observation and vision-language modality gaps. To overcome this, we present VISTA, a novel framework that employs an 'imagine-and-align' navigation strategy. Specifically, we leverage the generative prior of pre-trained diffusion models for dynamic visual imagination conditioned on both local observations and high-level language instructions. A Perceptual Alignment Filter module then grounds these goal imaginations against current observations, guiding an interpretable and structured reasoning process for action selection. Experiments show that VISTA sets new state-of-the-art results on Room-to-Room (R2R) and RoboTHOR benchmarks, e.g.,+3.6% increase in Success Rate on R2R. Extensive ablation analysis underscores the value of integrating forward-looking imagination, perceptual alignment, and structured reasoning for robust navigation in long-horizon environments.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2412.06465.pdf' target='_blank'>https://arxiv.org/pdf/2412.06465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuesong Zhang, Yunbo Xu, Jia Li, Zhenzhen Hu, Richnag Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06465">Agent Journey Beyond RGB: Unveiling Hybrid Semantic-Spatial Environmental Representations for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating unseen environments based on natural language instructions remains difficult for egocentric agents in Vision-and-Language Navigation (VLN). Existing approaches primarily rely on RGB images for environmental representation, underutilizing latent textual semantic and spatial cues and leaving the modality gap between instructions and scarce environmental representations unresolved. Intuitively, humans inherently ground semantic knowledge within spatial layouts during indoor navigation. Inspired by this, we propose a versatile Semantic Understanding and Spatial Awareness (SUSA) architecture to encourage agents to ground environment from diverse perspectives. SUSA includes a Textual Semantic Understanding (TSU) module, which narrows the modality gap between instructions and environments by generating and associating the descriptions of environmental landmarks in agent's immediate surroundings. Additionally, a Depth-enhanced Spatial Perception (DSP) module incrementally constructs a depth exploration map, enabling a more nuanced comprehension of environmental layouts. Experiments demonstrate that SUSA's hybrid semantic-spatial representations effectively enhance navigation performance, setting new state-of-the-art performance across three VLN benchmarks (REVERIE, R2R, and SOON). The source code will be publicly available.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2308.07997.pdf' target='_blank'>https://arxiv.org/pdf/2308.07997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peihao Chen, Xinyu Sun, Hongyan Zhi, Runhao Zeng, Thomas H. Li, Gaowen Liu, Mingkui Tan, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07997">$A^2$Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the task of zero-shot vision-and-language navigation (ZS-VLN), a practical yet challenging problem in which an agent learns to navigate following a path described by language instructions without requiring any path-instruction annotation data. Normally, the instructions have complex grammatical structures and often contain various action descriptions (e.g., "proceed beyond", "depart from"). How to correctly understand and execute these action demands is a critical problem, and the absence of annotated data makes it even more challenging. Note that a well-educated human being can easily understand path instructions without the need for any special training. In this paper, we propose an action-aware zero-shot VLN method ($A^2$Nav) by exploiting the vision-and-language ability of foundation models. Specifically, the proposed method consists of an instruction parser and an action-aware navigation policy. The instruction parser utilizes the advanced reasoning ability of large language models (e.g., GPT-3) to decompose complex navigation instructions into a sequence of action-specific object navigation sub-tasks. Each sub-task requires the agent to localize the object and navigate to a specific goal position according to the associated action demand. To accomplish these sub-tasks, an action-aware navigation policy is learned from freely collected action-specific datasets that reveal distinct characteristics of each action demand. We use the learned navigation policy for executing sub-tasks sequentially to follow the navigation instruction. Extensive experiments show $A^2$Nav achieves promising ZS-VLN performance and even surpasses the supervised learning methods on R2R-Habitat and RxR-Habitat datasets.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2110.13309.pdf' target='_blank'>https://arxiv.org/pdf/2110.13309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, Ivan Laptev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.13309">History Aware Multimodal Transformer for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy. HAMT achieves new state of the art on a broad range of VLN tasks, including VLN with fine-grained instructions (R2R, RxR), high-level instructions (R2R-Last, REVERIE), dialogs (CVDN) as well as long-horizon VLN (R4R, R2R-Back). We demonstrate HAMT to be particularly effective for navigation tasks with longer trajectories.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2506.02917.pdf' target='_blank'>https://arxiv.org/pdf/2506.02917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingpeng Sun, Zherong Pan, Xifeng Gao, Kui Wu, Aniket Bera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02917">Text-guided Generation of Efficient Personalized Inspection Plans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a training-free, Vision-Language Model (VLM)-guided approach for efficiently generating trajectories to facilitate target inspection planning based on text descriptions. Unlike existing Vision-and-Language Navigation (VLN) methods designed for general agents in unknown environments, our approach specifically targets the efficient inspection of known scenes, with widespread applications in fields such as medical, marine, and civil engineering. Leveraging VLMs, our method first extracts points of interest (POIs) from the text description, then identifies a set of waypoints from which POIs are both salient and align with the spatial constraints defined in the prompt. Next, we interact with the VLM to iteratively refine the trajectory, preserving the visibility and prominence of the POIs. Further, we solve a Traveling Salesman Problem (TSP) to find the most efficient visitation order that satisfies the order constraint implied in the text description. Finally, we apply trajectory optimization to generate smooth, executable inspection paths for aerial and underwater vehicles. We have evaluated our method across a series of both handcrafted and real-world scanned environments. The results demonstrate that our approach effectively generates inspection planning trajectories that adhere to user instructions.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2505.23266.pdf' target='_blank'>https://arxiv.org/pdf/2505.23266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunlong Xie, Jialing He, Shangwei Guo, Jiacheng Wang, Shudong Zhang, Tianwei Zhang, Tao Xiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23266">Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Adversarial Object Fusion (AdvOF), a novel attack framework targeting vision-and-language navigation (VLN) agents in service-oriented environments by generating adversarial 3D objects. While foundational models like Large Language Models (LLMs) and Vision Language Models (VLMs) have enhanced service-oriented navigation systems through improved perception and decision-making, their integration introduces vulnerabilities in mission-critical service workflows. Existing adversarial attacks fail to address service computing contexts, where reliability and quality-of-service (QoS) are paramount. We utilize AdvOF to investigate and explore the impact of adversarial environments on the VLM-based perception module of VLN agents. In particular, AdvOF first precisely aggregates and aligns the victim object positions in both 2D and 3D space, defining and rendering adversarial objects. Then, we collaboratively optimize the adversarial object with regularization between the adversarial and victim object across physical properties and VLM perceptions. Through assigning importance weights to varying views, the optimization is processed stably and multi-viewedly by iterative fusions from local updates and justifications. Our extensive evaluations demonstrate AdvOF can effectively degrade agent performance under adversarial conditions while maintaining minimal interference with normal navigation tasks. This work advances the understanding of service security in VLM-powered navigation systems, providing computational foundations for robust service composition in physical-world deployments.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2505.12443.pdf' target='_blank'>https://arxiv.org/pdf/2505.12443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqi Lyu, Zerui Li, Yanyuan Qiao, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12443">BadNAVer: Exploring Jailbreak Attacks On Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) have recently gained attention for their generalization and reasoning capabilities in Vision-and-Language Navigation (VLN) tasks, leading to the rise of MLLM-driven navigators. However, MLLMs are vulnerable to jailbreak attacks, where crafted prompts bypass safety mechanisms and trigger undesired outputs. In embodied scenarios, such vulnerabilities pose greater risks: unlike plain text models that generate toxic content, embodied agents may interpret malicious instructions as executable commands, potentially leading to real-world harm. In this paper, we present the first systematic jailbreak attack paradigm targeting MLLM-driven navigator. We propose a three-tiered attack framework and construct malicious queries across four intent categories, concatenated with standard navigation instructions. In the Matterport3D simulator, we evaluate navigation agents powered by five MLLMs and report an average attack success rate over 90%. To test real-world feasibility, we replicate the attack on a physical robot. Our results show that even well-crafted prompts can induce harmful actions and intents in MLLMs, posing risks beyond toxic output and potentially leading to physical harm.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2503.24065.pdf' target='_blank'>https://arxiv.org/pdf/2503.24065.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqi Zhang, Yanyuan Qiao, Qunbo Wang, Zike Yan, Qi Wu, Zhihua Wei, Jing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24065">COSMO: Combination of Selective Memorization for Low-cost Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) tasks have gained prominence within artificial intelligence research due to their potential application in fields like home assistants. Many contemporary VLN approaches, while based on transformer architectures, have increasingly incorporated additional components such as external knowledge bases or map information to enhance performance. These additions, while boosting performance, also lead to larger models and increased computational costs. In this paper, to achieve both high performance and low computational costs, we propose a novel architecture with the COmbination of Selective MemOrization (COSMO). Specifically, COSMO integrates state-space modules and transformer modules, and incorporates two VLN-customized selective state space modules: the Round Selective Scan (RSS) and the Cross-modal Selective State Space Module (CS3). RSS facilitates comprehensive inter-modal interactions within a single scan, while the CS3 module adapts the selective state space module into a dual-stream architecture, thereby enhancing the acquisition of cross-modal interactions. Experimental validations on three mainstream VLN benchmarks, REVERIE, R2R, and R2R-CE, not only demonstrate competitive navigation performance of our model but also show a significant reduction in computational costs.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2503.10069.pdf' target='_blank'>https://arxiv.org/pdf/2503.10069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Shi, Zerui Li, Wenqi Lyu, Jiatong Xia, Feras Dayoub, Yanyuan Qiao, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10069">SmartWay: Enhanced Waypoint Prediction and Backtracking for Zero-Shot Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) in continuous environments requires agents to interpret natural language instructions while navigating unconstrained 3D spaces. Existing VLN-CE frameworks rely on a two-stage approach: a waypoint predictor to generate waypoints and a navigator to execute movements. However, current waypoint predictors struggle with spatial awareness, while navigators lack historical reasoning and backtracking capabilities, limiting adaptability. We propose a zero-shot VLN-CE framework integrating an enhanced waypoint predictor with a Multi-modal Large Language Model (MLLM)-based navigator. Our predictor employs a stronger vision encoder, masked cross-attention fusion, and an occupancy-aware loss for better waypoint quality. The navigator incorporates history-aware reasoning and adaptive path planning with backtracking, improving robustness. Experiments on R2R-CE and MP3D benchmarks show our method achieves state-of-the-art (SOTA) performance in zero-shot settings, demonstrating competitive results compared to fully supervised methods. Real-world validation on Turtlebot 4 further highlights its adaptability.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2502.19024.pdf' target='_blank'>https://arxiv.org/pdf/2502.19024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zerui Li, Gengze Zhou, Haodong Hong, Yanyan Shao, Wenqi Lyu, Yanyuan Qiao, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19024">Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) empowers agents to associate time-sequenced visual observations with corresponding instructions to make sequential decisions. However, generalization remains a persistent challenge, particularly when dealing with visually diverse scenes or transitioning from simulated environments to real-world deployment. In this paper, we address the mismatch between human-centric instructions and quadruped robots with a low-height field of view, proposing a Ground-level Viewpoint Navigation (GVNav) approach to mitigate this issue. This work represents the first attempt to highlight the generalization gap in VLN across varying heights of visual observation in realistic robot deployments. Our approach leverages weighted historical observations as enriched spatiotemporal contexts for instruction following, effectively managing feature collisions within cells by assigning appropriate weights to identical features across different viewpoints. This enables low-height robots to overcome challenges such as visual obstructions and perceptual mismatches. Additionally, we transfer the connectivity graph from the HM3D and Gibson datasets as an extra resource to enhance spatial priors and a more comprehensive representation of real-world scenarios, leading to improved performance and generalizability of the waypoint predictor in real-world environments. Extensive experiments demonstrate that our Ground-level Viewpoint Navigation (GVnav) approach significantly improves performance in both simulated environments and real-world deployments with quadruped robots.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2409.18794.pdf' target='_blank'>https://arxiv.org/pdf/2409.18794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanyuan Qiao, Wenqi Lyu, Hui Wang, Zixu Wang, Zerui Li, Yuan Zhang, Mingkui Tan, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18794">Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in Continuous Environment with Open-Source LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) tasks require an agent to follow textual instructions to navigate through 3D environments. Traditional approaches use supervised learning methods, relying heavily on domain-specific datasets to train VLN models. Recent methods try to utilize closed-source large language models (LLMs) like GPT-4 to solve VLN tasks in zero-shot manners, but face challenges related to expensive token costs and potential data breaches in real-world applications. In this work, we introduce Open-Nav, a novel study that explores open-source LLMs for zero-shot VLN in the continuous environment. Open-Nav employs a spatial-temporal chain-of-thought (CoT) reasoning approach to break down tasks into instruction comprehension, progress estimation, and decision-making. It enhances scene perceptions with fine-grained object and spatial knowledge to improve LLM's reasoning in navigation. Our extensive experiments in both simulated and real-world environments demonstrate that Open-Nav achieves competitive performance compared to using closed-source LLMs.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2402.03561.pdf' target='_blank'>https://arxiv.org/pdf/2402.03561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialu Li, Aishwarya Padmakumar, Gaurav Sukhatme, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03561">VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with three proxy tasks: Masked Language Modeling, Instruction and Trajectory Matching, and Next Action Prediction, so as to learn temporally-aware and visually-aligned instruction representations. The learned instruction representation is adapted to the state-of-the-art navigator when fine-tuning on the Touchdown dataset. Empirical results demonstrate that VLN-Video significantly outperforms previous state-of-the-art models by 2.1% in task completion rate, achieving a new state-of-the-art on the Touchdown dataset.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2310.06654.pdf' target='_blank'>https://arxiv.org/pdf/2310.06654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanqi Chen, Lei Yang, Guanhua Chen, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06654">Evaluating Explanation Methods for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to navigate robots with natural language instructions in an unknown environment is a crucial step for achieving embodied artificial intelligence (AI). With the improving performance of deep neural models proposed in the field of vision-and-language navigation (VLN), it is equally interesting to know what information the models utilize for their decision-making in the navigation tasks. To understand the inner workings of deep neural models, various explanation methods have been developed for promoting explainable AI (XAI). But they are mostly applied to deep neural models for image or text classification tasks and little work has been done in explaining deep neural models for VLN tasks. In this paper, we address these problems by building quantitative benchmarks to evaluate explanation methods for VLN models in terms of faithfulness. We propose a new erasure-based evaluation pipeline to measure the step-wise textual explanation in the sequential decision-making setting. We evaluate several explanation methods for two representative VLN models on two popular VLN datasets and reveal valuable findings through our experiments.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2308.10172.pdf' target='_blank'>https://arxiv.org/pdf/2308.10172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanyuan Qiao, Zheng Yu, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10172">VLN-PETL: Parameter-Efficient Transfer Learning for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of the Vision-and-Language Navigation~(VLN) tasks has witnessed rapid progress recently thanks to the use of large pre-trained vision-and-language models. However, full fine-tuning the pre-trained model for every downstream VLN task is becoming costly due to the considerable model size. Recent research hotspot of Parameter-Efficient Transfer Learning (PETL) shows great potential in efficiently tuning large pre-trained models for the common CV and NLP tasks, which exploits the most of the representation knowledge implied in the pre-trained model while only tunes a minimal set of parameters. However, simply utilizing existing PETL methods for the more challenging VLN tasks may bring non-trivial degeneration to the performance. Therefore, we present the first study to explore PETL methods for VLN tasks and propose a VLN-specific PETL method named VLN-PETL. Specifically, we design two PETL modules: Historical Interaction Booster (HIB) and Cross-modal Interaction Booster (CIB). Then we combine these two modules with several existing PETL methods as the integrated VLN-PETL. Extensive experimental results on four mainstream VLN tasks (R2R, REVERIE, NDH, RxR) demonstrate the effectiveness of our proposed VLN-PETL, where VLN-PETL achieves comparable or even better performance to full fine-tuning and outperforms other PETL methods with promising margins.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2308.10141.pdf' target='_blank'>https://arxiv.org/pdf/2308.10141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanyuan Qiao, Yuankai Qi, Zheng Yu, Jing Liu, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10141">March in Chat: Interactive Prompting for Remote Embodied Referring Expression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many Vision-and-Language Navigation (VLN) tasks have been proposed in recent years, from room-based to object-based and indoor to outdoor. The REVERIE (Remote Embodied Referring Expression) is interesting since it only provides high-level instructions to the agent, which are closer to human commands in practice. Nevertheless, this poses more challenges than other VLN tasks since it requires agents to infer a navigation plan only based on a short instruction. Large Language Models (LLMs) show great potential in robot action planning by providing proper prompts. Still, this strategy has not been explored under the REVERIE settings. There are several new challenges. For example, the LLM should be environment-aware so that the navigation plan can be adjusted based on the current visual observation. Moreover, the LLM planned actions should be adaptable to the much larger and more complex REVERIE environment. This paper proposes a March-in-Chat (MiC) model that can talk to the LLM on the fly and plan dynamically based on a newly proposed Room-and-Object Aware Scene Perceiver (ROASP). Our MiC model outperforms the previous state-of-the-art by large margins by SPL and RGSPL metrics on the REVERIE benchmark.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2307.06082.pdf' target='_blank'>https://arxiv.org/pdf/2307.06082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu, Stefan Riezler, William Yang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.06082">VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation~(VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that VELMA is able to successfully follow navigation instructions in Street View with only two in-context examples. We further finetune the LLM agent on a few thousand examples and achieve 25%-30% relative improvement in task completion over the previous state-of-the-art for two datasets.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2305.19195.pdf' target='_blank'>https://arxiv.org/pdf/2305.19195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialu Li, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19195">PanoGen: Text-Conditioned Panoramic Environment Generation for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) requires the agent to follow language instructions to navigate through 3D environments. One main challenge in VLN is the limited availability of photorealistic training environments, which makes it hard to generalize to new and unseen environments. To address this problem, we propose PanoGen, a generation method that can potentially create an infinite number of diverse panoramic environments conditioned on text. Specifically, we collect room descriptions by captioning the room images in existing Matterport3D environments, and leverage a state-of-the-art text-to-image diffusion model to generate the new panoramic environments. We use recursive outpainting over the generated images to create consistent 360-degree panorama views. Our new panoramic environments share similar semantic information with the original environments by conditioning on text descriptions, which ensures the co-occurrence of objects in the panorama follows human intuition, and creates enough diversity in room appearance and layout with image outpainting. Lastly, we explore two ways of utilizing PanoGen in VLN pre-training and fine-tuning. We generate instructions for paths in our PanoGen environments with a speaker built on a pre-trained vision-and-language model for VLN pre-training, and augment the visual observation with our panoramic environments during agents' fine-tuning to avoid overfitting to seen environments. Empirically, learning with our PanoGen environments achieves the new state-of-the-art on the Room-to-Room, Room-for-Room, and CVDN datasets. Pre-training with our PanoGen speaker data is especially effective for CVDN, which has under-specified instructions and needs commonsense knowledge. Lastly, we show that the agent can benefit from training with more generated panoramic environments, suggesting promising results for scaling up the PanoGen environments.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2304.04907.pdf' target='_blank'>https://arxiv.org/pdf/2304.04907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialu Li, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04907">Improving Vision-and-Language Navigation by Generating Future-View Image Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) is the task that requires an agent to navigate through the environment based on natural language instructions. At each step, the agent takes the next action by selecting from a set of navigable locations. In this paper, we aim to take one step further and explore whether the agent can benefit from generating the potential future view during navigation. Intuitively, humans will have an expectation of how the future environment will look like, based on the natural language instructions and surrounding views, which will aid correct navigation. Hence, to equip the agent with this ability to generate the semantics of future navigation views, we first propose three proxy tasks during the agent's in-domain pre-training: Masked Panorama Modeling (MPM), Masked Trajectory Modeling (MTM), and Action Prediction with Image Generation (APIG). These three objectives teach the model to predict missing views in a panorama (MPM), predict missing steps in the full trajectory (MTM), and generate the next view based on the full instruction and navigation history (APIG), respectively. We then fine-tune the agent on the VLN task with an auxiliary loss that minimizes the difference between the view semantics generated by the agent and the ground truth view semantics of the next step. Empirically, our VLN-SIG achieves the new state-of-the-art on both the Room-to-Room dataset and the CVDN dataset. We further show that our agent learns to fill in missing patches in future views qualitatively, which brings more interpretability over agents' predicted actions. Lastly, we demonstrate that learning to predict future view semantics also enables the agent to have better performance on longer paths.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/1911.07308.pdf' target='_blank'>https://arxiv.org/pdf/1911.07308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tsu-Jui Fu, Xin Eric Wang, Matthew Peterson, Scott Grafton, Miguel Eckstein, William Yang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1911.07308">Counterfactual Vision-and-Language Navigation via Adversarial Path Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) is a task where agents must decide how to move through a 3D environment to reach a goal by grounding natural language instructions to the visual surroundings. One of the problems of the VLN task is data scarcity since it is difficult to collect enough navigation paths with human-annotated instructions for interactive environments. In this paper, we explore the use of counterfactual thinking as a human-inspired data augmentation method that results in robust models. Counterfactual thinking is a concept that describes the human propensity to create possible alternatives to life events that have already occurred. We propose an adversarial-driven counterfactual reasoning model that can consider effective conditions instead of low-quality augmented data. In particular, we present a model-agnostic adversarial path sampler (APS) that learns to sample challenging paths that force the navigator to improve based on the navigation performance. APS also serves to do pre-exploration of unseen environments to strengthen the model's ability to generalize. We evaluate the influence of APS on the performance of different VLN baseline models using the room-to-room dataset (R2R). The results show that the adversarial training process with our proposed APS benefits VLN models under both seen and unseen environments. And the pre-exploration process can further gain additional improvements under unseen environments.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2504.09843.pdf' target='_blank'>https://arxiv.org/pdf/2504.09843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Yue, Dongliang Zhou, Liang Xie, Erwei Yin, Feitian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09843">ST-Booster: An Iterative SpatioTemporal Perception Booster for Vision-and-Language Navigation in Continuous Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to navigate unknown, continuous spaces based on natural language instructions. Compared to discrete settings, VLN-CE poses two core perception challenges. First, the absence of predefined observation points leads to heterogeneous visual memories and weakened global spatial correlations. Second, cumulative reconstruction errors in three-dimensional scenes introduce structural noise, impairing local feature perception. To address these challenges, this paper proposes ST-Booster, an iterative spatiotemporal booster that enhances navigation performance through multi-granularity perception and instruction-aware reasoning. ST-Booster consists of three key modules -- Hierarchical SpatioTemporal Encoding (HSTE), Multi-Granularity Aligned Fusion (MGAF), and ValueGuided Waypoint Generation (VGWG). HSTE encodes long-term global memory using topological graphs and captures shortterm local details via grid maps. MGAF aligns these dualmap representations with instructions through geometry-aware knowledge fusion. The resulting representations are iteratively refined through pretraining tasks. During reasoning, VGWG generates Guided Attention Heatmaps (GAHs) to explicitly model environment-instruction relevance and optimize waypoint selection. Extensive comparative experiments and performance analyses are conducted, demonstrating that ST-Booster outperforms existing state-of-the-art methods, particularly in complex, disturbance-prone environments.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2503.09938.pdf' target='_blank'>https://arxiv.org/pdf/2503.09938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Wang, Dongliang Zhou, Liang Xie, Chao Xu, Ye Yan, Erwei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09938">PanoGen++: Domain-Adapted Text-Guided Panoramic Environment Generation for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) tasks require agents to navigate three-dimensional environments guided by natural language instructions, offering substantial potential for diverse applications. However, the scarcity of training data impedes progress in this field. This paper introduces PanoGen++, a novel framework that addresses this limitation by generating varied and pertinent panoramic environments for VLN tasks. PanoGen++ incorporates pre-trained diffusion models with domain-specific fine-tuning, employing parameter-efficient techniques such as low-rank adaptation to minimize computational costs. We investigate two settings for environment generation: masked image inpainting and recursive image outpainting. The former maximizes novel environment creation by inpainting masked regions based on textual descriptions, while the latter facilitates agents' learning of spatial relationships within panoramas. Empirical evaluations on room-to-room (R2R), room-for-room (R4R), and cooperative vision-and-dialog navigation (CVDN) datasets reveal significant performance enhancements: a 2.44% increase in success rate on the R2R test leaderboard, a 0.63% improvement on the R4R validation unseen set, and a 0.75-meter enhancement in goal progress on the CVDN validation unseen set. PanoGen++ augments the diversity and relevance of training environments, resulting in improved generalization and efficacy in VLN tasks.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2409.17313.pdf' target='_blank'>https://arxiv.org/pdf/2409.17313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehao Wang, Minye Wu, Yixin Cao, Yubo Ma, Meiqi Chen, Tinne Tuytelaars
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17313">Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study presents a novel evaluation framework for the Vision-Language Navigation (VLN) task. It aims to diagnose current models for various instruction categories at a finer-grained level. The framework is structured around the context-free grammar (CFG) of the task. The CFG serves as the basis for the problem decomposition and the core premise of the instruction categories design. We propose a semi-automatic method for CFG construction with the help of Large-Language Models (LLMs). Then, we induct and generate data spanning five principal instruction categories (i.e. direction change, landmark recognition, region recognition, vertical movement, and numerical comprehension). Our analysis of different models reveals notable performance discrepancies and recurrent issues. The stagnation of numerical comprehension, heavy selective biases over directional concepts, and other interesting findings contribute to the development of future language-guided navigation systems.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2311.02817.pdf' target='_blank'>https://arxiv.org/pdf/2311.02817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Yue, Dongliang Zhou, Liang Xie, Feitian Zhang, Ye Yan, Erwei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02817">Safe-VLN: Collision Avoidance for Vision-and-Language Navigation of Autonomous Robots Operating in Continuous Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of vision-and-language navigation in continuous environments (VLN-CE) aims at training an autonomous agent to perform low-level actions to navigate through 3D continuous surroundings using visual observations and language instructions. The significant potential of VLN-CE for mobile robots has been demonstrated across a large number of studies. However, most existing works in VLN-CE focus primarily on transferring the standard discrete vision-and-language navigation (VLN) methods to continuous environments, overlooking the problem of collisions. Such oversight often results in the agent deviating from the planned path or, in severe instances, the agent being trapped in obstacle areas and failing the navigational task. To address the above-mentioned issues, this paper investigates various collision scenarios within VLN-CE and proposes a classification method to predicate the underlying causes of collisions. Furthermore, a new VLN-CE algorithm, named Safe-VLN, is proposed to bolster collision avoidance capabilities including two key components, i.e., a waypoint predictor and a navigator. In particular, the waypoint predictor leverages a simulated 2D LiDAR occupancy mask to prevent the predicted waypoints from being situated in obstacle-ridden areas. The navigator, on the other hand, employs the strategy of `re-selection after collision' to prevent the robot agent from becoming ensnared in a cycle of perpetual collisions. The proposed Safe-VLN is evaluated on the R2R-CE, the results of which demonstrate an enhanced navigational performance and a statistically significant reduction in collision incidences.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2308.12587.pdf' target='_blank'>https://arxiv.org/pdf/2308.12587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibo Cui, Liang Xie, Yakun Zhang, Meishan Zhang, Ye Yan, Erwei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12587">Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-modal alignment is one key challenge for Vision-and-Language Navigation (VLN). Most existing studies concentrate on mapping the global instruction or single sub-instruction to the corresponding trajectory. However, another critical problem of achieving fine-grained alignment at the entity level is seldom considered. To address this problem, we propose a novel Grounded Entity-Landmark Adaptive (GELA) pre-training paradigm for VLN tasks. To achieve the adaptive pre-training paradigm, we first introduce grounded entity-landmark human annotations into the Room-to-Room (R2R) dataset, named GEL-R2R. Additionally, we adopt three grounded entity-landmark adaptive pre-training objectives: 1) entity phrase prediction, 2) landmark bounding box prediction, and 3) entity-landmark semantic alignment, which explicitly supervise the learning of fine-grained cross-modal alignment between entity phrases and environment landmarks. Finally, we validate our model on two downstream benchmarks: VLN with descriptive instructions (R2R) and dialogue instructions (CVDN). The comprehensive experiments show that our GELA model achieves state-of-the-art results on both tasks, demonstrating its effectiveness and generalizability.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2509.13733.pdf' target='_blank'>https://arxiv.org/pdf/2509.13733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaolin Zhou, Tingyang Xiao, Liu Liu, Yucheng Wang, Maiyue Chen, Xinrui Meng, Xinjie Wang, Wei Feng, Wei Sui, Zhizhong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13733">FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual-Language Navigation (VLN) is a fundamental challenge in robotic systems, with broad applications for the deployment of embodied agents in real-world environments. Despite recent advances, existing approaches are limited in long-range spatial reasoning, often exhibiting low success rates and high inference latency, particularly in long-range navigation tasks. To address these limitations, we propose FSR-VLN, a vision-language navigation system that combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation supporting progressive retrieval, from coarse room-level localization to fine-grained goal view and object identification. Building on HMSG, FSR first performs fast matching to efficiently select candidate rooms, views, and objects, then applies VLM-driven refinement for final goal selection. We evaluated FSR-VLN across four comprehensive indoor datasets collected by humanoid robots, utilizing 87 instructions that encompass a diverse range of object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all datasets, measured by the retrieval success rate (RSR), while reducing the response time by 82% compared to VLM-based methods on tour videos by activating slow reasoning only when fast intuition fails. Furthermore, we integrate FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1 humanoid robot, enabling natural language interaction and real-time navigation.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2509.11197.pdf' target='_blank'>https://arxiv.org/pdf/2509.11197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunheng Wang, Yuetong Fang, Taowen Wang, Yixiao Feng, Yawen Tan, Shuning Zhang, Peiran Liu, Yiding Ji, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11197">DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation in Continuous Environments (VLN-CE), which links language instructions to perception and control in the real world, is a core capability of embodied robots. Recently, large-scale pretrained foundation models have been leveraged as shared priors for perception, reasoning, and action, enabling zero-shot VLN without task-specific training. However, existing zero-shot VLN methods depend on costly perception and passive scene understanding, collapsing control to point-level choices. As a result, they are expensive to deploy, misaligned in action semantics, and short-sighted in planning. To address these issues, we present DreamNav that focuses on the following three aspects: (1) for reducing sensory cost, our EgoView Corrector aligns viewpoints and stabilizes egocentric perception; (2) instead of point-level actions, our Trajectory Predictor favors global trajectory-level planning to better align with instruction semantics; and (3) to enable anticipatory and long-horizon planning, we propose an Imagination Predictor to endow the agent with proactive thinking capability. On VLN-CE and real-world tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the strongest egocentric baseline with extra information by up to 7.49\% and 18.15\% in terms of SR and SPL metrics. To our knowledge, this is the first zero-shot VLN method to unify trajectory-level planning and active imagination while using only egocentric inputs.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2505.12835.pdf' target='_blank'>https://arxiv.org/pdf/2505.12835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengxing Cai, Jinhan Dong, Jingjun Tan, Jingcheng Deng, Sihang Li, Zhifeng Gao, Haidong Wang, Zicheng Su, Agachai Sumalee, Renxin Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12835">FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital for applications such as disaster response, logistics delivery, and urban inspection. However, existing methods often struggle with insufficient multimodal fusion, weak generalization, and poor interpretability. To address these challenges, we propose FlightGPT, a novel UAV VLN framework built upon Vision-Language Models (VLMs) with powerful multimodal perception capabilities. We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT) using high-quality demonstrations to improve initialization and structured reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by a composite reward that considers goal accuracy, reasoning quality, and format compliance, to enhance generalization and adaptability. Furthermore, FlightGPT introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve decision interpretability. Extensive experiments on the city-scale dataset CityNav demonstrate that FlightGPT achieves state-of-the-art performance across all scenarios, with a 9.22\% higher success rate than the strongest baseline in unseen environments. Our implementation is publicly available.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2503.14229.pdf' target='_blank'>https://arxiv.org/pdf/2503.14229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Dong, Fengyi Wu, Qi He, Heng Li, Minghan Li, Zebang Cheng, Yuxuan Zhou, Jingdong Sun, Qi Dai, Zhi-Qi Cheng, Alexander G Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14229">HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) systems often focus on either discrete (panoramic) or continuous (free-motion) paradigms alone, overlooking the complexities of human-populated, dynamic environments. We introduce a unified Human-Aware VLN (HA-VLN) benchmark that merges these paradigms under explicit social-awareness constraints. Our contributions include: 1. A standardized task definition that balances discrete-continuous navigation with personal-space requirements; 2. An enhanced human motion dataset (HAPS 2.0) and upgraded simulators capturing realistic multi-human interactions, outdoor contexts, and refined motion-language alignment; 3. Extensive benchmarking on 16,844 human-centric instructions, revealing how multi-human dynamics and partial observability pose substantial challenges for leading VLN agents; 4. Real-world robot tests validating sim-to-real transfer in crowded indoor spaces; and 5. A public leaderboard supporting transparent comparisons across discrete and continuous tasks. Empirical results show improved navigation success and fewer collisions when social context is integrated, underscoring the need for human-centric design. By releasing all datasets, simulators, agent code, and evaluation tools, we aim to advance safer, more capable, and socially responsible VLN research.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2503.11006.pdf' target='_blank'>https://arxiv.org/pdf/2503.11006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Xie, Binkai Ou, Fei Ma, Yaohua Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11006">Observation-Graph Interaction and Key-Detail Guidance for Vision and Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision and Language Navigation (VLN) requires an agent to navigate through environments following natural language instructions. However, existing methods often struggle with effectively integrating visual observations and instruction details during navigation, leading to suboptimal path planning and limited success rates. In this paper, we propose OIKG (Observation-graph Interaction and Key-detail Guidance), a novel framework that addresses these limitations through two key components: (1) an observation-graph interaction module that decouples angular and visual information while strengthening edge representations in the navigation space, and (2) a key-detail guidance module that dynamically extracts and utilizes fine-grained location and object information from instructions. By enabling more precise cross-modal alignment and dynamic instruction interpretation, our approach significantly improves the agent's ability to follow complex navigation instructions. Extensive experiments on the R2R and RxR datasets demonstrate that OIKG achieves state-of-the-art performance across multiple evaluation metrics, validating the effectiveness of our method in enhancing navigation precision through better observation-instruction alignment.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2411.08579.pdf' target='_blank'>https://arxiv.org/pdf/2411.08579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youzhi Liu, Fanglong Yao, Yuanchang Yue, Guangluan Xu, Xian Sun, Kun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08579">NavAgent: Multi-scale Urban Street View Fusion For UAV Embodied Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN), as a widely discussed research direction in embodied intelligence, aims to enable embodied agents to navigate in complicated visual environments through natural language commands. Most existing VLN methods focus on indoor ground robot scenarios. However, when applied to UAV VLN in outdoor urban scenes, it faces two significant challenges. First, urban scenes contain numerous objects, which makes it challenging to match fine-grained landmarks in images with complex textual descriptions of these landmarks. Second, overall environmental information encompasses multiple modal dimensions, and the diversity of representations significantly increases the complexity of the encoding process. To address these challenges, we propose NavAgent, the first urban UAV embodied navigation model driven by a large Vision-Language Model. NavAgent undertakes navigation tasks by synthesizing multi-scale environmental information, including topological maps (global), panoramas (medium), and fine-grained landmarks (local). Specifically, we utilize GLIP to build a visual recognizer for landmark capable of identifying and linguisticizing fine-grained landmarks. Subsequently, we develop dynamically growing scene topology map that integrate environmental information and employ Graph Convolutional Networks to encode global environmental data. In addition, to train the visual recognizer for landmark, we develop NavAgent-Landmark2K, the first fine-grained landmark dataset for real urban street scenes. In experiments conducted on the Touchdown and Map2seq datasets, NavAgent outperforms strong baseline models. The code and dataset will be released to the community to facilitate the exploration and development of outdoor VLN.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2305.14268.pdf' target='_blank'>https://arxiv.org/pdf/2305.14268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi-Yi Dou, Feng Gao, Nanyun Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14268">Masked Path Modeling for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) agents are trained to navigate in real-world environments by following natural language instructions. A major challenge in VLN is the limited availability of training data, which hinders the models' ability to generalize effectively. Previous approaches have attempted to address this issue by introducing additional supervision during training, often requiring costly human-annotated data that restricts scalability. In this paper, we introduce a masked path modeling (MPM) objective, which pretrains an agent using self-collected data for downstream navigation tasks. Our proposed method involves allowing the agent to actively explore navigation environments without a specific goal and collect the paths it traverses. Subsequently, we train the agent on this collected data to reconstruct the original path given a randomly masked subpath. This way, the agent can actively accumulate a diverse and substantial amount of data while learning conditional action generation. To evaluate the effectiveness of our technique, we conduct experiments on various VLN datasets and demonstrate the versatility of MPM across different levels of instruction complexity. Our results exhibit significant improvements in success rates, with enhancements of 1.32\%, 1.05\%, and 1.19\% on the val-unseen split of the Room-to-Room, Room-for-Room, and Room-across-Room datasets, respectively. Furthermore, we conduct an analysis that highlights the potential for additional improvements when the agent is allowed to explore unseen environments prior to testing.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2508.15232.pdf' target='_blank'>https://arxiv.org/pdf/2508.15232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruipu Wu, Yige Zhang, Jinyu Chen, Linjiang Huang, Shifeng Zhang, Xu Zhou, Liang Wang, Si Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15232">AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerial Vision-and-Language Navigation (VLN) is an emerging task that enables Unmanned Aerial Vehicles (UAVs) to navigate outdoor environments using natural language instructions and visual cues. However, due to the extended trajectories and complex maneuverability of UAVs, achieving reliable UAV-VLN performance is challenging and often requires human intervention or overly detailed instructions. To harness the advantages of UAVs' high mobility, which could provide multi-grained perspectives, while maintaining a manageable motion space for learning, we introduce a novel task called Dual-Altitude UAV Collaborative VLN (DuAl-VLN). In this task, two UAVs operate at distinct altitudes: a high-altitude UAV responsible for broad environmental reasoning, and a low-altitude UAV tasked with precise navigation. To support the training and evaluation of the DuAl-VLN, we construct the HaL-13k, a dataset comprising 13,838 collaborative high-low UAV demonstration trajectories, each paired with target-oriented language instructions. This dataset includes both unseen maps and an unseen object validation set to systematically evaluate the model's generalization capabilities across novel environments and unfamiliar targets. To consolidate their complementary strengths, we propose a dual-UAV collaborative VLN framework, AeroDuo, where the high-altitude UAV integrates a multimodal large language model (Pilot-LLM) for target reasoning, while the low-altitude UAV employs a lightweight multi-stage policy for navigation and target grounding. The two UAVs work collaboratively and only exchange minimal coordinate information to ensure efficiency.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2507.04430.pdf' target='_blank'>https://arxiv.org/pdf/2507.04430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqin Wang, Jinyu Chen, Xiangyi Zheng, Qinan Liao, Linjiang Huang, Si Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04430">"Hi AirStar, Guide Me to the Badminton Court."</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicles, operating in environments with relatively few obstacles, offer high maneuverability and full three-dimensional mobility. This allows them to rapidly approach objects and perform a wide range of tasks often challenging for ground robots, making them ideal for exploration, inspection, aerial imaging, and everyday assistance. In this paper, we introduce AirStar, a UAV-centric embodied platform that turns a UAV into an intelligent aerial assistant: a large language model acts as the cognitive core for environmental understanding, contextual reasoning, and task planning. AirStar accepts natural interaction through voice commands and gestures, removing the need for a remote controller and significantly broadening its user base. It combines geospatial knowledge-driven long-distance navigation with contextual reasoning for fine-grained short-range control, resulting in an efficient and accurate vision-and-language navigation (VLN) capability.Furthermore, the system also offers built-in capabilities such as cross-modal question answering, intelligent filming, and target tracking. With a highly extensible framework, it supports seamless integration of new functionalities, paving the way toward a general-purpose, instruction-driven intelligent UAV agent. The supplementary PPT is available at \href{https://buaa-colalab.github.io/airstar.github.io}{https://buaa-colalab.github.io/airstar.github.io}.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2503.11091.pdf' target='_blank'>https://arxiv.org/pdf/2503.11091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ganlong Zhao, Guanbin Li, Jia Pan, Yizhou Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11091">Aerial Vision-and-Language Navigation with Grid-based View Selection and Map Construction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerial Vision-and-Language Navigation (Aerial VLN) aims to obtain an unmanned aerial vehicle agent to navigate aerial 3D environments following human instruction. Compared to ground-based VLN, aerial VLN requires the agent to decide the next action in both horizontal and vertical directions based on the first-person view observations. Previous methods struggle to perform well due to the longer navigation path, more complicated 3D scenes, and the neglect of the interplay between vertical and horizontal actions. In this paper, we propose a novel grid-based view selection framework that formulates aerial VLN action prediction as a grid-based view selection task, incorporating vertical action prediction in a manner that accounts for the coupling with horizontal actions, thereby enabling effective altitude adjustments. We further introduce a grid-based bird's eye view map for aerial space to fuse the visual information in the navigation history, provide contextual scene information, and mitigate the impact of obstacles. Finally, a cross-modal transformer is adopted to explicitly align the long navigation history with the instruction. We demonstrate the superiority of our method in extensive experiments.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2403.17334.pdf' target='_blank'>https://arxiv.org/pdf/2403.17334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ganlong Zhao, Guanbin Li, Weikai Chen, Yizhou Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17334">OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Iterative Vision-and-Language Navigation (IVLN) introduce a more meaningful and practical paradigm of VLN by maintaining the agent's memory across tours of scenes. Although the long-term memory aligns better with the persistent nature of the VLN task, it poses more challenges on how to utilize the highly unstructured navigation memory with extremely sparse supervision. Towards this end, we propose OVER-NAV, which aims to go over and beyond the current arts of IVLN techniques. In particular, we propose to incorporate LLMs and open-vocabulary detectors to distill key information and establish correspondence between multi-modal signals. Such a mechanism introduces reliable cross-modal supervision and enables on-the-fly generalization to unseen scenes without the need of extra annotation and re-training. To fully exploit the interpreted navigation data, we further introduce a structured representation, coded Omnigraph, to effectively integrate multi-modal information along the tour. Accompanied with a novel omnigraph fusion mechanism, OVER-NAV is able to extract the most relevant knowledge from omnigraph for a more accurate navigating action. In addition, OVER-NAV seamlessly supports both discrete and continuous environments under a unified framework. We demonstrate the superiority of OVER-NAV in extensive experiments.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2211.14769.pdf' target='_blank'>https://arxiv.org/pdf/2211.14769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunchao Zhang, Zonglin Di, Kaiwen Zhou, Cihang Xie, Xin Eric Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.14769">Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated embodied agent learning protects the data privacy of individual visual environments by keeping data locally at each client (the individual environment) during training. However, since the local data is inaccessible to the server under federated learning, attackers may easily poison the training data of the local client to build a backdoor in the agent without notice. Deploying such an agent raises the risk of potential harm to humans, as the attackers may easily navigate and control the agent as they wish via the backdoor. Towards Byzantine-robust federated embodied agent learning, in this paper, we study the attack and defense for the task of vision-and-language navigation (VLN), where the agent is required to follow natural language instructions to navigate indoor environments. First, we introduce a simple but effective attack strategy, Navigation as Wish (NAW), in which the malicious client manipulates local trajectory data to implant a backdoor into the global model. Results on two VLN datasets (R2R and RxR) show that NAW can easily navigate the deployed VLN agent regardless of the language instruction, without affecting its performance on normal test sets. Then, we propose a new Prompt-Based Aggregation (PBA) to defend against the NAW attack in federated VLN, which provides the server with a ''prompt'' of the vision-and-language alignment variance between the benign and malicious clients so that they can be distinguished during training. We validate the effectiveness of the PBA method on protecting the global model from the NAW attack, which outperforms other state-of-the-art defense methods by a large margin in the defense metrics on R2R and RxR.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2509.25139.pdf' target='_blank'>https://arxiv.org/pdf/2509.25139.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Zhang, Tianyi Ma, Zun Wang, Yanyuan Qiao, Parisa Kordjamshidi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25139">Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrating large language models (LLMs) into embodied AI models is becoming increasingly prevalent. However, existing zero-shot LLM-based Vision-and-Language Navigation (VLN) agents either encode images as textual scene descriptions, potentially oversimplifying visual details, or process raw image inputs, which can fail to capture abstract semantics required for high-level reasoning. In this paper, we improve the navigation agent's contextual understanding by incorporating textual descriptions from multiple perspectives that facilitate analogical reasoning across images. By leveraging text-based analogical reasoning, the agent enhances its global scene understanding and spatial reasoning, leading to more accurate action decisions. We evaluate our approach on the R2R dataset, where our experiments demonstrate significant improvements in navigation performance.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2508.07642.pdf' target='_blank'>https://arxiv.org/pdf/2508.07642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Ma, Yue Zhang, Zehao Wang, Parisa Kordjamshidi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07642">Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) poses significant challenges in enabling agents to interpret natural language instructions and navigate complex 3D environments. While recent progress has been driven by large-scale pre-training and data augmentation, current methods still struggle to generalize to unseen scenarios, particularly when complex spatial and temporal reasoning is required. In this work, we propose SkillNav, a modular framework that introduces structured, skill-based reasoning into Transformer-based VLN agents. Our method decomposes navigation into a set of interpretable atomic skills (e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each handled by a specialized agent. We then introduce a novel zero-shot Vision-Language Model (VLM)-based router, which dynamically selects the most suitable agent at each time step by aligning sub-goals with visual observations and historical actions. SkillNav achieves a new state-of-the-art performance on the R2R benchmark and demonstrates strong generalization to the GSA-R2R benchmark that includes novel instruction styles and unseen environments.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2508.07642.pdf' target='_blank'>https://arxiv.org/pdf/2508.07642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Ma, Yue Zhang, Zehao Wang, Parisa Kordjamshidi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07642">Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) poses significant challenges for agents to interpret natural language instructions and navigate complex 3D environments. While recent progress has been driven by large-scale pre-training and data augmentation, current methods still struggle to generalize to unseen scenarios, particularly when complex spatial and temporal reasoning is required. In this work, we propose SkillNav, a modular framework that introduces structured, skill-based reasoning into Transformer-based VLN agents. Our method decomposes navigation into a set of interpretable atomic skills (e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each handled by a specialized agent. To support targeted skill training without manual data annotation, we construct a synthetic dataset pipeline that generates diverse, linguistically natural, skill-specific instruction-trajectory pairs. We then introduce a novel training-free Vision-Language Model (VLM)-based router, which dynamically selects the most suitable agent at each time step by aligning sub-goals with visual observations and historical actions. SkillNav obtains competitive results on commonly used benchmarks and establishes state-of-the-art generalization to the GSA-R2R, a benchmark with novel instruction styles and unseen environments.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2506.10756.pdf' target='_blank'>https://arxiv.org/pdf/2506.10756.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Zhang, Haosheng Yu, Jiaping Xiao, Mir Feroskhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10756">Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) is a long-standing challenge in autonomous robotics, aiming to empower agents with the ability to follow human instructions while navigating complex environments. Two key bottlenecks remain in this field: generalization to out-of-distribution environments and reliance on fixed discrete action spaces. To address these challenges, we propose Vision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles (UAVs) to execute language-guided flight. Without the requirement for localization or active ranging sensors, VLFly outputs continuous velocity commands purely from egocentric observations captured by an onboard monocular camera. The VLFly integrates three modules: an instruction encoder based on a large language model (LLM) that reformulates high-level language into structured prompts, a goal retriever powered by a vision-language model (VLM) that matches these prompts to goal images via vision-language similarity, and a waypoint planner that generates executable trajectories for real-time UAV control. VLFly is evaluated across diverse simulation environments without additional fine-tuning and consistently outperforms all baselines. Moreover, real-world VLN tasks in indoor and outdoor environments under direct and indirect instructions demonstrate that VLFly achieves robust open-vocabulary goal understanding and generalized navigation capabilities, even in the presence of abstract language input.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2408.10388.pdf' target='_blank'>https://arxiv.org/pdf/2408.10388.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Zhang, Parisa Kordjamshidi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10388">Narrowing the Gap between Vision and Action in Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The existing methods for Vision and Language Navigation in the Continuous Environment (VLN-CE) commonly incorporate a waypoint predictor to discretize the environment. This simplifies the navigation actions into a view selection task and improves navigation performance significantly compared to direct training using low-level actions. However, the VLN-CE agents are still far from the real robots since there are gaps between their visual perception and executed actions. First, VLN-CE agents that discretize the visual environment are primarily trained with high-level view selection, which causes them to ignore crucial spatial reasoning within the low-level action movements. Second, in these models, the existing waypoint predictors neglect object semantics and their attributes related to passibility, which can be informative in indicating the feasibility of actions. To address these two issues, we introduce a low-level action decoder jointly trained with high-level action prediction, enabling the current VLN agent to learn and ground the selected visual view to the low-level controls. Moreover, we enhance the current waypoint predictor by utilizing visual representations containing rich semantic information and explicitly masking obstacles based on humans' prior knowledge about the feasibility of actions. Empirically, our agent can improve navigation performance metrics compared to the strong baselines on both high-level and low-level actions.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2406.14240.pdf' target='_blank'>https://arxiv.org/pdf/2406.14240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jungdae Lee, Taiki Miyanishi, Shuhei Kurita, Koya Sakamoto, Daichi Azuma, Yutaka Matsuo, Nakamasa Inoue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14240">CityNav: A Large-Scale Dataset for Real-World Aerial Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation (VLN) aims to develop agents capable of navigating in realistic environments. While recent cross-modal training approaches have significantly improved navigation performance in both indoor and outdoor scenarios, aerial navigation over real-world cities remains underexplored primarily due to limited datasets and the difficulty of integrating visual and geographic information. To fill this gap, we introduce CityNav, the first large-scale real-world dataset for aerial VLN. Our dataset consists of 32,637 human demonstration trajectories, each paired with a natural language description, covering 4.65 km$^2$ across two real cities: Cambridge and Birmingham. In contrast to existing datasets composed of synthetic scenes such as AerialVLN, our dataset presents a unique challenge because agents must interpret spatial relationships between real-world landmarks and the navigation destination, making CityNav an essential benchmark for advancing aerial VLN. Furthermore, as an initial step toward addressing this challenge, we provide a methodology of creating geographic semantic maps that can be used as an auxiliary modality input during navigation. In our experiments, we compare performance of three representative aerial VLN agents (Seq2seq, CMA and AerialVLN models) and demonstrate that the semantic map representation significantly improves their navigation performance.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2406.05080.pdf' target='_blank'>https://arxiv.org/pdf/2406.05080.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Taioli, Stefano Rosa, Alberto Castellini, Lorenzo Natale, Alessio Del Bue, Alessandro Farinelli, Marco Cristani, Yiming Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05080">I2EDL: Interactive Instruction Error Detection and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the Vision-and-Language Navigation in Continuous Environments (VLN-CE) task, the human user guides an autonomous agent to reach a target goal via a series of low-level actions following a textual instruction in natural language. However, most existing methods do not address the likely case where users may make mistakes when providing such instruction (e.g. "turn left" instead of "turn right"). In this work, we address a novel task of Interactive VLN in Continuous Environments (IVLN-CE), which allows the agent to interact with the user during the VLN-CE navigation to verify any doubts regarding the instruction errors. We propose an Interactive Instruction Error Detector and Localizer (I2EDL) that triggers the user-agent interaction upon the detection of instruction errors during the navigation. We leverage a pre-trained module to detect instruction errors and pinpoint them in the instruction by cross-referencing the textual input and past observations. In such way, the agent is able to query the user for a timely correction, without demanding the user's cognitive load, as we locate the probable errors to a precise part of the instruction. We evaluate the proposed I2EDL on a dataset of instructions containing errors, and further devise a novel metric, the Success weighted by Interaction Number (SIN), to reflect both the navigation performance and the interaction effectiveness. We show how the proposed method can ask focused requests for corrections to the user, which in turn increases the navigation success, while minimizing the interactions.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2403.10700.pdf' target='_blank'>https://arxiv.org/pdf/2403.10700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Taioli, Stefano Rosa, Alberto Castellini, Lorenzo Natale, Alessio Del Bue, Alessandro Farinelli, Marco Cristani, Yiming Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10700">Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of the most intuitive yet challenging embodied AI tasks. Agents are tasked to navigate towards a target goal by executing a set of low-level actions, following a series of natural language instructions. All VLN-CE methods in the literature assume that language instructions are exact. However, in practice, instructions given by humans can contain errors when describing a spatial environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do not address this scenario, making the state-of-the-art methods in VLN-CE fragile in the presence of erroneous instructions from human users. For the first time, we propose a novel benchmark dataset that introduces various types of instruction errors considering potential human causes. This benchmark provides valuable insight into the robustness of VLN systems in continuous environments. We observe a noticeable performance drop (up to -25%) in Success Rate when evaluating the state-of-the-art VLN-CE methods on our benchmark. Moreover, we formally define the task of Instruction Error Detection and Localization, and establish an evaluation protocol on top of our benchmark dataset. We also propose an effective method, based on a cross-modal transformer architecture, that achieves the best performance in error detection and localization, compared to baselines. Surprisingly, our proposed method has revealed errors in the validation set of the two commonly used datasets for VLN-CE, i.e., R2R-CE and RxR-CE, demonstrating the utility of our technique in other tasks. Code and dataset available at https://intelligolabs.github.io/R2RIE-CE
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2402.02559.pdf' target='_blank'>https://arxiv.org/pdf/2402.02559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Zhang, Quan Guo, Parisa Kordjamshidi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02559">NavHint: Vision and Language Navigation Agent with a Hint Generator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing work on vision and language navigation mainly relies on navigation-related losses to establish the connection between vision and language modalities, neglecting aspects of helping the navigation agent build a deep understanding of the visual environment. In our work, we provide indirect supervision to the navigation agent through a hint generator that provides detailed visual descriptions. The hint generator assists the navigation agent in developing a global understanding of the visual environment. It directs the agent's attention toward related navigation details, including the relevant sub-instruction, potential challenges in recognition and ambiguities in grounding, and the targeted viewpoint description. To train the hint generator, we construct a synthetic dataset based on landmarks in the instructions and visible and distinctive objects in the visual environment. We evaluate our method on the R2R and R4R datasets and achieve state-of-the-art on several metrics. The experimental results demonstrate that generating hints not only enhances the navigation performance but also helps improve the interpretability of the agent's actions.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2311.17812.pdf' target='_blank'>https://arxiv.org/pdf/2311.17812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting Liu, Yue Hu, Wansen Wu, Youkai Wang, Kai Xu, Quanjun Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17812">DAP: Domain-aware Prompt Learning for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Following language instructions to navigate in unseen environments is a challenging task for autonomous embodied agents. With strong representation capabilities, pretrained vision-and-language models are widely used in VLN. However, most of them are trained on web-crawled general-purpose datasets, which incurs a considerable domain gap when used for VLN tasks. To address the problem, we propose a novel and model-agnostic domain-aware prompt learning (DAP) framework. For equipping the pretrained models with specific object-level and scene-level cross-modal alignment in VLN tasks, DAP applies a low-cost prompt tuning paradigm to learn soft visual prompts for extracting in-domain image semantics. Specifically, we first generate a set of in-domain image-text pairs with the help of the CLIP model. Then we introduce soft visual prompts in the input space of the visual encoder in a pretrained model. DAP injects in-domain visual knowledge into the visual encoder of the pretrained model in an efficient way. Experimental results on both R2R and REVERIE show the superiority of DAP compared to existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2309.03661.pdf' target='_blank'>https://arxiv.org/pdf/2309.03661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting Liu, Yue Hu, Wansen Wu, Youkai Wang, Kai Xu, Quanjun Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03661">Prompt-based Context- and Domain-aware Pretraining for Vision and Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pretrained visual-language models have extensive world knowledge and are widely used in visual and language navigation (VLN). However, they are not sensitive to indoor scenarios for VLN tasks. Another challenge for VLN is how the agent understands the contextual relations between actions on a path and performs cross-modal alignment sequentially. In this paper, we propose a novel Prompt-bAsed coNtext- and inDoor-Aware (PANDA) pretraining framework to address these problems. It performs prompting in two stages. In the indoor-aware stage, we apply an efficient tuning paradigm to learn deep visual prompts from an indoor dataset, in order to augment pretrained models with inductive biases towards indoor environments. This can enable more sample-efficient adaptation for VLN agents. Furthermore, in the context-aware stage, we design a set of hard context prompts to capture the sequence-level semantics in the instruction. They enable further tuning of the pretrained models via contrastive learning. Experimental results on both R2R and REVERIE show the superiority of PANDA compared to existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2302.09230.pdf' target='_blank'>https://arxiv.org/pdf/2302.09230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Zhang, Parisa Kordjamshidi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09230">VLN-Trans: Translator for the Vision and Language Navigation Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language understanding is essential for the navigation agent to follow instructions. We observe two kinds of issues in the instructions that can make the navigation task challenging: 1. The mentioned landmarks are not recognizable by the navigation agent due to the different vision abilities of the instructor and the modeled agent. 2. The mentioned landmarks are applicable to multiple targets, thus not distinctive for selecting the target among the candidate viewpoints. To deal with these issues, we design a translator module for the navigation agent to convert the original instructions into easy-to-follow sub-instruction representations at each step. The translator needs to focus on the recognizable and distinctive landmarks based on the agent's visual abilities and the observed visual environment. To achieve this goal, we create a new synthetic sub-instruction dataset and design specific tasks to train the translator and the navigation agent. We evaluate our approach on Room2Room~(R2R), Room4room~(R4R), and Room2Room Last (R2R-Last) datasets and achieve state-of-the-art results on multiple benchmarks.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2509.22548.pdf' target='_blank'>https://arxiv.org/pdf/2509.22548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuang Zeng, Dekang Qi, Xinyuan Chang, Feng Xiong, Shichao Xie, Xiaolong Wu, Shiyi Liang, Mu Xu, Xing Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22548">JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain's semantic understanding and the right brain's spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2412.05552.pdf' target='_blank'>https://arxiv.org/pdf/2412.05552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengze Zhou, Yicong Hong, Zun Wang, Chongyang Zhao, Mohit Bansal, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05552">SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The academic field of learning instruction-guided visual navigation can be generally categorized into high-level category-specific search and low-level language-guided navigation, depending on the granularity of language instruction, in which the former emphasizes the exploration process, while the latter concentrates on following detailed textual commands. Despite the differing focuses of these tasks, the underlying requirements of interpreting instructions, comprehending the surroundings, and inferring action decisions remain consistent. This paper consolidates diverse navigation tasks into a unified and generic framework -- we investigate the core difficulties of sharing general knowledge and exploiting task-specific capabilities in learning navigation and propose a novel State-Adaptive Mixture of Experts (SAME) model that effectively enables an agent to infer decisions based on different-granularity language and dynamic observations. Powered by SAME, we present a versatile agent capable of addressing seven navigation tasks simultaneously that outperforms or achieves highly comparable performance to task-specific agents.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2410.17267.pdf' target='_blank'>https://arxiv.org/pdf/2410.17267.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seongjun Jeong, Gi-Cheon Kang, Joochan Kim, Byoung-Tak Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17267">Zero-Shot Vision-and-Language Navigation with Collision Mitigation in Continuous Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose the zero-shot Vision-and-Language Navigation with Collision Mitigation (VLN-CM), which takes these considerations. VLN-CM is composed of four modules and predicts the direction and distance of the next movement at each step. We utilize large foundation models for each modules. To select the direction, we use the Attention Spot Predictor (ASP), View Selector (VS), and Progress Monitor (PM). The ASP employs a Large Language Model (e.g. ChatGPT) to split navigation instructions into attention spots, which are objects or scenes at the location to move to (e.g. a yellow door). The VS selects from panorama images provided at 30-degree intervals the one that includes the attention spot, using CLIP similarity. We then choose the angle of the selected image as the direction to move in. The PM uses a rule-based approach to decide which attention spot to focus on next, among multiple spots derived from the instructions. If the similarity between the current attention spot and the visual observations decreases consecutively at each step, the PM determines that the agent has passed the current spot and moves on to the next one. For selecting the distance to move, we employed the Open Map Predictor (OMP). The OMP uses panorama depth information to predict an occupancy mask. We then selected a collision-free distance in the predicted direction based on the occupancy mask. We evaluated our method using the validation data of VLN-CE. Our approach showed better performance than several baseline methods, and the OPM was effective in mitigating collisions for the agent.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2408.05090.pdf' target='_blank'>https://arxiv.org/pdf/2408.05090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huilin Tian, Jingke Meng, Wei-Shi Zheng, Yuan-Ming Li, Junkai Yan, Yunong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05090">Loc4Plan: Locating Before Planning for Outdoor Vision and Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision and Language Navigation (VLN) is a challenging task that requires agents to understand instructions and navigate to the destination in a visual environment.One of the key challenges in outdoor VLN is keeping track of which part of the instruction was completed. To alleviate this problem, previous works mainly focus on grounding the natural language to the visual input, but neglecting the crucial role of the agent's spatial position information in the grounding process. In this work, we first explore the substantial effect of spatial position locating on the grounding of outdoor VLN, drawing inspiration from human navigation. In real-world navigation scenarios, before planning a path to the destination, humans typically need to figure out their current location. This observation underscores the pivotal role of spatial localization in the navigation process. In this work, we introduce a novel framework, Locating be for Planning (Loc4Plan), designed to incorporate spatial perception for action planning in outdoor VLN tasks. The main idea behind Loc4Plan is to perform the spatial localization before planning a decision action based on corresponding guidance, which comprises a block-aware spatial locating (BAL) module and a spatial-aware action planning (SAP) module. Specifically, to help the agent perceive its spatial location in the environment, we propose to learn a position predictor that measures how far the agent is from the next intersection for reflecting its position, which is achieved by the BAL module. After the locating process, we propose the SAP module to incorporate spatial information to ground the corresponding guidance and enhance the precision of action planning. Extensive experiments on the Touchdown and map2seq datasets show that the proposed Loc4Plan outperforms the SOTA methods.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2407.12366.pdf' target='_blank'>https://arxiv.org/pdf/2407.12366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengze Zhou, Yicong Hong, Zun Wang, Xin Eric Wang, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12366">NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Capitalizing on the remarkable advancements in Large Language Models (LLMs), there is a burgeoning initiative to harness LLMs for instruction following robotic navigation. Such a trend underscores the potential of LLMs to generalize navigational reasoning and diverse language understanding. However, a significant discrepancy in agent performance is observed when integrating LLMs in the Vision-and-Language navigation (VLN) tasks compared to previous downstream specialist models. Furthermore, the inherent capacity of language to interpret and facilitate communication in agent interactions is often underutilized in these integrations. In this work, we strive to bridge the divide between VLN-specialized models and LLM-based navigation paradigms, while maintaining the interpretative prowess of LLMs in generating linguistic navigational reasoning. By aligning visual content in a frozen LLM, we encompass visual observation comprehension for LLMs and exploit a way to incorporate LLMs and navigation policy networks for effective action predictions and navigational reasoning. We demonstrate the data efficiency of the proposed methods and eliminate the gap between LM-based agents and state-of-the-art VLN specialists.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2403.15049.pdf' target='_blank'>https://arxiv.org/pdf/2403.15049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seongjun Jeong, Gi-Cheon Kang, Seongho Choi, Joochan Kim, Byoung-Tak Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15049">Continual Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In developing Vision-and-Language Navigation (VLN) agents that navigate to a destination using natural language instructions and visual cues, current studies largely assume a \textit{train-once-deploy-once strategy}. We argue that this kind of strategy is less realistic, as deployed VLN agents are expected to encounter novel environments continuously through their lifetime. To facilitate more realistic setting for VLN agents, we propose Continual Vision-and-Language Navigation (CVLN) paradigm for agents to continually learn and adapt to changing environments. In CVLN, the agents are trained and evaluated incrementally across multiple \textit{scene domains} (i.e., environments). We present two CVLN learning setups to consider diverse forms of natural language instructions: Initial-instruction based CVLN, focused on navigation via initial-instruction interpretation, and dialogue-based CVLN, designed for navigation through dialogue with other agents. We introduce two simple yet effective baseline methods, tailored to the sequential decision-making needs of CVLN: Perplexity Replay (PerpR) and Episodic Self-Replay (ESR), both employing a rehearsal mechanism. PerpR selects replay episodes based on episode difficulty, while ESR stores and revisits action logits from individual episode steps during training to refine learning. Experimental results indicate that while existing continual learning methods are insufficient for CVLN, PerpR and ESR outperform the comparison methods by effectively utilizing replay memory.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2311.17280.pdf' target='_blank'>https://arxiv.org/pdf/2311.17280.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wang Zhu, Ishika Singh, Yuan Huang, Robin Jia, Jesse Thomason
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17280">Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data augmentation via back-translation is common when pretraining Vision-and-Language Navigation (VLN) models, even though the generated instructions are noisy. But: does that noise matter? We find that nonsensical or irrelevant language instructions during pretraining can have little effect on downstream performance for both HAMT and VLN-BERT on R2R, and is still better than only using clean, human data. To underscore these results, we concoct an efficient augmentation method, Unigram + Object, which generates nonsensical instructions that nonetheless improve downstream performance. Our findings suggest that what matters for VLN R2R pretraining is the quantity of visual trajectories, not the quality of instructions.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2305.17102.pdf' target='_blank'>https://arxiv.org/pdf/2305.17102.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyang Huo, Qiang Sun, Boyan Jiang, Haitao Lin, Yanwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17102">GeoVLN: Learning Geometry-Enhanced Visual Representation with Slot Attention for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing works solving Room-to-Room VLN problem only utilize RGB images and do not consider local context around candidate views, which lack sufficient visual cues about surrounding environment. Moreover, natural language contains complex semantic information thus its correlations with visual inputs are hard to model merely with cross attention. In this paper, we propose GeoVLN, which learns Geometry-enhanced visual representation based on slot attention for robust Visual-and-Language Navigation. The RGB images are compensated with the corresponding depth maps and normal maps predicted by Omnidata as visual inputs. Technically, we introduce a two-stage module that combine local slot attention and CLIP model to produce geometry-enhanced representation from such input. We employ V&L BERT to learn a cross-modal representation that incorporate both language and vision informations. Additionally, a novel multiway attention module is designed, encouraging different phrases of input instruction to exploit the most related features from visual input. Extensive experiments demonstrate the effectiveness of our newly designed modules and show the compelling performance of the proposed method.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2305.16986.pdf' target='_blank'>https://arxiv.org/pdf/2305.16986.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengze Zhou, Yicong Hong, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16986">NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2406.19967.pdf' target='_blank'>https://arxiv.org/pdf/2406.19967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tzuf Paz-Argaman, John Palowitch, Sayali Kulkarni, Reut Tsarfaty, Jason Baldridge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19967">Into the Unknown: Generating Geospatial Descriptions for New Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Similar to vision-and-language navigation (VLN) tasks that focus on bridging the gap between vision and language for embodied navigation, the new Rendezvous (RVS) task requires reasoning over allocentric spatial relationships (independent of the observer's viewpoint) using non-sequential navigation instructions and maps. However, performance substantially drops in new environments with no training data. Using opensource descriptions paired with coordinates (e.g., Wikipedia) provides training data but suffers from limited spatially-oriented text resulting in low geolocation resolution. We propose a large-scale augmentation method for generating high-quality synthetic data for new environments using readily available geospatial data. Our method constructs a grounded knowledge-graph, capturing entity relationships. Sampled entities and relations (`shop north of school') generate navigation instructions via (i) generating numerous templates using context-free grammar (CFG) to embed specific entities and relations; (ii) feeding the entities and relation into a large language model (LLM) for instruction generation. A comprehensive evaluation on RVS, showed that our approach improves the 100-meter accuracy by 45.83% on unseen environments. Furthermore, we demonstrate that models trained with CFG-based augmentation achieve superior performance compared with those trained with LLM-based augmentation, both in unseen and seen environments. These findings suggest that the potential advantages of explicitly structuring spatial information for text-based geospatial reasoning in previously unknown, can unlock data-scarce scenarios.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2406.01256.pdf' target='_blank'>https://arxiv.org/pdf/2406.01256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bahram Mohammadi, Yicong Hong, Yuankai Qi, Qi Wu, Shirui Pan, Javen Qinfeng Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01256">Augmented Commonsense Knowledge for Remote Object Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vision-and-language navigation (VLN) task necessitates an agent to perceive the surroundings, follow natural language instructions, and act in photo-realistic unseen environments. Most of the existing methods employ the entire image or object features to represent navigable viewpoints. However, these representations are insufficient for proper action prediction, especially for the REVERIE task, which uses concise high-level instructions, such as ''Bring me the blue cushion in the master bedroom''. To address enhancing representation, we propose an augmented commonsense knowledge model (ACK) to leverage commonsense information as a spatio-temporal knowledge graph for improving agent navigation. Specifically, the proposed approach involves constructing a knowledge base by retrieving commonsense information from ConceptNet, followed by a refinement module to remove noisy and irrelevant knowledge. We further present ACK which consists of knowledge graph-aware cross-modal and concept aggregation modules to enhance visual representation and visual-textual data alignment by integrating visible objects, commonsense knowledge, and concept history, which includes object and knowledge temporal information. Moreover, we add a new pipeline for the commonsense-based decision-making process which leads to more accurate local action prediction. Experimental results demonstrate our proposed model noticeably outperforms the baseline and archives the state-of-the-art on the REVERIE benchmark.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2308.03244.pdf' target='_blank'>https://arxiv.org/pdf/2308.03244.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chongyang Zhao, Yuankai Qi, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03244">Mind the Gap: Improving Success Rate of Vision-and-Language Navigation by Revisiting Oracle Success Routes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) aims to navigate to the target location by following a given instruction. Unlike existing methods focused on predicting a more accurate action at each step in navigation, in this paper, we make the first attempt to tackle a long-ignored problem in VLN: narrowing the gap between Success Rate (SR) and Oracle Success Rate (OSR). We observe a consistently large gap (up to 9%) on four state-of-the-art VLN methods across two benchmark datasets: R2R and REVERIE. The high OSR indicates the robot agent passes the target location, while the low SR suggests the agent actually fails to stop at the target location at last. Instead of predicting actions directly, we propose to mine the target location from a trajectory given by off-the-shelf VLN models. Specially, we design a multi-module transformer-based model for learning compact discriminative trajectory viewpoint representation, which is used to predict the confidence of being a target location as described in the instruction. The proposed method is evaluated on three widely-adopted datasets: R2R, REVERIE and NDH, and shows promising results, demonstrating the potential for more future research.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2509.22653.pdf' target='_blank'>https://arxiv.org/pdf/2509.22653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chih Yao Hu, Yang-Sen Lin, Yuna Lee, Chih-Hai Su, Jie-Ying Lee, Shr-Ruei Tsai, Chin-Yang Lin, Kuan-Wen Chen, Tsung-Wei Ke, Yu-Lun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22653">See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs). SPF is capable of navigating to any goal based on any type of free-form instructions in any kind of environment. In contrast to existing VLM-based approaches that treat action prediction as a text generation task, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. SPF harnesses VLMs to decompose vague language instructions into iterative annotation of 2D waypoints on the input image. Along with the predicted traveling distance, SPF transforms predicted 2D waypoints into 3D displacement vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the traveling distance to facilitate more efficient navigation. Notably, SPF performs navigation in a closed-loop control manner, enabling UAVs to follow dynamic targets in dynamic environments. SPF sets a new state of the art in DRL simulation benchmark, outperforming the previous best method by an absolute margin of 63%. In extensive real-world evaluations, SPF outperforms strong baselines by a large margin. We also conduct comprehensive ablation studies to highlight the effectiveness of our design choice. Lastly, SPF shows remarkable generalization to different VLMs. Project page: https://spf-web.pages.dev
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2412.13026.pdf' target='_blank'>https://arxiv.org/pdf/2412.13026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karan Wanchoo, Xiaoye Zuo, Hannah Gonzalez, Soham Dan, Georgios Georgakis, Dan Roth, Kostas Daniilidis, Eleni Miltsakaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13026">NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for Vision and Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present NAVCON, a large-scale annotated Vision-Language Navigation (VLN) corpus built on top of two popular datasets (R2R and RxR). The paper introduces four core, cognitively motivated and linguistically grounded, navigation concepts and an algorithm for generating large-scale silver annotations of naturally occurring linguistic realizations of these concepts in navigation instructions. We pair the annotated instructions with video clips of an agent acting on these instructions. NAVCON contains 236, 316 concept annotations for approximately 30, 0000 instructions and 2.7 million aligned images (from approximately 19, 000 instructions) showing what the agent sees when executing an instruction. To our knowledge, this is the first comprehensive resource of navigation concepts. We evaluated the quality of the silver annotations by conducting human evaluation studies on NAVCON samples. As further validation of the quality and usefulness of the resource, we trained a model for detecting navigation concepts and their linguistic realizations in unseen instructions. Additionally, we show that few-shot learning with GPT-4o performs well on this task using large-scale silver annotations of NAVCON.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2407.07392.pdf' target='_blank'>https://arxiv.org/pdf/2407.07392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chashi Mahiul Islam, Shaeke Salman, Montasir Shams, Xiuwen Liu, Piyush Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07392">Malicious Path Manipulations via Exploitation of Representation Vulnerabilities of Vision-Language Navigation Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building on the unprecedented capabilities of large language models for command understanding and zero-shot recognition of multi-modal vision-language transformers, visual language navigation (VLN) has emerged as an effective way to address multiple fundamental challenges toward a natural language interface to robot navigation. However, such vision-language models are inherently vulnerable due to the lack of semantic meaning of the underlying embedding space. Using a recently developed gradient based optimization procedure, we demonstrate that images can be modified imperceptibly to match the representation of totally different images and unrelated texts for a vision-language model. Building on this, we develop algorithms that can adversarially modify a minimal number of images so that the robot will follow a route of choice for commands that require a number of landmarks. We demonstrate that experimentally using a recently proposed VLN system; for a given navigation command, a robot can be made to follow drastically different routes. We also develop an efficient algorithm to detect such malicious modifications reliably based on the fact that the adversarially modified images have much higher sensitivity to added Gaussian noise than the original images.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2406.13636.pdf' target='_blank'>https://arxiv.org/pdf/2406.13636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abrar Anwar, Rohan Gupta, Jesse Thomason
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13636">Contrast Sets for Evaluating Language-Guided Robot Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot evaluations in language-guided, real world settings are time-consuming and often sample only a small space of potential instructions across complex scenes. In this work, we introduce contrast sets for robotics as an approach to make small, but specific, perturbations to otherwise independent, identically distributed (i.i.d.) test instances. We investigate the relationship between experimenter effort to carry out an evaluation and the resulting estimated test performance as well as the insights that can be drawn from performance on perturbed instances. We use the relative performance change of different contrast set perturbations to characterize policies at reduced experimenter effort in both a simulated manipulation task and a physical robot vision-and-language navigation task. We encourage the use of contrast set evaluations as a more informative alternative to small scale, i.i.d. demonstrations on physical robots, and as a scalable alternative to industry-scale real world evaluations.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2210.03087.pdf' target='_blank'>https://arxiv.org/pdf/2210.03087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacob Krantz, Shurjo Banerjee, Wang Zhu, Jason Corso, Peter Anderson, Stefan Lee, Jesse Thomason
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.03087">Iterative Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Iterative Vision-and-Language Navigation (IVLN), a paradigm for evaluating language-guided agents navigating in a persistent environment over time. Existing Vision-and-Language Navigation (VLN) benchmarks erase the agent's memory at the beginning of every episode, testing the ability to perform cold-start navigation with no prior information. However, deployed robots occupy the same environment for long periods of time. The IVLN paradigm addresses this disparity by training and evaluating VLN agents that maintain memory across tours of scenes that consist of up to 100 ordered instruction-following Room-to-Room (R2R) episodes, each defined by an individual language instruction and a target path. We present discrete and continuous Iterative Room-to-Room (IR2R) benchmarks comprising about 400 tours each in 80 indoor scenes. We find that extending the implicit memory of high-performing transformer VLN agents is not sufficient for IVLN, but agents that build maps can benefit from environment persistence, motivating a renewed focus on map-building agents in VLN.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2509.12618.pdf' target='_blank'>https://arxiv.org/pdf/2509.12618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zekai Zhang, Weiye Zhu, Hewei Pan, Xiangchen Wang, Rongtao Xu, Xing Sun, Feng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12618">ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Vision-and-Language Navigation (VLN) task requires an agent to follow natural language instructions and navigate through complex environments. Existing MLLM-based VLN methods primarily rely on imitation learning (IL) and often use DAgger for post-training to mitigate covariate shift. While effective, these approaches incur substantial data collection and training costs. Reinforcement learning (RL) offers a promising alternative. However, prior VLN RL methods lack dynamic interaction with the environment and depend on expert trajectories for reward shaping, rather than engaging in open-ended active exploration. This restricts the agent's ability to discover diverse and plausible navigation routes. To address these limitations, we propose ActiveVLN, a VLN framework that explicitly enables active exploration through multi-turn RL. In the first stage, a small fraction of expert trajectories is used for IL to bootstrap the agent. In the second stage, the agent iteratively predicts and executes actions, automatically collects diverse trajectories, and optimizes multiple rollouts via the GRPO objective. To further improve RL efficiency, we introduce a dynamic early-stopping strategy to prune long-tail or likely failed trajectories, along with additional engineering optimizations. Experiments show that ActiveVLN achieves the largest performance gains over IL baselines compared to both DAgger-based and prior RL-based post-training methods, while reaching competitive performance with state-of-the-art approaches despite using a smaller model. Code and data will be released soon.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2502.07306.pdf' target='_blank'>https://arxiv.org/pdf/2502.07306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Navid Rajabi, Jana Kosecka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07306">TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose a modular approach for the Vision-Language Navigation (VLN) task by decomposing the problem into four sub-modules that use state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) in a zero-shot setting. Given navigation instruction in natural language, we first prompt LLM to extract the landmarks and the order in which they are visited. Assuming the known model of the environment, we retrieve the top-k locations of the last landmark and generate $k$ path hypotheses from the starting location to the last landmark using the shortest path algorithm on the topological map of the environment. Each path hypothesis is represented by a sequence of panoramas. We then use dynamic programming to compute the alignment score between the sequence of panoramas and the sequence of landmark names, which match scores obtained from VLM. Finally, we compute the nDTW metric between the hypothesis that yields the highest alignment score to evaluate the path fidelity. We demonstrate superior performance compared to other approaches that use joint semantic maps like VLMaps on the complex R2R-Habitat instruction dataset and quantify in detail the effect of visual grounding on navigation performance.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2412.02795.pdf' target='_blank'>https://arxiv.org/pdf/2412.02795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijiao Yang, Xiangxi Shi, Eric Slyman, Stefan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02795">Hijacking Vision-and-Language Navigation Agents with Adversarial Environmental Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assistive embodied agents that can be instructed in natural language to perform tasks in open-world environments have the potential to significantly impact labor tasks like manufacturing or in-home care -- benefiting the lives of those who come to depend on them. In this work, we consider how this benefit might be hijacked by local modifications in the appearance of the agent's operating environment. Specifically, we take the popular Vision-and-Language Navigation (VLN) task as a representative setting and develop a whitebox adversarial attack that optimizes a 3D attack object's appearance to induce desired behaviors in pretrained VLN agents that observe it in the environment. We demonstrate that the proposed attack can cause VLN agents to ignore their instructions and execute alternative actions after encountering the attack object -- even for instructions and agent paths not considered when optimizing the attack. For these novel settings, we find our attacks can induce early-termination behaviors or divert an agent along an attacker-defined multi-step trajectory. Under both conditions, environmental attacks significantly reduce agent capabilities to successfully follow user instructions.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2411.05831.pdf' target='_blank'>https://arxiv.org/pdf/2411.05831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Savitha Sam Abraham, Sourav Garg, Feras Dayoub
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05831">To Ask or Not to Ask? Detecting Absence of Information in Vision and Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research in Vision Language Navigation (VLN) has overlooked the development of agents' inquisitive abilities, which allow them to ask clarifying questions when instructions are incomplete. This paper addresses how agents can recognize "when" they lack sufficient information, without focusing on "what" is missing, particularly in VLN tasks with vague instructions. Equipping agents with this ability enhances efficiency by reducing potential digressions and seeking timely assistance. The challenge in identifying such uncertain points is balancing between being overly cautious (high recall) and overly confident (high precision). We propose an attention-based instruction-vagueness estimation module that learns associations between instructions and the agent's trajectory. By leveraging instruction-to-path alignment information during training, the module's vagueness estimation performance improves by around 52% in terms of precision-recall balance. In our ablative experiments, we also demonstrate the effectiveness of incorporating this additional instruction-to-path attention network alongside the cross-modal attention networks within the navigator module. Our results show that the attention scores from the instruction-to-path attention network serve as better indicators for estimating vagueness.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2408.08301.pdf' target='_blank'>https://arxiv.org/pdf/2408.08301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Senthil Hariharan Arul, Dhruva Kumar, Vivek Sugirtharaj, Richard Kim, Xuewei, Qi, Rajasimman Madhivanan, Arnie Sen, Dinesh Manocha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08301">VLPG-Nav: Object Navigation Using Visual Language Pose Graph and Object Localization Probability Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present VLPG-Nav, a visual language navigation method for guiding robots to specified objects within household scenes. Unlike existing methods primarily focused on navigating the robot toward objects, our approach considers the additional challenge of centering the object within the robot's camera view. Our method builds a visual language pose graph (VLPG) that functions as a spatial map of VL embeddings. Given an open vocabulary object query, we plan a viewpoint for object navigation using the VLPG. Despite navigating to the viewpoint, real-world challenges like object occlusion, displacement, and the robot's localization error can prevent visibility. We build an object localization probability map that leverages the robot's current observations and prior VLPG. When the object isn't visible, the probability map is updated and an alternate viewpoint is computed. In addition, we propose an object-centering formulation that locally adjusts the robot's pose to center the object in the camera view. We evaluate the effectiveness of our approach through simulations and real-world experiments, evaluating its ability to successfully view and center the object within the camera field of view. VLPG-Nav demonstrates improved performance in locating the object, navigating around occlusions, and centering the object within the robot's camera view, outperforming the selected baselines in the evaluation metrics.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2210.03112.pdf' target='_blank'>https://arxiv.org/pdf/2210.03112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aishwarya Kamath, Peter Anderson, Su Wang, Jing Yu Koh, Alexander Ku, Austin Waters, Yinfei Yang, Jason Baldridge, Zarana Parekh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.03112">A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies in Vision-and-Language Navigation (VLN) train RL agents to execute natural-language navigation instructions in photorealistic environments, as a step towards robots that can follow human instructions. However, given the scarcity of human instruction data and limited diversity in the training environments, these agents still struggle with complex language grounding and spatial language understanding. Pretraining on large text and image-text datasets from the web has been extensively explored but the improvements are limited. We investigate large-scale augmentation with synthetic instructions. We take 500+ indoor environments captured in densely-sampled 360 degree panoramas, construct navigation trajectories through these panoramas, and generate a visually-grounded instruction for each trajectory using Marky, a high-quality multilingual navigation instruction generator. We also synthesize image observations from novel viewpoints using an image-to-image GAN. The resulting dataset of 4.2M instruction-trajectory pairs is two orders of magnitude larger than existing human-annotated datasets, and contains a wider variety of environments and viewpoints. To efficiently leverage data at this scale, we train a simple transformer agent with imitation learning. On the challenging RxR dataset, our approach outperforms all existing RL agents, improving the state-of-the-art NDTW from 71.1 to 79.1 in seen environments, and from 64.6 to 66.8 in unseen test environments. Our work points to a new path to improving instruction-following agents, emphasizing large-scale imitation learning and the development of synthetic instruction generation capabilities.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2505.11383.pdf' target='_blank'>https://arxiv.org/pdf/2505.11383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Wang, Seungjun Lee, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11383">Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) is a core task where embodied agents leverage their spatial mobility to navigate in 3D environments toward designated destinations based on natural language instructions. Recently, video-language large models (Video-VLMs) with strong generalization capabilities and rich commonsense knowledge have shown remarkable performance when applied to VLN tasks. However, these models still encounter the following challenges when applied to real-world 3D navigation: 1) Insufficient understanding of 3D geometry and spatial semantics; 2) Limited capacity for large-scale exploration and long-term environmental memory; 3) Poor adaptability to dynamic and changing environments.To address these limitations, we propose Dynam3D, a dynamic layered 3D representation model that leverages language-aligned, generalizable, and hierarchical 3D representations as visual input to train 3D-VLM in navigation action prediction. Given posed RGB-D images, our Dynam3D projects 2D CLIP features into 3D space and constructs multi-level 3D patch-instance-zone representations for 3D geometric and semantic understanding with a dynamic and layer-wise update strategy. Our Dynam3D is capable of online encoding and localization of 3D instances, and dynamically updates them in changing environments to provide large-scale exploration and long-term memory capabilities for navigation. By leveraging large-scale 3D-language pretraining and task-specific adaptation, our Dynam3D sets new state-of-the-art performance on VLN benchmarks including R2R-CE, REVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for pre-exploration, lifelong memory, and real-world robot validate the effectiveness of practical deployment.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2502.11142.pdf' target='_blank'>https://arxiv.org/pdf/2502.11142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Wang, Yaohui Zhu, Gim Hee Lee, Yachun Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11142">NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) is an essential skill for embodied agents, allowing them to navigate in 3D environments following natural language instructions. High-performance navigation models require a large amount of training data, the high cost of manually annotating data has seriously hindered this field. Therefore, some previous methods translate trajectory videos into step-by-step instructions for expanding data, but such instructions do not match well with users' communication styles that briefly describe destinations or state specific needs. Moreover, local navigation trajectories overlook global context and high-level task planning. To address these issues, we propose NavRAG, a retrieval-augmented generation (RAG) framework that generates user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical scene description tree for 3D scene understanding from global layout to local details, then simulates various user roles with specific demands to retrieve from the scene tree, generating diverse instructions with LLM. We annotate over 2 million navigation instructions across 861 scenes and evaluate the data quality and navigation performance of trained models.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2411.17030.pdf' target='_blank'>https://arxiv.org/pdf/2411.17030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Wang, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17030">g3D-LF: Generalizable 3D-Language Feature Fields for Embodied Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Generalizable 3D-Language Feature Fields (g3D-LF), a 3D representation model pre-trained on large-scale 3D-language dataset for embodied tasks. Our g3D-LF processes posed RGB-D images from agents to encode feature fields for: 1) Novel view representation predictions from any position in the 3D scene; 2) Generations of BEV maps centered on the agent; 3) Querying targets using multi-granularity language within the above-mentioned representations. Our representation can be generalized to unseen environments, enabling real-time construction and dynamic updates. By volume rendering latent features along sampled rays and integrating semantic and spatial relationships through multiscale encoders, our g3D-LF produces representations at different scales and perspectives, aligned with multi-granularity language, via multi-level contrastive learning. Furthermore, we prepare a large-scale 3D-language dataset to align the representations of the feature fields with language. Extensive experiments on Vision-and-Language Navigation under both Panorama and Monocular settings, Zero-shot Object Navigation, and Situated Question Answering tasks highlight the significant advantages and effectiveness of our g3D-LF for embodied tasks.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2411.14811.pdf' target='_blank'>https://arxiv.org/pdf/2411.14811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Song, Mario Gianni, Chenguang Yang, Kunyang Lin, Te-Chuan Chiu, Anh Nguyen, Chun-Yi Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14811">Fine-Grained Alignment in Vision-and-Language Navigation through Bayesian Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenge of fine-grained alignment in Vision-and-Language Navigation (VLN) tasks, where robots navigate realistic 3D environments based on natural language instructions. Current approaches use contrastive learning to align language with visual trajectory sequences. Nevertheless, they encounter difficulties with fine-grained vision negatives. To enhance cross-modal embeddings, we introduce a novel Bayesian Optimization-based adversarial optimization framework for creating fine-grained contrastive vision samples. To validate the proposed methodology, we conduct a series of experiments to assess the effectiveness of the enriched embeddings on fine-grained vision negatives. We conduct experiments on two common VLN benchmarks R2R and REVERIE, experiments on the them demonstrate that these embeddings benefit navigation, and can lead to a promising performance enhancement. Our source code and trained models are available at: https://anonymous.4open.science/r/FGVLN.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2409.05593.pdf' target='_blank'>https://arxiv.org/pdf/2409.05593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muraleekrishna Gopinathan, Jumana Abu-Khalaf, David Suter, Martin Masek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05593">StratXplore: Strategic Novelty-seeking and Instruction-aligned Exploration for Vision and Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation requires robots to understand and interact with the environment based on given tasks. Vision-Language Navigation (VLN) is an embodied navigation task, where a robot navigates within a previously seen and unseen environment, based on linguistic instruction and visual inputs. VLN agents need access to both local and global action spaces; former for immediate decision making and the latter for recovering from navigational mistakes. Prior VLN agents rely only on instruction-viewpoint alignment for local and global decision making and back-track to a previously visited viewpoint, if the instruction and its current viewpoint mismatches. These methods are prone to mistakes, due to the complexity of the instruction and partial observability of the environment. We posit that, back-tracking is sub-optimal and agent that is aware of its mistakes can recover efficiently. For optimal recovery, exploration should be extended to unexplored viewpoints (or frontiers). The optimal frontier is a recently observed but unexplored viewpoint that aligns with the instruction and is novel. We introduce a memory-based and mistake-aware path planning strategy for VLN agents, called \textit{StratXplore}, that presents global and local action planning to select the optimal frontier for path correction. The proposed method collects all past actions and viewpoint features during navigation and then selects the optimal frontier suitable for recovery. Experimental results show this simple yet effective strategy improves the success rate on two VLN datasets with different task complexities.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2403.15691.pdf' target='_blank'>https://arxiv.org/pdf/2403.15691.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Huang, Yanwei Zheng, Chuanlin Lan, Xinpeng Zhao, Yifei Zou, Dongxiao yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15691">Temporal-Spatial Object Relations Modeling for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) is a challenging task where an agent is required to navigate to a natural language described location via vision observations. The navigation abilities of the agent can be enhanced by the relations between objects, which are usually learned using internal objects or external datasets. The relationships between internal objects are modeled employing graph convolutional network (GCN) in traditional studies. However, GCN tends to be shallow, limiting its modeling ability. To address this issue, we utilize a cross attention mechanism to learn the connections between objects over a trajectory, which takes temporal continuity into account, termed as Temporal Object Relations (TOR). The external datasets have a gap with the navigation environment, leading to inaccurate modeling of relations. To avoid this problem, we construct object connections based on observations from all viewpoints in the navigational environment, which ensures complete spatial coverage and eliminates the gap, called Spatial Object Relations (SOR). Additionally, we observe that agents may repeatedly visit the same location during navigation, significantly hindering their performance. For resolving this matter, we introduce the Turning Back Penalty (TBP) loss function, which penalizes the agent's repetitive visiting behavior, substantially reducing the navigational distance. Experimental results on the REVERIE, SOON, and R2R datasets demonstrate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2403.10008.pdf' target='_blank'>https://arxiv.org/pdf/2403.10008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hideki Deguchi, Kazuki Shibata, Shun Taguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10008">Language to Map: Topological map generation from natural language path instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, a method for generating a map from path information described using natural language (textual path) is proposed. In recent years, robotics research mainly focus on vision-and-language navigation (VLN), a navigation task based on images and textual paths. Although VLN is expected to facilitate user instructions to robots, its current implementation requires users to explain the details of the path for each navigation session, which results in high explanation costs for users. To solve this problem, we proposed a method that creates a map as a topological map from a textual path and automatically creates a new path using this map. We believe that large language models (LLMs) can be used to understand textual path. Therefore, we propose and evaluate two methods, one for storing implicit maps in LLMs, and the other for generating explicit maps using LLMs. The implicit map is in the LLM's memory. It is created using prompts. In the explicit map, a topological map composed of nodes and edges is constructed and the actions at each node are stored. This makes it possible to estimate the path and actions at waypoints on an undescribed path, if enough information is available. Experimental results on path instructions generated in a real environment demonstrate that generating explicit maps achieves significantly higher accuracy than storing implicit maps in the LLMs.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2509.25687.pdf' target='_blank'>https://arxiv.org/pdf/2509.25687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinda Xue, Junjun Hu, Minghua Luo, Xie Shichao, Jintao Chen, Zixun Xie, Quan Kuichen, Guo Wei, Mu Xu, Zedong Chu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25687">OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied navigation presents a core challenge for intelligent robots, requiring the comprehension of visual environments, natural language instructions, and autonomous exploration. Existing models often fall short in offering a unified solution across diverse navigation paradigms, resulting in low success rates and limited generalization. We introduce OmniNav, a unified framework addressing instruct-goal, object-goal, point-goal navigation, and frontier-based exploration within a single architecture. Our approach features a lightweight, low-latency policy that accurately predicts continuous-space waypoints (coordinates and orientations). This policy surpasses action-chunk methods in precision and supports real-world deployment at control frequencies up to 5 Hz. Architecturally, OmniNav employs a fast-slow system design: a fast module generates waypoints using short-horizon visual context and subtasks, while a slow module performs deliberative planning with long-horizon observations and candidate frontiers to select subsequent subgoals and subtasks. This collaboration enhances path efficiency and maintains trajectory coherence, particularly in exploration and memory-intensive scenarios. Crucially, we identify that the primary bottleneck isn't merely navigation policy learning, but a robust understanding of general instructions and objects. To boost generalization, OmniNav integrates large-scale, general-purpose training datasets, including those for image captioning and visual recognition, into a joint multi-task regimen. This significantly improves success rates and robustness. Extensive experiments confirm OmniNav's state-of-the-art performance across various navigation benchmarks, with real-world deployment further validating its efficacy. OmniNav provides practical insights for embodied navigation, charting a scalable path towards versatile, highly generalizable robotic intelligence.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2508.07406.pdf' target='_blank'>https://arxiv.org/pdf/2508.07406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobei Zhao, Xingqi Lyu, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07406">AgriVLN: Vision-and-Language Navigation for Agricultural Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agricultural robots have emerged as powerful members in agricultural tasks, nevertheless, still heavily rely on manual operation or untransportable railway for movement, resulting in limited mobility and poor adaptability. Vision-and-Language Navigation (VLN) enables robots to navigate to the target destinations following natural language instructions, demonstrating strong performance on several domains. However, none of the existing benchmarks or methods is specifically designed for agricultural scenes. To bridge this gap, we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560 episodes across six diverse agricultural scenes, in which all realistic RGB videos are captured by front-facing camera on a quadruped robot at a height of 0.38 meters, aligning with the practical deployment conditions. Meanwhile, we propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN) baseline based on Vision-Language Model (VLM) prompted with carefully crafted templates, which can understand both given instructions and agricultural environments to generate appropriate low-level actions for robot control. When evaluated on A2A, AgriVLN performs well on short instructions but struggles with long instructions, because it often fails to track which part of the instruction is currently being executed. To address this, we further propose Subtask List (STL) instruction decomposition module and integrate it into AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare AgriVLN with several existing VLN methods, demonstrating the state-of-the-art performance in the agricultural domain.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2508.00288.pdf' target='_blank'>https://arxiv.org/pdf/2508.00288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianqiang Xiao, Yuexuan Sun, Yixin Shao, Boxi Gan, Rongqiang Liu, Yanjing Wu, Weili Guan, Xiang Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00288">UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerial navigation is a fundamental yet underexplored capability in embodied intelligence, enabling agents to operate in large-scale, unstructured environments where traditional navigation paradigms fall short. However, most existing research follows the Vision-and-Language Navigation (VLN) paradigm, which heavily depends on sequential linguistic instructions, limiting its scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark for large-scale Object Goal Navigation (ObjectNav) by aerial agents in open-world environments, where agents operate based on high-level semantic goals without relying on detailed instructional guidance as in VLN. UAV-ON comprises 14 high-fidelity Unreal Engine environments with diverse semantic regions and complex spatial layouts, covering urban, natural, and mixed-use settings. It defines 1270 annotated target objects, each characterized by an instance-level instruction that encodes category, physical footprint, and visual descriptors, allowing grounded reasoning. These instructions serve as semantic goals, introducing realistic ambiguity and complex reasoning challenges for aerial agents. To evaluate the benchmark, we implement several baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that integrates instruction semantics with egocentric observations for long-horizon, goal-directed exploration. Empirical results show that all baselines struggle in this setting, highlighting the compounded challenges of aerial navigation and semantic goal grounding. UAV-ON aims to advance research on scalable UAV autonomy driven by semantic goal descriptions in complex real-world environments.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2507.06564.pdf' target='_blank'>https://arxiv.org/pdf/2507.06564.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianshun Li, Tianyi Huai, Zhen Li, Yichun Gao, Haoang Li, Xinhu Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06564">SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across various sectors, driven by their mobility and adaptability. This paper introduces SkyVLN, a novel framework integrating vision-and-language navigation (VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in complex urban environments. Unlike traditional navigation methods, SkyVLN leverages Large Language Models (LLMs) to interpret natural language instructions and visual observations, enabling UAVs to navigate through dynamic 3D spaces with improved accuracy and robustness. We present a multimodal navigation agent equipped with a fine-grained spatial verbalizer and a history path memory mechanism. These components allow the UAV to disambiguate spatial contexts, handle ambiguous instructions, and backtrack when necessary. The framework also incorporates an NMPC module for dynamic obstacle avoidance, ensuring precise trajectory tracking and collision prevention. To validate our approach, we developed a high-fidelity 3D urban simulation environment using AirSim, featuring realistic imagery and dynamic urban elements. Extensive experiments demonstrate that SkyVLN significantly improves navigation success rates and efficiency, particularly in new and unseen environments.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2503.16394.pdf' target='_blank'>https://arxiv.org/pdf/2503.16394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akhil Perincherry, Jacob Krantz, Stefan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16394">Do Visual Imaginations Improve Vision-and-Language Navigation Agents?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) agents are tasked with navigating an unseen environment using natural language instructions. In this work, we study if visual representations of sub-goals implied by the instructions can serve as navigational cues and lead to increased navigation performance. To synthesize these visual representations or imaginations, we leverage a text-to-image diffusion model on landmark references contained in segmented instructions. These imaginations are provided to VLN agents as an added modality to act as landmark cues and an auxiliary loss is added to explicitly encourage relating these with their corresponding referring expressions. Our findings reveal an increase in success rate (SR) of around 1 point and up to 0.5 points in success scaled by inverse path length (SPL) across agents. These results suggest that the proposed approach reinforces visual understanding compared to relying on language instructions alone. Code and data for our work can be found at https://www.akhilperincherry.com/VLN-Imagine-website/.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2501.05478.pdf' target='_blank'>https://arxiv.org/pdf/2501.05478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Malak Mansour, Ahmed Aly, Bahey Tharwat, Sarim Hashmi, Dong An, Ian Reid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05478">Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) such as GPT-4, trained on huge amount of datasets spanning multiple domains, exhibit significant reasoning, understanding, and planning capabilities across various tasks. This study presents the first-ever work in Arabic language integration within the Vision-and-Language Navigation (VLN) domain in robotics, an area that has been notably underexplored in existing research. We perform a comprehensive evaluation of state-of-the-art multi-lingual Small Language Models (SLMs), including GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the Arabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure LLM-based instruction-following navigation agent, to assess the impact of language on navigation reasoning through zero-shot sequential action prediction using the R2R dataset. Through comprehensive experiments, we demonstrate that our framework is capable of high-level planning for navigation tasks when provided with instructions in both English and Arabic. However, certain models struggled with reasoning and planning in the Arabic language due to inherent limitations in their capabilities, sub-optimal performance, and parsing issues. These findings highlight the importance of enhancing planning and reasoning capabilities in language models for effective navigation, emphasizing this as a key area for further development while also unlocking the potential of Arabic-language models for impactful real-world applications.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2405.07060.pdf' target='_blank'>https://arxiv.org/pdf/2405.07060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masaki Kuribayashi, Kohei Uehara, Allan Wang, Daisuke Sato, Simon Chu, Shigeo Morishima
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07060">Memory-Maze: Scenario Driven Benchmark and Visual Language Navigation Model for Guiding Blind People</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Language Navigation (VLN) powered navigation robots have the potential to guide blind people by understanding and executing route instructions provided by sighted passersby. This capability allows robots to operate in environments that are often unknown a priori. Existing VLN models are insufficient for the scenario of navigation guidance for blind people, as they need to understand routes described from human memory, which frequently contain stutters, errors, and omission of details as opposed to those obtained by thinking out loud, such as in the Room-to-Room dataset. However, currently, there is no benchmark that simulates instructions that were obtained from human memory in environments where blind people navigate. To this end, we present our benchmark, Memory-Maze, which simulates the scenario of seeking route instructions for guiding blind people. Our benchmark contains a maze-like structured virtual environment and novel route instruction data from human memory. To collect natural language instructions, we conducted two studies from sighted passersby onsite and annotators online. Our analysis demonstrates that instructions data collected onsite were more lengthy and contained more varied wording. Alongside our benchmark, we propose a VLN model better equipped to handle the scenario. Our proposed VLN model uses Large Language Models (LLM) to parse instructions and generate Python codes for robot control. We further show that the existing state-of-the-art model performed suboptimally on our benchmark. In contrast, our proposed method outperformed the state-of-the-art model by a fair margin. We found that future research should exercise caution when considering VLN technology for practical applications, as real-world scenarios have different characteristics than ones collected in traditional settings.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2301.04352.pdf' target='_blank'>https://arxiv.org/pdf/2301.04352.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting Wang, Zongkai Wu, Feiyu Yao, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.04352">Graph based Environment Representation for Vision-and-Language Navigation in Continuous Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation in Continuous Environments (VLN-CE) is a navigation task that requires an agent to follow a language instruction in a realistic environment. The understanding of environments is a crucial part of the VLN-CE task, but existing methods are relatively simple and direct in understanding the environment, without delving into the relationship between language instructions and visual environments. Therefore, we propose a new environment representation in order to solve the above problems. First, we propose an Environment Representation Graph (ERG) through object detection to express the environment in semantic level. This operation enhances the relationship between language and environment. Then, the relational representations of object-object, object-agent in ERG are learned through GCN, so as to obtain a continuous expression about ERG. Sequentially, we combine the ERG expression with object label embeddings to obtain the environment representation. Finally, a new cross-modal attention navigation framework is proposed, incorporating our environment representation and a special loss function dedicated to training ERG. Experimental result shows that our method achieves satisfactory performance in terms of success rate on VLN-CE tasks. Further analysis explains that our method attains better cross-modal matching and strong generalization ability.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2509.25655.pdf' target='_blank'>https://arxiv.org/pdf/2509.25655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongsheng Yang, Meiling Zhu, Yinfeng Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25655">Landmark-Guided Knowledge for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language navigation is one of the core tasks in embodied intelligence, requiring an agent to autonomously navigate in an unfamiliar environment based on natural language instructions. However, existing methods often fail to match instructions with environmental information in complex scenarios, one reason being the lack of common-sense reasoning ability. This paper proposes a vision-and-language navigation method called Landmark-Guided Knowledge (LGK), which introduces an external knowledge base to assist navigation, addressing the misjudgment issues caused by insufficient common sense in traditional methods. Specifically, we first construct a knowledge base containing 630,000 language descriptions and use knowledge Matching to align environmental subviews with the knowledge base, extracting relevant descriptive knowledge. Next, we design a Knowledge-Guided by Landmark (KGL) mechanism, which guides the agent to focus on the most relevant parts of the knowledge by leveraging landmark information in the instructions, thereby reducing the data bias that may arise from incorporating external knowledge. Finally, we propose Knowledge-Guided Dynamic Augmentation (KGDA), which effectively integrates language, knowledge, vision, and historical information. Experimental results demonstrate that the LGK method outperforms existing state-of-the-art methods on the R2R and REVERIE vision-and-language navigation datasets, particularly in terms of navigation error, success rate, and path efficiency.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2508.16654.pdf' target='_blank'>https://arxiv.org/pdf/2508.16654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Liu, Zhimu Zhou, Jiachen Zhang, Minghao Zhang, Songfang Huang, Huiling Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16654">MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) requires an agent to interpret natural language instructions and navigate complex environments. Current approaches often adopt a "black-box" paradigm, where a single Large Language Model (LLM) makes end-to-end decisions. However, it is plagued by critical vulnerabilities, including poor spatial reasoning, weak cross-modal grounding, and memory overload in long-horizon tasks. To systematically address these issues, we propose Memory Spatial Navigation(MSNav), a framework that fuses three modules into a synergistic architecture, which transforms fragile inference into a robust, integrated intelligence. MSNav integrates three modules: Memory Module, a dynamic map memory module that tackles memory overload through selective node pruning, enhancing long-range exploration; Spatial Module, a module for spatial reasoning and object relationship inference that improves endpoint recognition; and Decision Module, a module using LLM-based path planning to execute robust actions. Powering Spatial Module, we also introduce an Instruction-Object-Space (I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp), which outperforms leading commercial LLMs in object list extraction, achieving higher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the Room-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art performance with significant improvements in Success Rate (SR) and Success weighted by Path Length (SPL).
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2505.00743.pdf' target='_blank'>https://arxiv.org/pdf/2505.00743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinfeng Yu, Dongsheng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00743">DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) is a challenging task where an agent must understand language instructions and navigate unfamiliar environments using visual cues. The agent must accurately locate the target based on visual information from the environment and complete tasks through interaction with the surroundings. Despite significant advancements in this field, two major limitations persist: (1) Many existing methods input complete language instructions directly into multi-layer Transformer networks without fully exploiting the detailed information within the instructions, thereby limiting the agent's language understanding capabilities during task execution; (2) Current approaches often overlook the modeling of object relationships across different modalities, failing to effectively utilize latent clues between objects, which affects the accuracy and robustness of navigation decisions. We propose a Dual Object Perception-Enhancement Network (DOPE) to address these issues to improve navigation performance. First, we design a Text Semantic Extraction (TSE) to extract relatively essential phrases from the text and input them into the Text Object Perception-Augmentation (TOPA) to fully leverage details such as objects and actions within the instructions. Second, we introduce an Image Object Perception-Augmentation (IOPA), which performs additional modeling of object information across different modalities, enabling the model to more effectively utilize latent clues between objects in images and text, enhancing decision-making accuracy. Extensive experiments on the R2R and REVERIE datasets validate the efficacy of the proposed approach.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2504.21432.pdf' target='_blank'>https://arxiv.org/pdf/2504.21432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Saxena, Nishant Raghuvanshi, Neena Goveas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21432">UAV-VLN: End-to-End Vision Language guided Navigation for UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A core challenge in AI-guided autonomy is enabling agents to navigate realistically and effectively in previously unseen environments based on natural language commands. We propose UAV-VLN, a novel end-to-end Vision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs) that seamlessly integrates Large Language Models (LLMs) with visual perception to facilitate human-interactive navigation. Our system interprets free-form natural language instructions, grounds them into visual observations, and plans feasible aerial trajectories in diverse environments.
  UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse high-level semantic goals, while a vision model detects and localizes semantically relevant objects in the environment. By fusing these modalities, the UAV can reason about spatial relationships, disambiguate references in human instructions, and plan context-aware behaviors with minimal task-specific supervision. To ensure robust and interpretable decision-making, the framework includes a cross-modal grounding mechanism that aligns linguistic intent with visual context.
  We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios, demonstrating its ability to generalize to novel instructions and environments with minimal task-specific training. Our results show significant improvements in instruction-following accuracy and trajectory efficiency, highlighting the potential of LLM-driven vision-language interfaces for safe, intuitive, and generalizable UAV autonomy.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2504.21432.pdf' target='_blank'>https://arxiv.org/pdf/2504.21432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Saxena, Nishant Raghuvanshi, Neena Goveas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21432">UAV-VLN: End-to-End Vision Language guided Navigation for UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A core challenge in AI-guided autonomy is enabling agents to navigate realistically and effectively in previously unseen environments based on natural language commands. We propose UAV-VLN, a novel end-to-end Vision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs) that seamlessly integrates Large Language Models (LLMs) with visual perception to facilitate human-interactive navigation. Our system interprets free-form natural language instructions, grounds them into visual observations, and plans feasible aerial trajectories in diverse environments. UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse high-level semantic goals, while a vision model detects and localizes semantically relevant objects in the environment. By fusing these modalities, the UAV can reason about spatial relationships, disambiguate references in human instructions, and plan context-aware behaviors with minimal task-specific supervision. To ensure robust and interpretable decision-making, the framework includes a cross-modal grounding mechanism that aligns linguistic intent with visual context. We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios, demonstrating its ability to generalize to novel instructions and environments with minimal task-specific training. Our results show significant improvements in instruction-following accuracy and trajectory efficiency, highlighting the potential of LLM-driven vision-language interfaces for safe, intuitive, and generalizable UAV autonomy.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2504.16516.pdf' target='_blank'>https://arxiv.org/pdf/2504.16516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junrong Yue, Yifan Zhang, Chuan Qin, Bo Li, Xiaomin Lie, Xinlei Yu, Wenxin Zhang, Zhendong Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16516">Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow natural language instructions and reach target locations in real-world environments. While prior methods often rely on either global scene representations or object-level features, these approaches are insufficient for capturing the complex interactions across modalities required for accurate navigation. In this paper, we propose a Multi-level Fusion and Reasoning Architecture (MFRA) to enhance the agent's ability to reason over visual observations, language instructions and navigation history. Specifically, MFRA introduces a hierarchical fusion mechanism that aggregates multi-level features-ranging from low-level visual cues to high-level semantic concepts-across multiple modalities. We further design a reasoning module that leverages fused representations to infer navigation actions through instruction-guided attention and dynamic context integration. By selectively capturing and combining relevant visual, linguistic, and temporal signals, MFRA improves decision-making accuracy in complex navigation scenarios. Extensive experiments on benchmark VLN datasets including REVERIE, R2R, and SOON demonstrate that MFRA achieves superior performance compared to state-of-the-art methods, validating the effectiveness of multi-level modal fusion for embodied navigation.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2408.02535.pdf' target='_blank'>https://arxiv.org/pdf/2408.02535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhao Kaichen, Song Yaoxian, Zhao Haiquan, Liu Haoyu, Li Tiefeng, Li Zhixu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02535">Towards Coarse-grained Visual Language Navigation Task Planning Enhanced by Event Knowledge Graph</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual language navigation (VLN) is one of the important research in embodied AI. It aims to enable an agent to understand the surrounding environment and complete navigation tasks. VLN instructions could be categorized into coarse-grained and fine-grained commands. Fine-grained command describes a whole task with subtasks step-by-step. In contrast, coarse-grained command gives an abstract task description, which more suites human habits. Most existing work focuses on the former kind of instruction in VLN tasks, ignoring the latter abstract instructions belonging to daily life scenarios. To overcome the above challenge in abstract instruction, we attempt to consider coarse-grained instruction in VLN by event knowledge enhancement. Specifically, we first propose a prompt-based framework to extract an event knowledge graph (named VLN-EventKG) for VLN integrally over multiple mainstream benchmark datasets. Through small and large language model collaboration, we realize knowledge-enhanced navigation planning (named EventNav) for VLN tasks with coarse-grained instruction input. Additionally, we design a novel dynamic history backtracking module to correct potential error action planning in real time. Experimental results in various public benchmarks show our knowledge-enhanced method has superiority in coarse-grained-instruction VLN using our proposed VLN-EventKG with over $5\%$ improvement in success rate. Our project is available at https://sites.google.com/view/vln-eventkg
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2405.10620.pdf' target='_blank'>https://arxiv.org/pdf/2405.10620.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaohuan Zhan, Lisha Yu, Sijie Yu, Guang Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10620">MC-GPT: Empowering Vision-and-Language Navigation with Memory Map and Reasoning Chains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the Vision-and-Language Navigation (VLN) task, the agent is required to navigate to a destination following a natural language instruction. While learning-based approaches have been a major solution to the task, they suffer from high training costs and lack of interpretability. Recently, Large Language Models (LLMs) have emerged as a promising tool for VLN due to their strong generalization capabilities. However, existing LLM-based methods face limitations in memory construction and diversity of navigation strategies. To address these challenges, we propose a suite of techniques. Firstly, we introduce a method to maintain a topological map that stores navigation history, retaining information about viewpoints, objects, and their spatial relationships. This map also serves as a global action space. Additionally, we present a Navigation Chain of Thoughts module, leveraging human navigation examples to enrich navigation strategy diversity. Finally, we establish a pipeline that integrates navigational memory and strategies with perception and action prediction modules. Experimental results on the REVERIE and R2R datasets show that our method effectively enhances the navigation ability of the LLM and improves the interpretability of navigation reasoning.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2403.19336.pdf' target='_blank'>https://arxiv.org/pdf/2403.19336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacui Huang, Hongtao Zhang, Mingbo Zhao, Zhou Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19336">IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) is a challenging task that requires a robot to navigate in photo-realistic environments with human natural language promptings. Recent studies aim to handle this task by constructing the semantic spatial map representation of the environment, and then leveraging the strong ability of reasoning in large language models for generalizing code for guiding the robot navigation. However, these methods face limitations in instance-level and attribute-level navigation tasks as they cannot distinguish different instances of the same object. To address this challenge, we propose a new method, namely, Instance-aware Visual Language Map (IVLMap), to empower the robot with instance-level and attribute-level semantic mapping, where it is autonomously constructed by fusing the RGBD video data collected from the robot agent with special-designed natural language map indexing in the bird's-in-eye view. Such indexing is instance-level and attribute-level. In particular, when integrated with a large language model, IVLMap demonstrates the capability to i) transform natural language into navigation targets with instance and attribute information, enabling precise localization, and ii) accomplish zero-shot end-to-end navigation tasks based on natural language commands. Extensive navigation experiments are conducted. Simulation results illustrate that our method can achieve an average improvement of 14.4\% in navigation accuracy. Code and demo are released at https://ivlmap.github.io/.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2403.18454.pdf' target='_blank'>https://arxiv.org/pdf/2403.18454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valay Bundele, Mahesh Bhupati, Biplab Banerjee, Aditya Grover
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18454">Scaling Vision-and-Language Navigation With Offline RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The study of vision-and-language navigation (VLN) has typically relied on expert trajectories, which may not always be available in real-world situations due to the significant effort required to collect them. On the other hand, existing approaches to training VLN agents that go beyond available expert data involve data augmentations or online exploration which can be tedious and risky. In contrast, it is easy to access large repositories of suboptimal offline trajectories. Inspired by research in offline reinforcement learning (ORL), we introduce a new problem setup of VLN-ORL which studies VLN using suboptimal demonstration data. We introduce a simple and effective reward-conditioned approach that can account for dataset suboptimality for training VLN agents, as well as benchmarks to evaluate progress and promote research in this area. We empirically study various noise models for characterizing dataset suboptimality among other unique challenges in VLN-ORL and instantiate it for the VLN$\circlearrowright$BERT and MTVM architectures in the R2R and RxR environments. Our experiments demonstrate that the proposed reward-conditioned approach leads to significant performance improvements, even in complex and intricate environments.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2307.10790.pdf' target='_blank'>https://arxiv.org/pdf/2307.10790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijiao Yang, Arjun Majumdar, Stefan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10790">Behavioral Analysis of Vision-and-Language Navigation Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To be successful, Vision-and-Language Navigation (VLN) agents must be able to ground instructions to actions based on their surroundings. In this work, we develop a methodology to study agent behavior on a skill-specific basis -- examining how well existing agents ground instructions about stopping, turning, and moving towards specified objects or rooms. Our approach is based on generating skill-specific interventions and measuring changes in agent predictions. We present a detailed case study analyzing the behavior of a recent agent and then compare multiple agents in terms of skill-specific competency scores. This analysis suggests that biases from training have lasting effects on agent behavior and that existing models are able to ground simple referring expressions. Our comparisons between models show that skill-specific scores correlate with improvements in overall VLN task performance.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2303.04077.pdf' target='_blank'>https://arxiv.org/pdf/2303.04077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minyoung Hwang, Jaeyeon Jeong, Minsoo Kim, Yoonseon Oh, Songhwai Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04077">Meta-Explore: Exploratory Hierarchical Vision-and-Language Navigation Using Scene Object Spectrum Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The main challenge in vision-and-language navigation (VLN) is how to understand natural-language instructions in an unseen environment. The main limitation of conventional VLN algorithms is that if an action is mistaken, the agent fails to follow the instructions or explores unnecessary regions, leading the agent to an irrecoverable path. To tackle this problem, we propose Meta-Explore, a hierarchical navigation method deploying an exploitation policy to correct misled recent actions. We show that an exploitation policy, which moves the agent toward a well-chosen local goal among unvisited but observable states, outperforms a method which moves the agent to a previously visited state. We also highlight the demand for imagining regretful explorations with semantically meaningful clues. The key to our approach is understanding the object placements around the agent in spectral-domain. Specifically, we present a novel visual representation, called scene object spectrum (SOS), which performs category-wise 2D Fourier transform of detected objects. Combining exploitation policy and SOS features, the agent can correct its path by choosing a promising local goal. We evaluate our method in three VLN benchmarks: R2R, SOON, and REVERIE. Meta-Explore outperforms other baselines and shows significant generalization performance. In addition, local goal search using the proposed spectral-domain SOS features significantly improves the success rate by 17.1% and SPL by 20.6% for the SOON benchmark.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2508.02917.pdf' target='_blank'>https://arxiv.org/pdf/2508.02917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>VebjÃ¸rn Haug KÃ¥sene, Pierre Lison
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02917">Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) refers to the task of enabling autonomous robots to navigate unfamiliar environments by following natural language instructions. While recent Large Vision-Language Models (LVLMs) have shown promise in this task, most current VLM systems rely on models specifically designed and optimized for navigation, leaving the potential of off-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used low-level action spaces with egocentric views and atomic actions (such as "turn left" or "move forward"), newer models tend to favor panoramic action spaces with discrete navigable viewpoints. This paper investigates (1) whether off-the-shelf LVLMs (fine-tuned without architectural modifications or simulator-based training) can effectively support VLN tasks and (2) whether such models can support both low-level and panoramic action paradigms. To this end, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the Room-to-Room (R2R) dataset and evaluate its empirical performance across both low-level and panoramic action spaces. The best resulting model achieves a 41% success rate on the R2R test set, demonstrating that while off-the-shelf LVLMs can learn to perform Vision-and-Language Navigation, they still lag behind models specifically designed for this task.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2506.10172.pdf' target='_blank'>https://arxiv.org/pdf/2506.10172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yicheng Duan, Kaiyu tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10172">A Navigation Framework Utilizing Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) presents a complex challenge in embodied AI, requiring agents to interpret natural language instructions and navigate through visually rich, unfamiliar environments. Recent advances in large vision-language models (LVLMs), such as CLIP and Flamingo, have significantly improved multimodal understanding but introduced new challenges related to computational cost and real-time deployment. In this project, we propose a modular, plug-and-play navigation framework that decouples vision-language understanding from action planning. By integrating a frozen vision-language model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to achieve flexible, fast, and adaptable navigation without extensive model fine-tuning. Our framework leverages prompt engineering, structured history management, and a two-frame visual input strategy to enhance decision-making continuity across navigation steps. We evaluate our system on the Room-to-Room benchmark within the VLN-CE setting using the Matterport3D dataset and Habitat-Lab simulation environment. Although our initial results reveal challenges in generalizing to unseen environments under strict evaluation settings, our modular approach lays a foundation for scalable and efficient navigation systems, highlighting promising directions for future improvement through enhanced environmental priors and expanded multimodal input integration.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2504.08806.pdf' target='_blank'>https://arxiv.org/pdf/2504.08806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luo Ling, Bai Qianqian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08806">Endowing Embodied Agents with Spatial Reasoning Capabilities for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enhancing the spatial perception capabilities of mobile robots is crucial for achieving embodied Vision-and-Language Navigation (VLN). Although significant progress has been made in simulated environments, directly transferring these capabilities to real-world scenarios often results in severe hallucination phenomena, causing robots to lose effective spatial awareness. To address this issue, we propose BrainNav, a bio-inspired spatial cognitive navigation framework inspired by biological spatial cognition theories and cognitive map theory. BrainNav integrates dual-map (coordinate map and topological map) and dual-orientation (relative orientation and absolute orientation) strategies, enabling real-time navigation through dynamic scene capture and path planning. Its five core modules-Hippocampal Memory Hub, Visual Cortex Perception Engine, Parietal Spatial Constructor, Prefrontal Decision Center, and Cerebellar Motion Execution Unit-mimic biological cognitive functions to reduce spatial hallucinations and enhance adaptability. Validated in a zero-shot real-world lab environment using the Limo Pro robot, BrainNav, compatible with GPT-4, outperforms existing State-of-the-Art (SOTA) Vision-and-Language Navigation in Continuous Environments (VLN-CE) methods without fine-tuning.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2411.18539.pdf' target='_blank'>https://arxiv.org/pdf/2411.18539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dillon Loh, Tomasz Bednarz, Xinxing Xia, Frank Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18539">AdaVLN: Towards Visual Language Navigation in Continuous Indoor Environments with Moving Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Language Navigation is a task that challenges robots to navigate in realistic environments based on natural language instructions. While previous research has largely focused on static settings, real-world navigation must often contend with dynamic human obstacles. Hence, we propose an extension to the task, termed Adaptive Visual Language Navigation (AdaVLN), which seeks to narrow this gap. AdaVLN requires robots to navigate complex 3D indoor environments populated with dynamically moving human obstacles, adding a layer of complexity to navigation tasks that mimic the real-world. To support exploration of this task, we also present AdaVLN simulator and AdaR2R datasets. The AdaVLN simulator enables easy inclusion of fully animated human models directly into common datasets like Matterport3D. We also introduce a "freeze-time" mechanism for both the navigation task and simulator, which pauses world state updates during agent inference, enabling fair comparisons and experimental reproducibility across different hardware. We evaluate several baseline models on this task, analyze the unique challenges introduced by AdaVLN, and demonstrate its potential to bridge the sim-to-real gap in VLN research.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2312.00151.pdf' target='_blank'>https://arxiv.org/pdf/2312.00151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meera Hahn, Amit Raj, James M. Rehg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00151">Which way is `right'?: Uncovering limitations of Vision-and-Language Navigation model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The challenging task of Vision-and-Language Navigation (VLN) requires embodied agents to follow natural language instructions to reach a goal location or object (e.g. `walk down the hallway and turn left at the piano'). For agents to complete this task successfully, they must be able to ground objects referenced into the instruction (e.g.`piano') into the visual scene as well as ground directional phrases (e.g.`turn left') into actions. In this work we ask the following question -- to what degree are spatial and directional language cues informing the navigation model's decisions? We propose a series of simple masking experiments to inspect the model's reliance on different parts of the instruction. Surprisingly we uncover that certain top performing models rely only on the noun tokens of the instructions. We propose two training methods to alleviate this concerning limitation.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2310.10822.pdf' target='_blank'>https://arxiv.org/pdf/2310.10822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengguang Xu, Hieu T. Nguyen, Christopher Amato, Lawson L. S. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10822">Vision and Language Navigation in the Real World via Online Visual Language Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating in unseen environments is crucial for mobile robots. Enhancing them with the ability to follow instructions in natural language will further improve navigation efficiency in unseen cases. However, state-of-the-art (SOTA) vision-and-language navigation (VLN) methods are mainly evaluated in simulation, neglecting the complex and noisy real world. Directly transferring SOTA navigation policies trained in simulation to the real world is challenging due to the visual domain gap and the absence of prior knowledge about unseen environments. In this work, we propose a novel navigation framework to address the VLN task in the real world. Utilizing the powerful foundation models, the proposed framework includes four key components: (1) an LLMs-based instruction parser that converts the language instruction into a sequence of pre-defined macro-action descriptions, (2) an online visual-language mapper that builds a real-time visual-language map to maintain a spatial and semantic understanding of the unseen environment, (3) a language indexing-based localizer that grounds each macro-action description into a waypoint location on the map, and (4) a DD-PPO-based local controller that predicts the action. We evaluate the proposed pipeline on an Interbotix LoCoBot WX250 in an unseen lab environment. Without any fine-tuning, our pipeline significantly outperforms the SOTA VLN baseline in the real world.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2103.00852.pdf' target='_blank'>https://arxiv.org/pdf/2103.00852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aly Magassouba, Komei Sugiura, Hisashi Kawai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.00852">CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double Back-Translation for Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigation guided by natural language instructions is particularly suitable for Domestic Service Robots that interacts naturally with users. This task involves the prediction of a sequence of actions that leads to a specified destination given a natural language navigation instruction. The task thus requires the understanding of instructions, such as ``Walk out of the bathroom and wait on the stairs that are on the right''. The Visual and Language Navigation remains challenging, notably because it requires the exploration of the environment and at the accurate following of a path specified by the instructions to model the relationship between language and vision. To address this, we propose the CrossMap Transformer network, which encodes the linguistic and visual features to sequentially generate a path. The CrossMap transformer is tied to a Transformer-based speaker that generates navigation instructions. The two networks share common latent features, for mutual enhancement through a double back translation model: Generated paths are translated into instructions while generated instructions are translated into path The experimental results show the benefits of our approach in terms of instruction understanding and instruction generation.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2411.01153.pdf' target='_blank'>https://arxiv.org/pdf/2411.01153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sonit Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01153">Designing a Robust Radiology Report Generation System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in deep learning have enabled researchers to explore tasks at the intersection of computer vision and natural language processing, such as image captioning, visual question answering, visual dialogue, and visual language navigation. Taking inspiration from image captioning, the task of radiology report generation aims at automatically generating radiology reports by having a comprehensive understanding of medical images. However, automatically generating radiology reports from medical images is a challenging task due to the complexity, diversity, and nature of medical images. In this paper, we outline the design of a robust radiology report generation system by integrating different modules and highlighting best practices drawing upon lessons from our past work and also from relevant studies in the literature. We also discuss the impact of integrating different components to form a single integrated system. We believe that these best practices, when implemented, could improve automatic radiology report generation, augment radiologists in decision making, and expedite diagnostic workflow, in turn improve healthcare and save human lives.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2405.16994.pdf' target='_blank'>https://arxiv.org/pdf/2405.16994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Hanlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16994">Vision-and-Language Navigation Generative Pretrained Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the Vision-and-Language Navigation (VLN) field, agents are tasked with navigating real-world scenes guided by linguistic instructions. Enabling the agent to adhere to instructions throughout the process of navigation represents a significant challenge within the domain of VLN. To address this challenge, common approaches often rely on encoders to explicitly record past locations and actions, increasing model complexity and resource consumption. Our proposal, the Vision-and-Language Navigation Generative Pretrained Transformer (VLN-GPT), adopts a transformer decoder model (GPT2) to model trajectory sequence dependencies, bypassing the need for historical encoding modules. This method allows for direct historical information access through trajectory sequence, enhancing efficiency. Furthermore, our model separates the training process into offline pre-training with imitation learning and online fine-tuning with reinforcement learning. This distinction allows for more focused training objectives and improved performance. Performance assessments on the VLN dataset reveal that VLN-GPT surpasses complex state-of-the-art encoder-based models.
